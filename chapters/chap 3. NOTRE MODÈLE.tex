\chapter{NOTRE MODÈLE}
% Avant de présenter notre modèle, il est essentiel d'introduire les concepts théoriques et méthodologiques qui constituent son fondement. Ce chapitre expose ainsi les principes du compressed sensing, la formulation des problèmes inverses en tomodensitométrie, les techniques de régularisation par parcimonie, ainsi que les outils d'optimisation et les transformées en ondelettes utilisés dans notre approche. Dans ce cadre, nous proposons un modèle dédié à la reconstruction d'images en tomodensitométrie dans un contexte de données limitées. Ce modèle s'appuie sur le principe de parcimonie afin de permettre une reconstruction fidèle à partir d'un nombre réduit de mesures. Le problème est formulé comme un problème inverse mal posé, traité à l'aide d'une approche variationnelle intégrant une régularisation basée sur les transformées en ondelettes. Sa résolution repose sur l'algorithme ADMM, reconnu pour son efficacité dans ce type de problèmes d'optimisation. Enfin, des indicateurs quantitatifs sont introduits afin d'évaluer la qualité des reconstructions et de valider les performances du modèle proposé.
\section{Introduction du chapitre}

Avant d'introduire formellement le modèle de reconstruction proposé, il est nécessaire d'établir le cadre théorique et méthodologique qui en constitue le fondement. La reconstruction d'images en tomodensitométrie à faible dose s'inscrit en effet dans le cadre des problèmes inverses mal posés, où la réduction du nombre de mesures compromet la stabilité et l'unicité de la solution. Dans ce contexte, le paradigme du \textit{compressed sensing} fournit un cadre théorique rigoureux permettant d'exploiter la parcimonie intrinsèque des images afin de rendre le problème bien conditionné et de permettre une reconstruction fiable à partir de données limitées.

Ce chapitre présente ainsi les principes fondamentaux du \textit{compressed sensing}, la formulation mathématique du problème inverse en tomodensitométrie, ainsi que les approches de régularisation parcimonieuse fondées sur les représentations multi-échelles. Une attention particulière est portée aux transformées en ondelettes biorthogonales, dont les propriétés de localisation et de représentation efficace des singularités les rendent particulièrement adaptées aux images tomographiques. Nous présentons également le cadre variationnel retenu, dans lequel le problème de reconstruction est formulé comme un problème d'optimisation convexe combinant un terme de fidélité aux données et un terme de régularisation favorisant la parcimonie.

La résolution de ce problème est réalisée à l'aide de l'algorithme ADMM (\textit{Alternating Direction Method of Multipliers}), dont la structure permet de découpler efficacement les différentes composantes du problème tout en garantissant la stabilité et l'efficacité numérique du processus de reconstruction. Enfin, ce chapitre introduit les métriques quantitatives utilisées pour l'évaluation des performances du modèle, notamment la MSE, le PSNR et le SSIM, qui permettront d'analyser de manière objective la qualité et la fidélité structurelle des images reconstruites.

L'ensemble de ces éléments conduit à la définition d'un modèle de reconstruction parcimonieux adapté au contexte de tomodensitométrie à faible dose, constituant le socle théorique et algorithmique des développements présentés dans la suite de ce manuscrit.

% ---- TODO -----
% \section{Limites de la méthode analytique}
% ============================================================
\section{Compressed Sensing}
% ============================================================

En tomodensitométrie (CT), la réduction du nombre de projections et de la dose de rayonnement \cite{15}
constitue un enjeu majeur de sécurité clinique et de performance opérationnelle. La diminution
de l'exposition aux rayons $\mathbf{X}$ vise à limiter les risques biologiques associés aux
rayonnements ionisants, en particulier dans les contextes d'examens répétés ou pour les
populations sensibles. Toutefois, cette réduction conduit inévitablement à une acquisition
de données incomplètes et bruitées, rendant la reconstruction d'image plus difficile.\vspace{5pt}\\
D'un point de vue mathématique, cette situation se traduit par un problème inverse
sous-déterminé, pour lequel les méthodes analytiques classiques, telles que la
rétroprojection filtrée, deviennent instables ou génèrent des artefacts importants.
Le \emph{Compressed Sensing} (CS) fournit un cadre théorique et algorithmique permettant
d'aborder cette problématique en exploitant des propriétés structurelles des images CT.

\begin{definition}
    Le \emph{compressed sensing} (CS) est un cadre mathématique et algorithmique permettant la
    reconstruction de signaux de grande dimension à partir d'un nombre de mesures
    significativement inférieur à celui requis par les méthodes d'échantillonnage classiques,
    sous réserve que le signal présente une structure de parcimonie adaptée.
\end{definition}

\begin{definition}
    Soit $x \in \mathbb{R}^{n}$ un signal inconnu. On dit que $x$ est
    \(k\)-parcimonieux dans une base (ou un dictionnaire) \(\Psi\)
    (par exemple ondelettes, DCT) si
    \[
        x = \Psi \alpha, \qquad \text{où } \alpha \text{ possède au plus } k \ll n
        \text{ coefficients non nuls}.
    \]
\end{definition}

Dans le cas des images CT, bien que la distribution d'atténuation ne soit pas parcimonieuse
dans le domaine spatial, elle est souvent compressible dans des bases multi-échelles ou via
le gradient de l'image. Cette propriété constitue le fondement de l'application du
compressed sensing à la reconstruction tomographique.

Les mesures acquises lors d'un examen CT peuvent être modélisées par un ensemble de relations
linéaires :
\[
    \mathbf{y} = \mathbf{A}\mathbf{x},
\]
où $\mathbf{A} \in \mathbb{R}^{m \times n}$ représente l'opérateur de projection discrétisé
(assimilable à la transformée de Radon discrète) et $m \ll n$ lorsque le nombre de projections
est réduit.

Contrairement au cadre classique de l'échantillonnage, qui impose un nombre de mesures au
moins égal à la dimension du signal, le compressed sensing montre que
\[
    m \gtrsim k \log(n/k)
\]
peut être suffisant pour une reconstruction stable, sous des conditions appropriées sur
l'opérateur $\mathbf{A}$, telles que l'incohérence ou la propriété d'isométrie restreinte
(\emph{Restricted Isometry Property}, RIP).

\subsection{Le problème inverse en tomodensitométrie}

La reconstruction CT s'inscrit dans le cadre général des problèmes inverses, où l'objectif
est d'estimer une image à partir de mesures indirectes, bruitées et incomplètes. Ce problème
peut être formulé sous la forme :
\begin{equation}
    \mathbf{y} = \mathcal{A}\mathbf{x} + \mathbf{n},
    \label{eq:inverse_problem}
\end{equation}
où :
\begin{itemize}
    \item[-] $\mathbf{x} \in \mathbb{R}^n$ représente la distribution d'atténuation à reconstruire,
    \item[-] $\mathbf{y} \in \mathbb{R}^m$ correspond aux données de projection (sinogramme),
    \item[-] $\mathcal{A}$ modélise le processus de projection CT,
    \item[-] $\mathbf{n}$ représente le bruit de mesure, principalement de nature quantique.
\end{itemize}

Lorsque le nombre de projections est réduit, l'opérateur $\mathcal{A}$ devient non inversible
et le problème est sous-déterminé. Cette situation est inhérente aux stratégies de réduction
de dose et ne peut être évitée sans compromettre la sécurité du patient.

\subsection{Mal-positude et conséquences pratiques}

\begin{definition}
    Un problème est dit \textbf{bien posé} au sens de Hadamard s'il vérifie l'existence,
    l'unicité et la stabilité de la solution. Si l'une de ces conditions n'est pas satisfaite,
    le problème est dit \emph{mal posé}.
\end{definition}

Dans le contexte de la reconstruction CT à faible dose, la condition d'unicité est violée
du fait de la sous-détermination, et la condition de stabilité est fortement compromise par
la présence de bruit. De faibles fluctuations du sinogramme peuvent ainsi engendrer des
artefacts marqués dans l'image reconstruite.

\subsection{Régularisation par parcimonie et Compressed Sensing}

Pour rendre le problème inverse traitable, il est nécessaire d'introduire des informations
a priori sur la solution recherchée. Le compressed sensing propose d'utiliser la parcimonie
ou la compressibilité de l'image CT dans une représentation appropriée comme mécanisme de
régularisation.\vspace{5pt}\\
Cette hypothèse restreint l'ensemble des solutions admissibles et permet de transformer un
problème inverse mal posé en un problème d'optimisation bien conditionné, pour lequel une
solution stable et physiquement plausible peut être obtenue malgré la réduction du nombre
de projections.\vspace{5pt}\\
Jusqu'à présent, le compressed sensing a été présenté comme un cadre
théorique exploitant la parcimonie pour résoudre des problèmes inverses
sous-déterminés. En pratique, cette hypothèse de parcimonie est intégrée
au processus de reconstruction via des formulations variationnelles.
Ces formulations constituent un cadre général permettant d'unifier les
approches classiques de régularisation et les méthodes issues du
compressed sensing.

\subsection{Formulation variationnelle des problèmes inverses}
% ============================================================================================

Dans de nombreux problèmes d'imagerie, et en particulier en tomodensitométrie
à faible dose, l'objectif est de reconstruire une image inconnue
$\mathbf{x} \in \mathbb{R}^n$ à partir d'un ensemble de mesures
$\mathbf{y} \in \mathbb{R}^m$ (i.e synogramme) obtenues par un système d'acquisition indirect.
Ce processus est généralement modélisé par une relation linéaire de la forme
\[
\mathbf{y} = \mathcal{A}\mathbf{x} + \boldsymbol{\varepsilon},
\]
où $\mathcal{A}$ représente l'opérateur direct du système CT et
$\boldsymbol{\varepsilon}$ un terme de bruit.

Lorsque les données sont bruitées et/ou acquises de manière incomplète
($m \ll n$), l'opérateur $\mathcal{A}$ devient non inversible ou mal conditionné.
Dans ce cas, une inversion directe est soit impossible, soit extrêmement
instable, et de petites perturbations des données peuvent engendrer de fortes
dégradations de la solution reconstruite. Ce phénomène est caractéristique des
problèmes inverses mal posés.

\begin{definition}
Un \emph{problème inverse} consiste à estimer une quantité inconnue
$\mathbf{x}$ à partir d'observations indirectes $\mathbf{y}$, reliées par un
opérateur $\mathcal{A}$, lorsque l'inversion directe de cet opérateur est
impossible ou instable.
\end{definition}

\subsubsection{Principe de la régularisation.}
Afin de rendre le problème inverse traitable, il est nécessaire d'introduire
des informations a priori sur la solution recherchée. Cette démarche est
connue sous le nom de \emph{régularisation}.

\begin{definition}
Une régularisation est une application
$\mathfrak{R}_{\alpha} : \mathbb{R}^m \rightarrow \mathbb{R}^n$ qui associe à
des données observées $\mathbf{y}$ une solution stable $\hat{\mathbf{x}}$,
en incorporant des hypothèses supplémentaires sur la structure de la solution.
\end{definition}

Intuitivement, une méthode de régularisation vise à étendre la notion d'inverse
au cadre bruité et mal posé, de sorte que
\[
\mathfrak{R}_{\alpha}(\mathcal{A}\mathbf{x} + \boldsymbol{\varepsilon})
\approx \mathbf{x},
\]
même lorsque $\boldsymbol{\varepsilon} \neq \mathbf{0}$ ou que
$\mathcal{A}$ n'est pas inversible.

\subsubsection{Formulation variationnelle.}
Une approche largement utilisée pour implémenter la régularisation consiste à
formuler le problème inverse comme un problème d'optimisation variationnelle,
dans lequel on recherche une solution équilibrant fidélité aux données et
conformité aux a priori. Cette formulation s'écrit généralement sous la forme

\begin{equation}
    \hat{\mathbf{x}} =
    \underset{\mathbf{x} \in \mathbb{R}^n}{\arg\min}
    \left\{
    \underbrace{\left\| \mathcal{A}\mathbf{x} - \mathbf{y} \right\|_{2}^{2}}_{\text{fidélité aux données}}
    + \alpha
    \underbrace{\mathcal{R}(\mathbf{x})}_{\text{terme de régularisation}}
    \right\}.
\end{equation}

Les différents termes de cette formulation jouent des rôles complémentaires :

\begin{itemize}
    \item \textbf{Fidélité aux données :}
    Ce terme impose la cohérence entre l'image reconstruite $\mathbf{x}$ et les
    mesures observées $\mathbf{y}$. Dans un contexte bruité, il n'est pas souhaitable
    de l'annuler strictement, car cela conduirait à une reconstruction amplifiant
    le bruit.

    \item \textbf{Terme de régularisation :}
    Le régularisant $\mathcal{R}(\mathbf{x})$ encode les informations a priori
    disponibles sur la solution recherchée, telles que la régularité, la
    parcimonie ou des contraintes physiques. Il permet de restreindre l'ensemble
    des solutions admissibles et d'améliorer la stabilité du problème.

    \item \textbf{Paramètre de régularisation $\alpha$ :}
    Le paramètre $\alpha > 0$ contrôle le compromis entre fidélité aux données
    et influence de l'a priori. Un choix inadéquat peut conduire soit à une
    reconstruction bruitée (faible $\alpha$), soit à une image excessivement
    lissée (grand $\alpha$).
\end{itemize}

% \subsubsection{Cas particulier : régularisation de Tikhonov.}
% Une régularisation classique consiste à choisir un régularisant quadratique,
% conduisant à la régularisation dite de Tikhonov. Par exemple, en supposant que
% la solution recherchée soit proche d'un modèle de référence $\boldsymbol{\mu}$,
% on peut définir
% \[
% \mathcal{R}(\mathbf{x})
% = \| \mathbf{x} - \boldsymbol{\mu} \|_{L^{2},\mathcal{Q}}^{2}
% := \langle \mathbf{x} - \boldsymbol{\mu},
% \mathcal{Q}(\mathbf{x} - \boldsymbol{\mu}) \rangle,
% \]
% où $\mathcal{Q}$ est un opérateur positif définissant une pondération
% directionnelle.

% Bien que cette approche soit mathématiquement simple et numériquement stable,
% elle favorise des solutions lisses et ne permet pas de promouvoir des structures
% parcimonieuses. Dans le contexte de la reconstruction CT à faible dose, elle est
% souvent insuffisante pour préserver les contours et les détails fins.

\medskip
\noindent
Le compressed sensing s'inscrit naturellement dans ce cadre variationnel en
choisissant des régularisants non quadratiques conçus pour promouvoir la
parcimonie ou la compressibilité de l'image, tels que les normes $\ell_1$ ou la
variation totale. Ces choix conduisent à des problèmes d'optimisation
non différentiables, nécessitant des algorithmes itératifs spécifiques, qui
seront abordés dans les sections suivantes.

% ============================================================================================
\subsection{Formulation du problème}

Dans le cadre de la tomographie par rayons X (CT), la reconstruction d'image à partir d'un nombre limité de projections conduit à un problème inverse sous-déterminé. Le cadre du \emph{Compressed Sensing} (CS) permet de résoudre ce problème en exploitant la parcimonie intrinsèque des images CT dans un domaine approprié, typiquement le domaine du gradient.

\begin{definition}[Image et représentation parcimonieuse]
Considérons une image $f$, vue comme un vecteur colonne de dimension $n \times 1$ dans $\mathbb{R}^n$, dont les éléments individuels $f_j$, pour $j = 1, 2, \ldots, n$, représentent les $n$ valeurs de pixels de l'image. On développe le vecteur $f$ dans une base orthonormée $\Psi$ comme suit :
\[
f = \Psi \mathbf{x},
\]
où $\Psi$ est la matrice $n \times n$ $[\boldsymbol{\psi}_1, \ldots, \boldsymbol{\psi}_n]$, dont les vecteurs $\{\boldsymbol{\psi}_i\}_{i=1}^{n}$ constituent les colonnes, et où $\mathbf{x}$ est un vecteur colonne de dimension $n \times 1$.  

Si la majorité des composantes du vecteur $\mathbf{x}$ sont nulles ou quasi nulles, on dira que $f$ est \textbf{parcimonieuse} dans le domaine $\Psi$, et que $\mathbf{x}$ constitue sa \textbf{représentation parcimonieuse}.
\end{definition}

Dans le cas des images CT, la parcimonie ne s'exprime généralement pas directement dans le domaine spatial, mais plutôt dans le domaine du gradient. Les images CT sont en effet caractérisées par des régions quasi homogènes séparées par des discontinuités nettes, ce qui rend leur gradient parcimonieux.

Considérons l'exemple du fantôme de Shepp--Logan représenté à la \Cref{fig:shepp-logan} et de son équivalent en gradient à la \Cref{fig:shepp-logan-gradient}. On note l'intensité d'un pixel d'une image bidimensionnelle par $f_{h,w}$, où $h = 1,2,\ldots,H$ et $w = 1,2,\ldots,W$ ; $H$ et $W$ désignent respectivement la hauteur et la largeur de l'image 2D, et $W \times H = n$.

\begin{definition}[Module du gradient]
Si les valeurs des pixels sont notées $f_{h,w}$, le module du gradient discret est défini comme suit :
\begin{equation}
\left| \nabla f_{h,w} \right|
=
\sqrt{
\left( f_{h+1,w} - f_{h,w} \right)^2
+
\left( f_{h,w+1} - f_{h,w} \right)^2
}.
\label{eq:gradient-modulus}
\end{equation}
\end{definition}

La \emph{variation totale} (Total Variation, TV) de l'image est alors définie comme la somme du module du gradient sur l'ensemble des pixels :
\[
\mathrm{TV}(f) = \sum_{h,w} \left| \nabla f_{h,w} \right|.
\]
La minimisation de la variation totale correspond à la minimisation de la norme $\ell_1$ du gradient et constitue une pénalisation standard dans le cadre du Compressed Sensing appliqué au CT.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth, height=5cm]{./images/shepp-logan phantom.png}
    \caption{Fantôme de Shepp--Logan}
    \label{fig:shepp-logan}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth, height=5cm]{./images/shepp-logan phantom gradient.png}
    \caption{Gradient du fantôme de Shepp--Logan}
    \label{fig:shepp-logan-gradient}
\end{figure}

\begin{proposition}[Modèle d'acquisition en tomographie CT]
En imagerie CT réaliste, les données de projection à faisceau parallèle, également appelées \emph{sinogramme}, sont modélisées par un système linéaire discret :
\begin{equation}
\mathbf{g} = \Phi \mathbf{f},
\end{equation}
où $\mathbf{g} \in \mathbb{R}^m$ est le vecteur des mesures de projection, et $\Phi \in \mathbb{R}^{m \times n}$ est la matrice système décrivant la géométrie d'acquisition CT.
\end{proposition}

En introduisant la représentation parcimonieuse de l'image, le modèle devient :
\begin{equation}
\mathbf{g} = \Phi \mathbf{f} = \Phi \Psi \mathbf{x} = \Phi' \mathbf{x},
\label{eq:4}
\end{equation}
où $\Phi' = \Phi \Psi$.

Lorsque le nombre de projections est limité, on a $m \ll n$, ce qui rend le système sous-déterminé.

\begin{proposition}[Reconstruction CT par Compressed Sensing]
La reconstruction de l'image consiste alors à résoudre le problème d'optimisation suivant :
\begin{equation}
\mathbf{x}
=
\arg\min_{\tilde{\mathbf{x}}}
\left\| \tilde{\mathbf{x}} \right\|_{1}
\quad
\text{sous la contrainte}
\quad
\left\| \Phi^{'}\tilde{\mathbf{x}} - \mathbf{g} \right\|_{2} \leq \varepsilon,
\end{equation}
où $\varepsilon$ modélise le bruit présent dans les mesures.
\end{proposition}

Dans le cas particulier du CT, cette formulation est équivalente à une minimisation de la variation totale de l'image sous contrainte de fidélité aux données.

\section{ADMM (Alternating Direction Method of Multipliers)}
Cette section expose brièvement deux algorithmes d'optimisation précurseurs de ADMM, fournissant ainsi le contexte théorique nécessaire à sa compréhension.

\subsection{Ascension Duale.}
Considérons le problème d'optimisation convexe sous contrainte d'égalité
\begin{equation}
    \begin{aligned}
        & \text{minimiser} && f(x) \\
        & \text{sous contrainte} && Ax = b,
    \end{aligned}
    \label{eq:optimisation_convexe}
\end{equation}

avec la variable \( x \in \mathbb{R}^n \), où \( A \in \mathbb{R}^{m \times n} \) et \( f : \mathbb{R}^n \to \mathbb{R} \) est convexe.
Le Lagrangien pour ce problème est
\[L(x, y) = f(x) + y^T(Ax - b)\]
et la fonction duale est
\[g(y) = \inf_x L(x, y) = -f^*(-A^Ty) - b^Ty,\]
où \( y \) est la variable duale ou le multiplicateur de Lagrange, et \( f^* \) est le conjugué convexe de \( f \).\\
Le problème duale est
\[\text{maximiser } g(y),\]

avec la variable \( y \in \mathbb{R}^m \). En supposant qu'il y a une dualité forte, les valeurs optimales des problèmes primal et dual sont les mêmes. Nous pouvons retrouver un point optimal primal \( x^* \) à partir d'un point optimal dual \( y^* \) comme

\[x^* = \arg\min_x L(x, y^*),\]

à condition qu'il n'y ait qu'un seul minimiseur de \( L(x, y^*) \). (C'est le cas si, par exemple, \( f \) est strictement convexe.) Dans la suite, nous utiliserons la notation \(\arg\min_x F(x)\) pour désigner n'importe quel minimiseur de \( F \), même lorsque \( F \) n'a pas un minimiseur unique.

Dans la méthode d'ascension duale, nous résolvons le problème dual en utilisant l'ascension de gradient. En supposant que \( g \) est différentiable, le gradient \( \nabla g(y) \) peut être évalué comme suit. Nous trouvons d'abord \( x^+ = \arg\min_x L(x, y) \) ; ensuite nous avons \( \nabla g(y) = Ax^+ - b \), qui est le résidu pour la contrainte d'égalité. La méthode d'ascension duale consiste à itérer les mises à jour\\
Étape 1: 
\[x^{k+1} := \arg\min_x L(x, y^k) \]
Étape 2: 
\[y^{k+1} := y^k + \alpha^k (Ax^{k+1} - b),\]

où \( \alpha^k > 0 \) est une taille de pas, et l'exposant est le compteur d'itération. La première étape est une étape de minimisation en \( x \), et la deuxième étape est une mise à jour de la variable duale. La variable duale \( y \) peut être interprétée comme un vecteur de prix, et la mise à jour de \( y \) est alors appelée mise à jour des prix ou étape d'ajustement des prix. Cet algorithme est appelé ascension duale car, avec un choix approprié de \( \alpha^k \), la fonction duale augmente à chaque étape, c'est-à-dire \( g(y^{k+1}) > g(y^k) \).

La méthode d'ascension duale peut être utilisée même dans certains cas où \( g \) n'est pas différentiable. Dans ce cas, le résidu \( Ax^{k+1} - b \) n'est pas le gradient de \( g \), mais l'opposé d'un sous-gradient de \(-g\). Ce cas nécessite un choix différent de \( \alpha^k \) que lorsque \( g \) est différentiable, et la convergence n'est pas monotone ; il arrive souvent que \( g(y^{k+1}) \neq g(y^k) \). Dans ce cas, l'algorithme est habituellement appelé la méthode de sous-gradient dual.

Si \( \alpha^k \) est choisi de manière appropriée et que plusieurs autres hypothèses sont vérifiées, alors \( x^k \) converge vers un point optimal et \( y^k \) converge vers un point dual optimal. Cependant, ces hypothèses ne sont pas vérifiées dans de nombreuses applications, donc l'ascension duale ne peut souvent pas être utilisée. Par exemple, si \( f \) est une fonction affine non nulle d'une composante quelconque de \( x \), alors la mise à jour de \( x \) échoue, car \( L \) n'est pas bornée inférieurement en \( x \) pour la plupart des \( y \).

\subsection{Décomposition Duale.}
L'avantage majeur de la méthode d'ascension duale est qu'elle peut conduire à un algorithme décentralisé dans certains cas. Supposons, par exemple, que l'objectif \( f \) est séparable (par rapport à une partition ou un éclatement de la variable en sous-vecteurs), ce qui signifie que

\[f(x) = \sum_{i=1}^N f_i(x_i),\]

où \( x = (x_1, \ldots, x_N) \) et les variables \( x_i \in \mathbb{R}^{n_i} \) sont des sous-vecteurs de \( x \). En partitionnant la matrice \( A \) de manière conforme comme

\[A = [A_1 \cdots A_N],\]

ainsi \( Ax = \sum_{i=1}^N A_i x_i \), le Lagrangien peut s'écrire comme

\[L(x, y) = \sum_{i=1}^N L_i(x_i, y) = \sum_{i=1}^N \left( f_i(x_i) + y^T A_i x_i - (1/N)y^T b \right),\]

qui est également séparable en \( x \). Cela signifie que l'étape de minimisation en \( x \) se divise en \( N \) problèmes distincts qui peuvent être résolus en parallèle. Explicitement, l'algorithme est
\begin{equation}
    x_i^{k+1} := \arg\min_{x_i} L_i(x_i, y^k) 
    \label{eq:x_minimisation}
\end{equation}

\begin{equation}
    y^{k+1} := y^k + \alpha^k (Ax^{k+1} - b)
    \label{eq:dual_update}
\end{equation}

L'étape de minimisation en \( x \) \eqref{eq:x_minimisation} est effectuée indépendamment, en parallèle, pour chaque \( i = 1, \ldots, N \). Dans ce cas, nous nous référons à la méthode d'ascension duale sous le nom de décomposition duale.

Dans le cas général, chaque itération de la méthode de décomposition duale nécessite une opération de diffusion et une opération de collecte. Dans l'étape de mise à jour duale \eqref{eq:dual_update}, les contributions du résidu de la contrainte d'égalité \( A_i x_i^{k+1} \) sont collectées (rassemblées) afin de calculer le résidu \( Ax^{k+1} - b \). Une fois que la variable duale (globale) \( y^{k+1} \) est calculée, elle doit être distribuée (diffusée) aux processeurs qui effectuent les \( N \) étapes individuelles de minimisation de \( x_i \) \eqref{eq:x_minimisation}.

\subsection{Lagrangiens Augmentés et Méthode des Multiplicateurs}
Les méthodes de Lagrangien augmenté ont été développées en partie pour apporter de la robustesse à la méthode d'ascension duale, et en particulier, pour obtenir une convergence sans hypothèses comme la stricte convexité ou le caractère fini de \( f \). Le Lagrangien augmenté pour \eqref{eq:optimisation_convexe} est

\begin{equation}
L_\rho(x, y) = f(x) + y^T(Ax - b) + (\rho/2)\|Ax - b\|_2^2
    \label{eq:lagrangien_augmente}
\end{equation}

où \( \rho > 0 \) est appelé le paramètre de pénalité. (Notez que \( L_0 \) est le Lagrangien standard pour le problème.) Le Lagrangien augmenté peut être vu comme le Lagrangien (non augmenté) associé au problème

\[\text{minimiser } f(x) + (\rho/2)\|Ax - b\|_2^2\]
\[\text{sous contrainte } Ax = b.\]

Ce problème est clairement équivalent au problème original \eqref{eq:optimisation_convexe}, puisque pour tout \( x \) réalisable, le terme ajouté à l'objectif est nul. La fonction duale associée est \[g_\rho(y) = \inf_x L_\rho(x, y)\].

L'avantage d'inclure le terme de pénalité est que \( g_\rho \) peut être démontrée comme étant différentiable sous des conditions plutôt faibles sur le problème original. Le gradient de la fonction duale augmentée est trouvé de la même manière qu'avec le Lagrangien ordinaire, c'est-à-dire en minimisant sur \( x \), puis en évaluant le résidu de la contrainte d'égalité résultant. En appliquant l'ascension duale au problème modifié, on obtient l'algorithme

\begin{equation}
x^{k+1} := \arg\min_x L_\rho(x, y^k)
\label{eq:methodes_multiplicateurs1}
\end{equation}

\begin{equation}
y^{k+1} := y^k + \rho (Ax^{k+1} - b)
\label{eq:methodes_multiplicateurs2}
\end{equation}

qui est connu sous le nom de méthode des multiplicateurs pour résoudre \eqref{eq:optimisation_convexe}. C'est la même chose que l'ascension duale standard, sauf que l'étape de minimisation en \( x \) utilise le Lagrangien augmenté, et que le paramètre de pénalité \( \rho \) est utilisé comme taille de pas \( \alpha^k \). La méthode des multiplicateurs converge sous des conditions bien plus générales que l'ascension duale, y compris dans les cas où \( f \) prend la valeur \( +\infty \) ou n'est pas strictement convexe.

Il est facile de motiver le choix de la taille de pas particulière \( \rho \) dans la mise à jour duale \eqref{eq:methodes_multiplicateurs2}. Par souci de simplicité, nous supposons ici que \( f \) est différentiable, bien que cela ne soit pas nécessaire pour le bon fonctionnement de l'algorithme. Les conditions d'optimalité pour \eqref{eq:optimisation_convexe} sont la faisabilité primale et duale, c'est-à-dire

\[Ax^* - b = 0, \quad \nabla f(x^*) + A^T y^k = 0,\]

respectivement. Par définition, \( x^{k+1} \) minimise \( L_\rho(x, y^k) \), donc

\[
    \begin{array}{l l l}
        0 &=& \nabla_x L_\rho(x^{k+1}, y^k)\\
        &=& \nabla_x f(x^{k+1}) + A^T \left( y^k + \rho (Ax^{k+1} - b) \right)\\
        &=& \nabla_x f(x^{k+1}) + A^T y^{k+1}
    \end{array}
\]

Nous voyons qu'en utilisant \( \rho \) comme taille de pas dans la mise à jour duale, l'itéré \((x^{k+1}, y^{k+1})\) est dualement réalisable. Au fur et à mesure que la méthode des multiplicateurs progresse, le résidu primal \( Ax^{k+1} - b \) converge vers zéro, ce qui donne l'optimalité.

Les propriétés de convergence grandement améliorées de la méthode des multiplicateurs par rapport à l'ascension duale ont un coût. Lorsque \( f \) est séparable, le Lagrangien augmenté \( L_\rho \) n'est pas séparable, donc l'étape de minimisation en \( x \) \eqref{eq:methodes_multiplicateurs1} ne peut pas être effectuée séparément en parallèle pour chaque \( x_i \). Cela signifie que la méthode de base des multiplicateurs ne peut pas être utilisée pour la décomposition. Nous verrons comment résoudre ce problème ensuite.

\subsection{ Etapes sur l'algorithme ADMM}
L'ADMM est un algorithme conçu pour fusionner la décomposabilité de l'ascension duale avec les propriétés de convergence supérieures de la méthode des multiplicateurs. L'algorithme résout des problèmes de la forme


\begin{equation}
    \begin{aligned}
        & \text{minimiser} && f(x) + g(z)\\
        & \text{sous contrainte} && Ax + Bz = c,
    \end{aligned}
    \label{eq:new_optimisation_convexe}
\end{equation}

avec les variables \( x \in \mathbb{R}^n \) et \( z \in \mathbb{R}^m \), où \( A \in \mathbb{R}^{p \times n} \), \( B \in \mathbb{R}^{p \times m} \), et \( c \in \mathbb{R}^p \). Nous supposerons que \( f \) et \( g \) sont convexes. La seule différence par rapport au problème général sous contrainte d'égalité linéaire \eqref{eq:optimisation_convexe} est que la variable, appelée \( x \) dans ce problème, a été divisée en deux parties, appelées \( x \) et \( z \) ici, avec la fonction objectif séparable selon cette division. La valeur optimale du problème \eqref{eq:new_optimisation_convexe} sera notée par

\[p^\star = \inf \{ f(x) + g(z) | Ax + Bz = c \}.\]

Comme dans la méthode des multiplicateurs, nous formons le Lagrangien augmenté

\[L_\rho(x, z, y) = f(x) + g(z) + y^T(Ax + Bz - c) + (\rho/2) \|Ax + Bz - c\|_2^2.\]


L'ADMM consiste en les itérations suivantes :
\begin{equation}
    x^{k+1} := \arg\min_x L_\rho(x, z^k, y^k)
    \label{eq:x_update}
\end{equation}

\begin{equation}
    z^{k+1} := \arg\min_z L_\rho(x^{k+1}, z, y^k)
    \label{eq:z_update}
\end{equation}

\begin{equation}
    y^{k+1} := y^k + \rho(Ax^{k+1} + Bz^{k+1} - c)
    \label{eq:y_update}
\end{equation}

où \(\rho > 0\). L'algorithme est très similaire à l'ascension duale et à la méthode des multiplicateurs : il consiste en une étape de minimisation en \(x\) \eqref{eq:x_update}, une étape de minimisation en \(z\) \eqref{eq:z_update}, et une mise à jour de la variable duale \eqref{eq:y_update}. Comme dans la méthode des multiplicateurs, la mise à jour de la variable duale utilise une taille de pas égale au paramètre du Lagrangien augmenté \(\rho\).

La méthode des multiplicateurs pour \eqref{eq:new_optimisation_convexe} a la forme
\[
    \begin{array}{l l l}
        (x^{k+1}, z^{k+1}) &:=& \arg\min_{x,z} L_\rho(x, z, y^k)\\
        y^{k+1} &:=& y^k + \rho(Ax^{k+1} + Bz^{k+1} - c)
    \end{array}
\]

Ici, le Lagrangien augmenté est minimisé conjointement par rapport aux deux variables primales. Dans ADMM, en revanche, \(x\) et \(z\) sont mis à jour de manière alternée ou séquentielle, ce qui explique le terme direction alternée. Séparer la minimisation sur \(x\) et \(z\) en deux étapes est précisément ce qui permet la décomposition lorsque \(f\) ou \(g\) sont séparables.\\
L'état de l'algorithme dans ADMM est constitué de \(z^k\) et \(y^k\). En d'autres termes, \((z^{k+1}, y^{k+1})\) est une fonction de \((z^k, y^k)\). La variable \(x^k\) ne fait pas partie de l'état ; c'est un résultat intermédiaire calculé à partir de l'état précédent \((z^{k-1}, y^{k-1})\).

Si nous échangeons (réétiquetons) \(x\) et \(z\), \(f\) et \(g\), et \(A\) et \(B\) dans le problème \eqref{eq:new_optimisation_convexe}, nous obtenons une variante d'ADMM avec l'ordre de l'étape de mise à jour de \(x\) \eqref{eq:x_update} et de l'étape de mise à jour de \(z\) \eqref{eq:z_update} inversé. Les rôles de \(x\) et \(z\) sont presque symétriques, mais pas tout à fait, puisque la mise à jour duale est effectuée après la mise à jour de \(z\) mais avant la mise à jour de \(x\).


\subsubsection{Forme Mise à l'Echelle}
L'ADMM peut être écrit sous une forme légèrement différente, souvent plus pratique, en combinant les termes linéaires et quadratiques dans le Lagrangien augmenté et en mettant à l'échelle la variable duale. En définissant le résidu \( r = Ax + Bz - c \), nous avons

\[
    \begin{array}{l l l}
        y^T r + (\rho/2) \|r\|_2^2 &=& (\rho/2) \|r + (1/\rho)y\|_2^2 - (1/2\rho) \|y\|_2^2\\
        &=& (\rho/2) \|r + u\|_2^2 - (\rho/2) \|u\|_2^2,
    \end{array}
\]

où \( u = (1/\rho)y \) est la variable duale mise à l'échelle. En utilisant la variable duale mise à l'échelle, nous pouvons exprimer l'ADMM comme
\begin{equation}
    x^{k+1} := \arg\min_x \left( f(x) + (\rho/2) \|Ax + Bz^k - c + u^k\|_2^2 \right)
    \label{eq:x_update_scaled}
\end{equation}

\begin{equation}
    z^{k+1} := \arg\min_z \left( g(z) + (\rho/2) \|Ax^{k+1} + Bz - c + u^k\|_2^2 \right)
    \label{eq:z_update_scaled}
\end{equation}

\begin{equation}
    u^{k+1} := u^k + Ax^{k+1} + Bz^{k+1} - c
    \label{eq:u_update_scaled}
\end{equation}

En définissant le résidu à l'itération \( k \) comme \( r^k = Ax^k + Bz^k - c \), nous voyons que
\[u^k = u^0 + \sum_{j=1}^k r^j,\]
la somme cumulée des résidus.\vspace{5pt}\\
Nous appelons la première forme de l'ADMM ci-dessus, donnée par (\eqref{eq:x_update}-\eqref{eq:y_update}), la forme non mise à l'échelle, et la seconde forme (\eqref{eq:x_update_scaled}-\eqref{eq:u_update_scaled}) la forme mise à l'échelle, car elle est exprimée en fonction d'une version mise à l'échelle de la variable duale. Les deux sont clairement équivalentes, mais les formules dans la forme mise à l'échelle de l'ADMM sont souvent plus courtes que dans la forme non mise à l'échelle, donc nous utiliserons la forme mise à l'échelle dans la suite. Nous utiliserons la forme non mise à l'échelle lorsque nous souhaitons souligner le rôle de la variable duale ou donner une interprétation qui repose sur la variable duale (non mise à l'échelle).

% ===================================================================
\section{Transformées en Ondelettes Biorthogonales}

L'analyse et le traitement du signal constituent un pilier fondamental des sciences et technologies modernes. De la compression d'images médicales à la reconnaissance de formes en passant par la transmission de données numériques, les outils mathématiques permettant de représenter, transformer et interpréter les signaux sont au cœur d'innombrables innovations.

Parmi ces outils, la transformée de Fourier a longtemps régné en maître, offrant une représentation fréquentielle des signaux qui a révolutionné notre compréhension des phénomènes physiques. Pourtant, cette représentation présente une limitation majeure : elle perd toute information temporelle. Pour un signal variant dans le temps, savoir quelles fréquences le composent sans savoir à quel moment elles apparaissent revient à lire une partition sans connaître le tempo.

C'est dans ce contexte que les ondelettes ont émergé comme une alternative puissante et élégante. Véritables fenêtres mathématiques, elles permettent une analyse conjointe dans les domaines temporel et fréquentiel, offrant ainsi une résolution adaptative qui fait défaut à la transformée de Fourier classique.

Les ondelettes sont utiles dans diverses applications telles que la sismologie, l'analyse et le traitement du signal. Il est difficile de représenter efficacement des signaux avec une petite valeur de fonction calculable. Généralement, les ondelettes sont liées au filtrage. Il existe certaines similitudes entre les évaluations multi-résolution en temps continu, tandis que le banc de filtres signifie un filtrage sous-bande en temps discret. L'objectif principal du traitement des données est de générer des fonctions basées sur la construction de l'ensemble. Cela permet une description incisive, efficace et explicative du signal. La sélection du signal de base dépend du signal. Les exponentielles complexes de Fourier sont disponibles pour les signaux lisses, mais leurs performances sont médiocres pour les signaux discontinus dans les zones de hautes et basses fréquences. Les ondelettes ne sont rien d'autre qu'une petite fenêtre facile à traiter. Par conséquent, récemment, la transformation en ondelettes a joué un rôle important.

Dans de nombreux cas, la transformée en ondelettes joue un rôle important. Différentes familles d'ondelettes sont disponibles, telles que les ondelettes orthogonales, biorthogonales et biorthogonales inverses, l'ondelette de Daubechies, l'ondelette de Haar, les Symlets, les Coifflets et l'ondelette de Meyer discrète. Les familles d'ondelettes orthogonales et biorthogonales démontrent une propriété de phase linéaire utile dans la reconstruction des données.


\subsection{Fondements Théoriques des Ondelettes}
Avant de comparer leurs performances, posons les fondations mathématiques sur lesquelles repose l'édifice des ondelettes \cite{17}.
% \subsubsection{ Concepts Fondamentaux}

\paragraph{Orthogonalité : } L'orthogonalité est un aspect important lié au traitement du signal car c'est une méthode facile dans les langages pour construire des structures de contrôle et de données. Sa nature est simple, elle a donc de nombreuses conceptions orthogonales avec moins d'attentes. Grâce à cela, nous sommes capables de lire, d'écrire et d'apprendre n'importe quel aspect d'un langage de programmation. Les significations des caractéristiques de l'orthogonalité sont indépendantes pour chaque contexte \cite{18}. L'orthogonalité reflète les angles droits, c'est-à-dire \(90^{\circ}\).\\Dans le modèle mathématique, l'orthogonalité décrit la perpendicularité pour l'algèbre linéaire avec des formes bilinéaires. Il y a deux éléments, x et y, qui sont orthogonaux. Si B(x, y) = angle de 90 degrés. L'espace vectoriel a des vecteurs non nuls, qui sont auto-orthogonaux. La famille de fonctions orthogonales est utile dans la formation de la base.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{./images/orthogonality.png}
    \caption{Orthogonalité}
    \label{fig:vector_orthogonal}
\end{figure}
% <center>Figure 1.1. Orthogonalité </center>

\paragraph{Signal Orthogonal : } Le signal orthogonal est formé à l'aide du filtre miroir conjugué. Le filtre miroir divise l'axe des fréquences en différents intervalles. Ces intervalles ont des tailles différentes. Le banc de filtres conjugués correspond à diverses bases de paquets. Il est nécessaire d'isoler les petites fenêtres, c'est-à-dire la fenêtre d'ondelettes en présence du système variant dans le temps, car elle est responsable de la modification des propriétés du signal. Généralement, les bases cosinus locales sont obtenues par la simple multiplication des fenêtres et de la fonction cosinus. Ici, un paquet d'ondelettes divise l'axe des fréquences et est systématiquement transcrit dans le temps. Les bases cosinus locales divisent l'axe du temps et sont systématiquement transcrites en fréquence.

En fait, la condition linéaire et quadratique n'est pas suffisante pour la production d'un système orthogonal. La fonction \(\psi (x)\) fournit le système orthogonal. Cela signifie que les \(\psi_{j,k}\) comprennent la base orthonormale lorsqu'ils satisfont à la condition ci-dessous :

\begin{equation}
    \int_{-\infty}^{\infty}\theta (x)\theta (x - k)dx = \delta_{0,k}
    \label{eq:orthogonality_condition}
\end{equation}
Cette condition donne la condition équivalente pour l'orthogonalité.
\paragraph{Transformation en Ondelettes : } Une ondelette est l'une des fonctions mathématiques utiles pour diviser un signal temporel continu ou une fonction en différentes composantes d'échelle. Fondamentalement, les ondelettes sont des oscillations qui ont une certaine valeur d'amplitude. Elles commencent à zéro, puis augmentent, et diminuent à nouveau pour revenir à 0. Cela ressemble aux brèves oscillations qui se produisent sur un sismographe.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{./images/wt.png}
    \caption{Transformation en ondelettes}
    \label{fig:wavelet_transformation}
\end{figure}
% <center>Figure 1.2. Transformation en ondelettes </center>

Une plage de fréquences est assignée à chaque composante d'échelle. L'étude de chaque composante porte une résolution qui correspond à l'échelle. L'ondelette est utilisée pour diverses applications. Elle utilise la convolution. Les ondelettes sont très utiles pour obtenir la partie inconnue. Elle est combinée avec la partie connue d'un signal détruit pour récupérer les données des parties inconnues.\\
Fondamentalement, la transformée en ondelettes entre en jeu au moment où le signal est analysé, à la fois dans les domaines temporel et fréquentiel. Elle est surtout applicable au système variant dans le temps car elle offre une résolution de fréquence variable à la fin du processus de décomposition, en alternant STFT (Voir Equation \eqref{eq:stft}) et Fourier. Ici, la STFT supporte la transformée de Fourier pour le temps instantané t avec une fenêtre courte ayant un point médian au temps t. La longueur de la fenêtre est constante pour la STFT, donc elle ne supporte pas l'anatomie multi-résolution des signaux. L'ondelette est différente en ce sens qu'elle fournit à la fois une résolution temporelle et fréquentielle.\\
La fonction de transformation est appelée ondelette \textit{Mère}. Le nom d'ondelette est dérivé des bébés ondes. Ces bébés ondes sont appelées une fonction oscillatoire. Le nom \textit{Mère} entre en jeu car ces ondelettes sont formées à partir de diverses transformations de base.\\
Considérons l'ondelette mère, qui est une fonction fenêtrée, qui est décalée avec le signal temporel où le temps varie de 0 à T. Les coefficients d'ondelette sont calculés par un processus simple. Après avoir multiplié une partie/morceau du signal dans la fenêtre et l'ondelette mère, leur intégration sur le temps complet aboutit à la génération du coefficient d'ondelette. Techniquement, la fenêtre de l'ondelette mère est aussi appelée la fenêtre de support. En raison de l'apparition d'un problème de résolution lié au temps et à la fréquence, la croyance complète en les ondelettes émane de la transformée de Fourier.

Une ondelette mère \(\psi (t)\) est fondamentalement la fonction de moyenne 0. Mathématiquement, elle est représentée par :
\begin{equation}
    \int_{-\infty}^{\infty}\psi (t)dt = 0 \quad (4)
    \label{eq:mean_zero_wt}
\end{equation}
La fonction ci-dessus est mise à l'échelle par $\mathbf{a}$ puis translatée vers un second scalaire $\mathbf{b}$. Alors son ondelette mère est représentée par :

\[\psi_{a,T}(t) = \frac{1}{\sqrt{a}}\psi \left(\frac{t - T}{a}\right) \quad (5)\]

La transformée en ondelettes W(a, T) est représentée par :

\begin{equation}
    W_{x}(a,T) = \int_{-\infty}^{\infty}y(t)\frac{1}{\sqrt{a}}\Psi^{n}\left(\frac{t - T}{a}\right)dt \quad (6)
    \label{eq:wt_func}
\end{equation}

où :
\begin{itemize}
    \item[-] \(\mathrm{T} =\) Point quelconque dans le temps
    \item[-] \(\Psi^{*}(a,T)(t) =\) fonction ondelette
    \item[-] y(t) \(=\) entrée
\end{itemize}
Nous pouvons concevoir n'importe quelle base d'ondelettes orthogonale en utilisant le filtre miroir conjugué \(\hat{h} (\omega)\) :

\begin{equation}
    \left|H_{\theta}(\omega)\right|^{2} + \left|H_{\theta}(\omega +\pi)\right|^{2} = 2 \quad (7)
    \label{eq:mirror_filter}
\end{equation}
\begin{equation}
    H_{\theta}(0) = \sqrt{2} \quad (8)
    \label{eq:mirror_filter_zero}
\end{equation}
Le filtre qui satisfait la relation ci-dessus est appelé filtre miroir conjugué \cite{19}. Un filtre miroir conjugué est l'un des moyens les plus simples pour la construction d'une base d'ondelettes orthogonale. Le filtre miroir conjugué et la multi-résolution sont les méthodes de base pour la base d'ondelettes orthogonale \cite{20}\vspace{7pt}\\
% \paragraph{Propriétés des Ondelettes}
La mise à l'échelle, le décalage, la remise à l'échelle, les moments disparaissants, le support, la régularité et la symétrie sont les propriétés de base de la transformation en ondelettes. L'approximation, le support compact, la symétrie, la régularité et les bases inconditionnelles sont quelques propriétés du système d'ondelettes biorthogonal. Le filtre d'échelle est utile pour déterminer la fonction d'échelle. Chaque filtre d'ondelette est décomposé en procédures de lifting, qui sont présentées par Sweldens et Daubechies.

La représentation de la décomposition d'une ondelette biorthogonale avec la forme du schéma de lifting est représentée comme suit :

\[
    \begin{array}{l}
        {d^{1} = s_{2j} - \frac{1}{2}\left[s_{2j - 1} + s_{2j + 2}\right]}\\
        {}\\
        {s^{1}=s_{2j - 1} + \frac{1}{4}\left[d_{j - 1}^{1} + d^{(1)}\right]}\\
        {}\\ 
        {D = \frac{1}{\sqrt{2}} d^{(1)}}\\ {}\\ {s_{1} = \sqrt{2} s^{(1)}} 
        
    \end{array} 
\]

À partir de l'équation mathématique ci-dessus, nous obtenons le nouveau signal \[\mathrm{s} = [\mathrm{s}_1\mathrm{D}]\]
% \subsubsection{Conclusion}
% Ce introductif a posé les fondements théoriques nécessaires à la compréhension des ondelettes. Nous avons vu comment la transformation en ondelettes dépasse les limitations de l'analyse de Fourier en offrant une représentation temps-fréquence adaptative. Les concepts d'orthogonalité, de fonction d'échelle et d'ondelette mère constituent la base sur laquelle nous allons maintenant construire notre étude comparative.



\subsection{Ondelettes Orthogonales : Théorie et Propriétés}
% \subsubsection{Introduction}

L'ondelette orthogonale est liée à la transformée en ondelettes, et elles sont orthogonales entre elles. L'orthogonalité évite les interférences afin qu'une sortie sans erreur puisse être obtenue. Grâce à ces caractéristiques, elle est le plus souvent préférable. Une ondelette orthogonale est l'adjoint de la transformée en ondelettes. Une fonction d'échelle unique avec une ondelette unique est le résultat d'une ondelette orthogonale.\vspace{7pt}\\
% \subsubsection{Définition et Caractérisation}
Une ondelette orthogonale génère une ondelette dont la nature est orthogonale. Cela signifie qu'une transformée en ondelettes inverse ou transposée n'est rien d'autre que l'adjoint d'une transformée en ondelettes.
% Une ondelette de Daubechies a une réponse en phase linéaire par rapport à un filtre orthogonal.

\subsubsection{Exemples d'Ondelettes Orthogonales}
\paragraph{L'Ondelette de Daubechies : } appartient à la catégorie des ondelettes orthogonales. Un nombre plus élevé de moments disparaissants pour une longueur prédéfinie est une caractéristique de cette ondelette. Le nombre de moments est égal à la moitié de la longueur du support. Le nom de l'ondelette représente dbN, où N indique l'ordre de l'ondelette. N varie de 1 à 8. La figure ci-dessous montre l'ondelette de Daubechies pour différentes valeurs de N allant de 2 à 10.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{./images/Daubechies.png}
    \caption{Ondelette de Daubechies pour différentes valeurs}
    \label{fig:daubechies}
\end{figure}
% <center>Figure 2.1. Ondelette de Daubechies pour différentes valeurs </center>

\paragraph{ L'Ondelette de Haar : } a été étudiée par Alfred Haar et est considérée comme la première ondelette mère. Elle est célèbre car elle supporte la longueur la plus courte parmi toutes les ondelettes. Elle échoue en cas de reconstruction d'une fonction lisse car elle a un seul mouvement disparaissant. En tant que concept simple avec une réponse plus rapide, un moyen plus facile de détecter les informations et une utilisation efficace de la mémoire, l'ondelette de Haar est plus utile que les autres ondelettes. Elle est discontinue et similaire à une fonction échelon. Le signal divisé en deux sous-signaux de demi-longueur est décomposé par l'ondelette de Haar. Elle représente la fonction d'ondelette Daubechies 1.\vspace{7pt}\\
Ici, nous définissons la fonction d'ondelette de Haar comme suit :

\begin{equation}
    \psi^{Haar}(x) = \left\{ \begin{array}{ll}1, & 0\leq x\leq \frac{1}{2}\\ -1, & \frac{1}{2}\leq x\leq 1\\ 0, & ailleurs \end{array} \right. 
\end{equation}
À partir de cela, nous obtenons la fonction suivante :
\begin{equation}
    \psi^{Haar}_{j,k}(x) = 2^{j / 2}\psi^{Haar}(2^{j}x - k) = \left\{ \begin{array}{ll}1, & \frac{1}{2}\leq x\leq \frac{k + 1 / 2}{2}\\ -1, & \frac{k + 1 / 2}{2}\leq x\leq \frac{k + 1}{2}\\ 0, & ailleurs, \end{array} \right. 
\end{equation}
Un \(\phi\) (t) est une fonction d'échelle dans le temps t, qui est nommée l'ondelette père (fonction d'échelle, caractérise l'échelle de base de l'ondelette) :

\begin{equation}
    \phi_{j,k}(t) = 2^{j / 2}\phi (2^{j}t - k) 
    % \quad (16)
\end{equation}

où \(\mathrm{j} =\) paramètre d'échelle; \(\mathrm{k} =\) paramètre de translation; \(\mathrm{j}\) et \(\mathrm{k}\) appartiennent à S. S est un ensemble d'entiers.

La translation et l'échelle sont responsables de la génération d'une famille de fonctions, et à l'aide de la dilatation, peuvent former l'équation suivante :
\begin{equation}
    \phi (t) = \sqrt{2\sum_{n}h_{n}\phi (2t - n)} 
\end{equation}

Ici, \(h_n\) est l'ensemble des coefficients du filtre et \(\sqrt{2}\) est un facteur d'échelle. L'existence de l'ondelette mère (fonction ondelette, caractérise la forme de base de l'ondelette) est représentée par :

\begin{equation}
    \psi_{j,k}(t) = 2^{j / 2}\psi (2^{j}t - k) 
\end{equation}


Ceci est responsable de la formation de la base orthogonale pour \(L^2 R\). Ici, \(\psi (t)\) représente la somme pondérée des \(\psi (2t)\) décalés :
\begin{equation}
    \psi (t) = \sqrt{2\sum_{n}g_{n}\phi (2t - n)} 
\end{equation}
La partie ondelette nécessite un espace complémentaire orthogonal. L'orthogonalité nécessite le filtre d'ondelette et les coefficients d'échelle, qui sont liés entre eux par :

\begin{equation}
    g_{n} = (-1)^{n}h_{1 - n} 
\end{equation}
Ici, la paire de \(g_{n}\) et \(h_{n}\) est appelée un filtre miroir en quadrature passe-haut et passe-bas.

\paragraph{Avantages et Limitations.}
Les ondelettes orthogonales offrent une représentation efficace et sans redondance des signaux. Cependant, de fortes limitations apparaissent au moment de la construction de l'ondelette, en raison de la propriété d'orthogonalité. Il est difficile de chercher ou de trouver des ondelettes qui soient symétriques, à support compact et orthogonales.


\subsection{Ondelettes Biorthogonales}
Dans certains cas, lorsque la condition d'orthogonalité échoue, cela donne lieu à des ondelettes biorthogonales. À ce stade, les ondelettes biorthogonales se forment et l'orthogonalité des ondelettes disparaît.\vspace{7pt}\\
% \subsubsection{ De l'Orthogonalité à la Biorthogonalité}
Fondamentalement, l'ondelette biorthogonale est le résultat de la généralisation de l'orthogonale classique. Une ondelette biorthogonale associée à la transformation en ondelettes est inversible. Si elle est inversible, elle n'a pas besoin d'être orthogonale. Dans le cas de la conception d'ondelettes orthogonales, l'ondelette biorthogonale permet une liberté maximale. Elle supporte également la construction de fonctions d'ondelettes symétriques.

Une ondelette biorthogonale génère une ondelette et une fonction d'échelle pour la décomposition et une paire supplémentaire pour la reconstruction. Deux fonctions d'échelle sont responsables de la génération de diverses analyses multi-résolution.\vspace{7pt}\\
% \subsubsection{Formulation Mathématique}
Ici, la fonction d'échelle duale est représentée mathématiquement comme suit :

\begin{equation}
    \tilde{\phi} (t) = \sqrt{2}\sum_{n}\tilde{h}_{n}\tilde{\phi} (2t - n) 
\end{equation}
De même, l'ondelette duale est représentée par :

\begin{equation}
    \tilde{\psi} (t) = \sqrt{2}\sum_{n}\tilde{g}_{n}\tilde{\phi} (2t - n) 
\end{equation}
Maintenant, l'échelle duale est liée comme suit :

\begin{equation}
    \begin{array}{l}{g_{n} = (-1)^{n}h_{1 - n}}\\ {\tilde{g}_{n} = (-1)^{n}h_{1 - n}} \end{array} 
\end{equation}

\subsubsection{Construction par le Schéma de Lifting. }
W. Sweldens a introduit la construction de l'ondelette biorthogonale en utilisant le schéma de lifting. Fondamentalement, le lifting est une méthode simple utile pour augmenter les moments disparaissants pour les ondelettes duales.

La naissance du schéma de lifting vient de Cohen et al. et Swelden \cite{21}. Nous avons supposé que l'ensemble initial \(\{h,\tilde{h}^0,g^0,\tilde{g}\}\) concernait des ondelettes biorthogonales finies. Ainsi, nous avons rassemblé un nouvel ensemble \(\{h,\tilde{h},g,\tilde{g}\}\) , qui consiste en un filtre biorthogonal fini.

\begin{equation}
    \begin{array}{l}\tilde{h} (\omega) = \tilde{h}^0 (\omega) + \tilde{g} (\omega)\overline{s(2\omega)} \\ \tilde{g} (\omega) = g^0 (\omega) + h(\omega)s2(\omega) \end{array} 
\end{equation}
D'après les équations ci-dessus, il est clair que les filtres d'échelle et d'ondelette duale ne sont pas modifiés. Par conséquent, les fonctions biorthogonales sont dérivées des filtres ci-dessus. L'ensemble \(\{\phi ,\tilde{\phi}^0,\psi^0,\tilde{\psi}^0\}\) est l'ensemble initial, qui contient la fonction d'échelle biorthogonale. Le nouvel ensemble obtenu est \(\{\phi ,\tilde{\phi},\psi ,\tilde{\psi}\}\) . Ainsi, l'ondelette biorthogonale a été obtenue comme suit :

\begin{equation}
    \psi (x) = \psi^0 (x) - \sum_k s_k\phi (x - k) 
\end{equation}

\begin{equation}
    \tilde{\phi} (x) = 2\sum_k\tilde{h}_k^0\phi (2x - k) + \sum_k s_{-k}\tilde{\psi} (x - k) 
\end{equation}

\begin{equation}
    \tilde{\psi} (x) = 2\sum_k\tilde{g}_k\tilde{\phi} (2x - k)
\end{equation}

Ici \(s_k\) peut être choisi aléatoirement. La valeur de s est responsable de la manipulation des fonctions duales et des ondelettes dérivées d'une fonction d'échelle simple.

\subsubsection{Propriétés des Ondelettes Biorthogonales}
Une ondelette biorthogonale permet deux analyses multi-résolution : \(V_{j},W_{j},\phi_{j,k},\psi_{j,k}\) et \(\tilde{V}_{j},\tilde{W}_{j},\tilde{\phi}_{j,k},\tilde{\psi}_{j,k}\). La dilatation et la translation des fonctions d'échelle, \((\tilde{\phi}_{j,k}(x))\) et \((\tilde{\psi}_{j,k}(x))\) constituent la base pour \(\tilde{V}_{j}\) et \(\tilde{W}_{j}\), respectivement. La biorthogonalité satisfait les propriétés suivantes :

\begin{equation}
    \begin{array}{l}
        \tilde{V}_j\perp W_j\\ V_j\perp \tilde{W}_j\\ 
        \langle\tilde{\phi}_{j,l},\phi_{j,l'}\rangle = \delta_{l,l'}\\
        \langle\tilde{\psi}_{j,l},\psi_{j,l'}\rangle = \delta_{j,j'}\delta_{l,l'}\\
        \langle \tilde{\phi}_{j,l},\psi_{j,l'} \rangle = 0\quad et \quad \langle \tilde{\psi}_{j,l},\phi_{j,l'}\rangle = 0. 
    \end{array} 
\end{equation}

La caractéristique la plus significative de la biorthogonalité est la reconstruction parfaite.

\subsubsection{Comparaison avec les Ondelettes Orthogonales}

Une ondelette biorthogonale est plus avancée qu'une ondelette orthogonale. Cependant, elle nécessite plus d'efforts de calcul et est un processus plus long. Une variation de longueur est la principale différence entre l'ondelette orthogonale et l'ondelette biorthogonale lors de l'analyse, ainsi que dans le processus de synthèse. Dans une ondelette orthogonale, des filtres de même longueur sont utilisés et dans l'ondelette biorthogonale, des filtres de longueurs différentes sont utilisés.

La différence entre les ondelettes orthogonales et biorthogonales est qu'une ondelette orthogonale fournit une matrice orthogonale avec transformation unitaire, tandis qu'une ondelette biorthogonale donne une matrice inversible, c'est-à-dire une matrice inverse avec reconstruction parfaite. Dans le cas d'un filtre d'ondelette biorthogonal, le filtre passe-bas (LPF) et le filtre passe-haut (HPF) ont des longueurs différentes. Le filtre passe-bas démontre la symétrie, tandis que le filtre passe-haut est asymétrique ou symétrique.

\subsection{Indicateurs permettant d'évaluer la qualité de la reconstruction}

Dans le domaine du traitement d'images et de la compression de données, il est crucial de pouvoir mesurer objectivement la qualité d'une image reconstruite (ou décompressée) par rapport à son original. Ces mesures permettent de quantifier la fidélité de la reconstruction et l'information perdue (la distorsion) lors du processus. Cette section détaille trois métriques fondamentales, dont les définitions sont présentées dans le document source (image jointe).

\subsubsection{Mean Squared Error (Erreur Quadratique Moyenne)}

L'Erreur Quadratique Moyenne, ou Mean Squared Error (MSE), est la métrique la plus simple et la plus courante pour évaluer la différence entre deux images. Elle quantifie l'énergie de l'erreur pixel par pixel.

Comme indiqué dans le document [\cite{22}, \cite{23}], sa définition mathématique est la suivante :

\[
MSE = \frac{1}{P \times Q} \sum_{i=0}^{P-1} \sum_{j=0}^{Q-1} (I_{ij} - \hat{T}_{ij})^2
\]

Où :
\begin{itemize}
\item[-] \(MSE\) : désigne l'Erreur Quadratique Moyenne.
\item[-] \(P\) : désigne le nombre de lignes (hauteur) de l'image.
\item[-] \(Q\) : désigne le nombre de colonnes (largeur) de l'image.
\item[-] \(I_{ij}\) : désigne la valeur du pixel à la position \((i,j)\) dans l'image originale.
\item[-] \(\hat{T}_{ij}\) : désigne la valeur du pixel à la position \((i,j)\) dans l'image reconstruite (ou décompressée).
\end{itemize}

\paragraph{Interprétation :}
\begin{itemize}
    \item[-] Le calcul consiste à faire la différence entre chaque pixel de l'original et de la reconstruction, à élever cette différence au carré (pour éviter les compensations entre erreurs positives et négatives, et pour pénaliser plus lourdement les grandes erreurs), puis à en faire la moyenne sur toute l'image.
    \item[-] Une valeur de MSE = 0 signifie que les deux images sont identiques (reconstruction parfaite).
    \item[-] Plus la valeur de la MSE est élevée, plus la différence entre l'original et la reconstruction est grande, indiquant une perte de qualité significative.
\end{itemize}
\subsubsection{Peak Signal-to-Noise Ratio (Rapport Signal sur Bruit de Crête)}

Le Rapport Signal sur Bruit de Crête, ou Peak Signal-to-Noise Ratio (PSNR), est une métrique dérivée de la MSE. Elle est très populaire car elle exprime la qualité en décibels (dB), une échelle souvent plus intuitive pour les ingénieurs. Le PSNR met en relation la puissance maximale possible d'un signal (l'image) avec la puissance du bruit (l'erreur) qui dégrade sa qualité.

La formule donnée dans le document [\cite{24}, \cite{25}] est la suivante :

\[
PSNR = 10 \log_{10} \left( \frac{65025}{MSE} \right)
\]

Où :
\begin{itemize}
\item[-] \(PSNR\) : désigne le Rapport Signal sur Bruit de Crête.
\item[-] \(MSE\) : désigne l'Erreur Quadratique Moyenne calculée en (3).
\end{itemize}

Détail important : La valeur 65025 dans la formule est spécifique aux images en niveaux de gris codées sur 8 bits. Dans ce codage, la valeur maximale qu'un pixel peut prendre est \(2^8 - 1 = 255\). Le terme 65025 est simplement le carré de cette valeur maximale (\(255^2\)). Plus généralement, la formule s'écrit \( PSNR = 10 \log_{10} \left( \frac{MAX^2}{MSE} \right) \), où MAX est la valeur dynamique maximale des pixels.

\paragraph{Interprétation :}
\begin{itemize}
    \item Le PSNR est inversement proportionnel à la MSE. Une MSE faible donnera un PSNR élevé, et vice-versa.
    \item Une reconstruction parfaite (MSE = 0) donne un PSNR théoriquement infini.
    \item En pratique, pour une image 8 bits :
    \begin{itemize}
        \item Un PSNR > 40 dB est généralement considéré comme une excellente qualité (très proche de l'original).
        \item Un PSNR entre 30 et 40 dB est souvent jugé comme une bonne qualité (les pertes sont visibles mais acceptables).
        \item Un PSNR < 20 dB indique une très mauvaise qualité.
    \end{itemize}
\end{itemize}

\subsubsection{Structural Similarity Index (Indice de Similarité Structurelle)}

Bien que très utilisées, la MSE et le PSNR sont des métriques purement numériques basées sur les différences de valeurs de pixels. Elles ne correspondent pas toujours à la perception visuelle humaine. L'Indice de Similarité Structurelle, ou Structural Similarity Index (SSIM), a été développé pour pallier ce problème en se concentrant sur la perception de l'information structurelle d'une scène.

Le document [\cite{26}, \cite{27}] en fournit la définition :

\[
SSIM(x,y) = \frac{(2\mu_x\mu_y + C_1)(2\sigma_{xy} + C_2)}{(\mu_x^2 + \mu_y^2 + C_1)(\sigma_x^2 + \sigma_y^2 + C_2)}
\]

Où, pour une fenêtre de l'image \(x\) (original) et \(y\) (reconstruit) :
\begin{itemize}
    \item[-] \(\mu_x, \mu_y\) : sont les moyennes des intensités des pixels dans les fenêtres \(x\) et \(y\). Elles estiment la luminance.
    \item[-] \(\sigma_x^2, \sigma_y^2\) : sont les variances des intensités. Elles estiment le contraste.
    \item[-] \(\sigma_{xy}\) : est la covariance entre \(x\) et \(y\). Elle mesure la corrélation ou la similarité structurelle (comment les motifs varient ensemble).
    \item[-] \(C_1, C_2\) : sont deux constantes servant à stabiliser la division lorsque les dénominateurs sont faibles.
\end{itemize}

\paragraph{Interprétation :}
\begin{itemize}
    \item[-] Le SSIM est calculé localement sur différentes fenêtres de l'image, puis la moyenne de ces valeurs donne un score global.
    \item[-] La valeur du SSIM est comprise entre -1 et 1.
    \item[-] Une valeur de 1 indique une similarité parfaite (les deux images sont identiques).
    \item[-] Plus la valeur est proche de 1, meilleure est la qualité perçue. Une valeur de 0,97 ou 0,98 indique une très haute similarité structurelle, correspondant mieux à la perception humaine qu'un simple calcul d'erreur quadratique.
\end{itemize}

\subsection{Résolution du problème inverse CT par régularisation parcimonieuse : une approche ADMM associée aux ondelettes biorthogonales 4.4}
La reconstruction tomographique à partir de données incomplètes ou bruitées constitue un problème inverse mal posé. Pour y remédier, une approche classique consiste à introduire un a priori sur la solution dans un cadre variationnel. Nous détaillons ici la stratégie d'optimisation retenue, fondée sur une régularisation favorisant la parcimonie des coefficients d'ondelettes.

\subsubsection{Formulation variationnelle du problème de reconstruction CT régularisée}
Le problème de reconstruction d'image $x \in \mathbb{R}^n$ à partir de mesures sinographiques $y \in \mathbb{R}^m$ peut s'écrire sous la forme du problème d'optimisation suivant :
\begin{equation}
\label{eq:problem_initial}
\min_{x} \frac{1}{2}\left\| \mathcal{A}\mathbf{x} - \mathbf{y} \right\|_{2}^{2} + \lambda \left\|Wx\right\|_{1},
\end{equation}
où:
\begin{itemize}
    \item \(\mathcal{A} : \mathbb{R}^n \to \mathbb{R}^m\) est l'opérateur de projection modélisant l'acquisition CT (transformée de Radon discrète ou matrice système) ;
    \item \(\mathbf{y} \in \mathbb{R}^m\) représente les données mesurées (sinogramme) ;
    \item \(\lambda > 0\) est le paramètre de régularisation contrôlant le compromis entre la fidélité aux données et le terme a priori ;
    \item \(W : \mathbb{R}^n \to \mathbb{R}^n\) désigne la transformée en ondelettes biorthogonales \texttt{bior4.4} (définie par les filtres d'analyse) ;
    \item \(\|\cdot\|_1\) est la norme \(\ell_1\) qui, appliquée aux coefficients d'ondelettes \(Wx\), promeut la parcimonie.
\end{itemize}

La minimisation directe du problème \eqref{eq:problem_initial} est rendue complexe par la présence de la norme \(\ell_1\), un terme non-différentiable. Cette caractéristique exclut l'utilisation de méthodes d'optimisation simples basées uniquement sur le gradient et empêche l'obtention d'une solution sous forme analytique explicite. Il est donc nécessaire de recourir à des algorithmes itératifs plus sophistiqués, capables de traiter efficacement ce type de fonction objectif composite.

\subsubsection{Séparation de variables et algorithme ADMM}
Pour surmonter la difficulté liée à la non-différentiabilité, nous introduisons l'algorithme du Multiplicateur à Direction Alternée (ADMM). La première étape consiste à reformuler le problème en séparant les termes quadratique et non-différentiable via l'introduction d'une variable auxiliaire \(z\). En posant \(z = Wx\), le problème \eqref{eq:problem_initial} devient un problème d'optimisation sous contrainte d'égalité :
\[
\min_{x,z} \frac{1}{2}\left\| \mathcal{A}\mathbf{x} - \mathbf{y} \right\|_{2}^{2} + \lambda \left\|z\right\|_{1} \quad \text{tel que} \quad Wx - z = 0.
\]
Cette formulation est au cœur de l'ADMM : elle découple la difficulté du problème initial. La variable \(x\) n'intervient désormais que dans le terme d'attache aux données (quadratique), tandis que la variable \(z\) supporte seule le terme de régularisation non-différentiable (\(\ell_1\)).

\subsubsection{Lagrangien augmenté et mises à jour itératives}
L'algorithme ADMM résout le problème contraint en minimisant le Lagrangien augmenté associé. Introduisons la variable duale mise à l'échelle \(u\) et le paramètre de pénalité \(\rho > 0\). Le Lagrangien augmenté s'écrit :
\[
L_\rho(x, z, u) = \frac{1}{2}\left\| \mathcal{A}\mathbf{x} - \mathbf{y} \right\|_{2}^{2} + \lambda \left\|z\right\|_{1} + \frac{\rho}{2}\left\| Wx - z + u \right\|_{2}^{2}.
\]
Le terme quadratique supplémentaire, pondéré par \(\rho\), pénalise l'écart à la contrainte \(Wx = z\) et rend le sous-problème en \(x\) strictement convexe, facilitant ainsi sa résolution. Le paramètre \(\rho\) joue un rôle crucial dans la vitesse de convergence de l'algorithme en contrôlant le pas de la mise à jour duale. L'optimisation se fait alors de manière itérative et alternée sur les blocs de variables \(x\), \(z\), et \(u\).

\paragraph{Mise à jour de \(x\) (sous-problème quadratique)}
À l'itération \(k\), les variables \(z^k\) et \(u^k\) étant fixées, la mise à jour de \(x\) consiste à résoudre le problème de minimisation quadratique suivant :
\[
x^{k+1} = \arg\min_{x} \frac{1}{2}\left\| \mathcal{A}\mathbf{x} - \mathbf{y} \right\|_{2}^{2} + \frac{\rho}{2}\left\| Wx - z^{k} + u^{k} \right\|_{2}^{2}.
\]
Ce problème admet une solution unique obtenue en annulant le gradient. En développant les deux normes quadratiques et en calculant le gradient par rapport à \(x\), on obtient le système linéaire suivant :
\[
\left(A^T A + \rho W^T W\right)x = A^T y + \rho W^T \left(z^k - u^k\right),
\]
où \(A\) est la représentation matricielle de l'opérateur \(\mathcal{A}\). La matrice \(A^T A\) est symétrique semi-définie positive, typiquement mal conditionnée en tomographie. L'ajout du terme \(\rho W^T W\), où \(W^T W\) est définie positive (car la transformée en ondelettes est bi-orthogonale et surjective), améliore le conditionnement du système, le rendant définie positif et bien posé. La résolution de ce système linéaire à chaque itération est effectuée de manière efficace par l'algorithme du gradient conjugué \cite{28}, en exploitant la structure particulière des opérateurs.

\paragraph{Mise à jour de \(z\) (seuillage doux)}
La variable \(z\) est ensuite mise à jour en fixant \(x^{k+1}\) et \(u^{k}\). Le problème à résoudre est :
\[
z^{k+1} = \arg\min_{z} \lambda \left\|z\right\|_{1} + \frac{\rho}{2}\left\| z - \left(Wx^{k+1} + u^{k}\right) \right\|_{2}^{2}.
\]
Cette formulation correspond exactement à l'opérateur proximal \cite{29} de la norme \(\ell_1\), dont la solution est donnée par l'opérateur de seuillage doux (soft-thresholding) :
\[
z^{k+1} = \mathcal{S}_{\lambda / \rho}\left(Wx^{k+1} + u^{k}\right),
\]
où \(\mathcal{S}_{\tau}(t) = \operatorname{sign}(t) \max\{0, |t| - \tau\}\) est appliqué composante par composante. Cette étape est fondamentale pour imposer la parcimonie : les coefficients d'ondelettes de faible amplitude (inférieurs au seuil \(\tau = \lambda/\rho\)) sont mis à zéro, tandis que les coefficients significatifs sont réduits. La parcimonie est donc explicitement appliquée dans le domaine des ondelettes via la variable \(z\).

\paragraph{Mise à jour de la variable duale \(u\)}
Enfin, la variable duale (mise à l'échelle) est actualisée pour tenir compte de la violation de la contrainte \(Wx - z = 0\) :
\[
u^{k+1} = u^{k} + \left(Wx^{k+1} - z^{k+1}\right).
\]
Cette mise à jour, caractéristique de l'approche de type "dual ascent", ajuste la pénalité linéaire. Lorsque la contrainte est satisfaite (\(Wx^{k+1} \approx z^{k+1}\)), la variation de \(u\) devient négligeable, signalant la convergence de l'algorithme. Dans le cas contraire, la variable duale intègre l'erreur de contrainte, ce qui influence les itérations suivantes de \(x\) et \(z\) pour forcer leur accord.

\paragraph{Conclusion}\text{}\vspace{5pt}\\
Ce chapitre a posé les fondements théoriques et algorithmiques du modèle de reconstruction proposé pour la tomodensitométrie à faible dose. La réduction du nombre de mesures transforme le problème de reconstruction en un problème inverse mal posé, caractérisé par une non-unicité de la solution et une sensibilité accrue au bruit. Dans ce contexte, le cadre du \textit{compressed sensing} fournit une approche rigoureuse en exploitant la parcimonie des images dans un domaine de représentation approprié, permettant ainsi de restreindre l'espace des solutions admissibles et de stabiliser le processus de reconstruction.

Sur cette base, le problème a été formulé sous la forme d'un problème variationnel combinant un terme de fidélité aux données, dérivé du modèle d'acquisition tomographique, et un terme de régularisation favorisant la parcimonie des coefficients dans une base d'ondelettes biorthogonales \texttt{bior4.4}. Ce choix se justifie par la capacité de ces ondelettes à fournir une représentation multi-échelles efficace des images, tout en préservant les structures géométriques et les discontinuités, caractéristiques essentielles des images tomographiques. La régularisation par norme $\ell_1$ constitue, dans ce cadre, un compromis optimal entre expressivité du modèle et tractabilité numérique.

La résolution du problème d'optimisation associé a été réalisée à l'aide de l'algorithme ADMM (\textit{Alternating Direction Method of Multipliers}), dont la structure permet de découpler le terme de fidélité et le terme de régularisation. Cette décomposition conduit à une succession de sous-problèmes convexes, admettant des solutions efficaces et bien conditionnées : une étape quadratique pour la mise à jour de la variable image, une étape proximale correspondant à un opérateur de seuillage doux imposant la parcimonie, et une mise à jour duale assurant la cohérence des contraintes. Cette stratégie garantit à la fois la robustesse, la stabilité et l'efficacité computationnelle du schéma de reconstruction.

Par ailleurs, un cadre d'évaluation rigoureux a été défini à l'aide de métriques quantitatives complémentaires, notamment la MSE, le PSNR et le SSIM, permettant d'évaluer respectivement la précision numérique, la qualité énergétique et la fidélité structurelle des images reconstruites. Ces indicateurs offrent une caractérisation objective et multidimensionnelle des performances du modèle.

En définitive, ce chapitre a permis d'établir une formulation mathématique cohérente et un schéma algorithmique efficace pour la reconstruction d'images tomographiques à faible dose, fondés sur la combinaison d'une régularisation parcimonieuse en ondelettes biorthogonales et d'un algorithme d'optimisation de type ADMM. Ce cadre constitue une base solide pour l'étude expérimentale présentée dans le chapitre suivant, qui visera à valider empiriquement les propriétés du modèle et à analyser son comportement dans des conditions réalistes d'acquisition à faible dose.