%===========================================================
% Chapitre 3 — Méthodes variationnelles, parcimonieuses et Compressive Sensing
%===========================================================
\chapter{Reconstruction d'images : problèmes inverses et compressive sensing}

\section{Cadre des problèmes inverses}
\subsection{Le problème inverse : formulation générale}
Un problème inverse consiste à estimer une quantité inconnue (ici, une image) à partir d'observations indirectes, bruitées et souvent incomplètes. Formellement, ce processus peut être modélisé par :

\begin{equation}
    \mathbf{y} = \mathcal{A}\mathbf{x} + \mathbf{n}
    \label{eq:inverse_problem}
\end{equation}

où :
\begin{itemize}
    \item[-] $\mathbf{x} \in \mathbb{R}^n$ représente l'image à reconstruire (inconnue),
    \item[-] $\mathbf{y} \in \mathbb{R}^m$ correspond aux données acquises (observations),
    \item[-] $\mathcal{A} : \mathbb{R}^n \rightarrow \mathbb{R}^m$ est l'opérateur direct modélisant le processus de dégradation (flou, projection, sous-échantillonnage, etc.),
    \item[-] $\mathbf{n} \in \mathbb{R}^m$ désigne le bruit de mesure additif.
\end{itemize}

L'opérateur $\mathcal{A}$ est typiquement \textit{mal conditionné} voire non inversible ($m < n$ dans le cas sous-déterminé), rendant la reconstruction de $\mathbf{x}$ à partir de $\mathbf{y}$ intrinsèquement difficile.


\begin{definition}
    Un problème est dit \textbf{bien posé} au sens de Hadamard s'il vérifie simultanément trois conditions :
    \begin{enumerate}
        \item \textbf{Existence :} Il existe au moins un $\mathbf{x} \in \mathbb{R}^n$ tel que $\mathcal{A} \mathbf{x} = \mathbf{y}$.
        
        \item \textbf{Unicité :} Cette solution est unique ; c'est-à-dire que si $\mathbf{z} \in \mathbb{R}^n$ vérifie $\mathcal{A} \mathbf{z} = \mathbf{y}$, alors nécessairement $\mathbf{z} = \mathbf{x}$.
        
        \item \textbf{Stabilité :} La solution dépend continûment des données $\mathbf{y}$. Plus précisément, pour toute suite $\{\mathbf{x}_n\}_{n\in\mathbb{N}} \subset \mathbb{R}^n$ telle que $\mathcal{A} \mathbf{x}_n \to \mathbf{y}$ (convergence dans $\mathbb{R}^m$), on a également $\mathbf{x}_n \to \mathbf{x}$ (convergence dans $\mathbb{R}^n$).
    \end{enumerate}
    Si l'une de ces trois conditions n'est pas vérifiée, le problème \eqref{eq:inverse_problem} est dit \emph{mal posé} au sens de Hadamard.\vspace{5pt}
\end{definition}

La majorité des problèmes inverses en imagerie violent au moins une de ces conditions, les rendant \textbf{mal posés}. La mal-positude se manifeste notamment par une grande sensibilité au bruit : de infimes variations des données $\mathbf{y}$ peuvent provoquer des changements arbitrairement grands dans la solution estimée.\vspace{5pt}

En pratique, les observations $\mathbf{y}$ sont corrompues par différents types de bruit (photonique, électronique, quantique, etc.) et sont souvent acquises de manière incomplète ($m \ll n$) pour des raisons techniques ou temporelles. Cette sous-détermination aggrave la mal-positude et rend le problème \textit{indéterminé} (multiples solutions possibles).\vspace{5pt}

La résolution d'un problème inverse mal posé nécessite l'injection d'\textit{information a priori} sur la solution recherchée. Ces contraintes peuvent être de différentes natures :
\begin{itemize}
    \item[-] Contraintes physiques (positivité, support limité, etc.)
    \item[-] Propriétés statistiques (distribution du bruit, régularité spatiale)
    \item[-] Structures spécifiques (parcimonie dans une base appropriée, bas rang, etc.)
\end{itemize}
L'intégration judicieuse de ces informations constitue le cœur des méthodes modernes de reconstruction.

% ================================
\section{Approches variationnelles} 
% ================================
Lors de la résolution de problèmes inverses, l'objectif est de retrouver une quantité inconnue \(\mathbf{x}\) à partir de données mesurées \(\mathbf{y}\). Comme les exemples précédents l'ont montré, cette tâche peut s'avérer difficile. Même si l'opérateur \(\mathcal{A}\) admet un inverse bien défini sur son image, c'est-à-dire que \(\mathcal{A}^{-1}: \mathbb{R}^m \rightarrow \mathbb{R}^n\) existe, rien ne garantit que les données bruitées appartiennent encore à l'image de l'opérateur. On peut également n'avoir accès qu'à des mesures partielles, ce qui mène à des systèmes sous-déterminés et rend l'inversion directe impossible, même sur des données non bruitées.\vspace{5pt}\\
\textbf{Régularisation.} Une approche pour obtenir des solutions significatives dans les scénarios décrits est appelée \textit{régularisation}. Une régularisation \(\mathfrak{R}_{\alpha}:\mathbb{R}^m \rightarrow \mathbb{R}^n\) associe à tout point de données une solution favorable. Intuitivement, on espère que la régularisation étend approximativement la notion d'inverse au cadre bruité et potentiellement mal posé, c'est-à-dire
\begin{equation}
    \mathfrak{R}_{\alpha}(\mathcal{A}\mathbf{x}+\varepsilon)\approx \mathbf{x}.
\end{equation}
Une stratégie typique est la régularisation dite \textit{de type Tikhonov} ou \textit{variationnelle}, où la sortie de l'application de régularisation est définie comme la solution du problème suivant

\begin{equation}
    \underset{\mathbf{x}\in\mathbb{R}^n}{\arg\min}
    \underbrace{\left\|{\mathcal{A}\mathbf{x}-\mathbf{y}} \right\|_{L^{2}}^{2}}_{\text{fidélité aux données}}+\alpha\underbrace{{\mathcal{R}}(\mathbf{x}) }_{\text{régularisant}}.
\end{equation}


Les termes s'interprètent comme suit :

\begin{itemize}
    \item \textbf{Fidélité aux données :} Ce terme indique à quel point notre estimation \(\mathbf{x}\) correspond aux données observées \(\mathbf{y}\). Bien que nous souhaitions minimiser la fidélité aux données, il n'est pas toujours pertinent de l'annuler dans le cas bruité, car nous cherchons \(\hat{\mathbf{x}}\) tel que \({\mathcal{A}}\hat{\mathbf{x}}=\mathbf{y}-\varepsilon\), où potentiellement \({\mathcal{A}}\hat{\mathbf{x}}\neq \mathbf{y}\).

    \item \textbf{Régularisant :} Le régularisant nous permet d'incorporer des informations supplémentaires sur la solution recherchée. Dans le cas classique, nous savons déjà que \(\mathbf{x}\) doit être proche d'un certain point \(\mu\), c'est-à-dire que nous voulons pénaliser la distance entre la solution \(\mathbf{x}\) et \(\mu\). De plus, on souhaite souvent pénaliser certaines directions plus que d'autres, pour lesquelles nous considérons un opérateur unitaire \(\mathcal{Q}:\mathbb{R}^{n}\to\mathbb{R}^{n}\) et choisissons ensuite
        \[{\mathcal{R}}(\mathbf{x})=\|\mathbf{x}-\mu\|_{L^{2},\mathcal{Q}}^{2}:=\langle \mathbf{x}-\mu,\mathcal{Q}(\mathbf{x}-\mu)\rangle\,,\] 
        ce qui est appelé \textit{régularisation de Tikhonov}\footnote{parfois appelée « régularisation aux moindres carrés »}.

    \item \textbf{Paramètre de régularisation \(\alpha\) :} Le paramètre \(\alpha>0\) contrôle la force du régularisant. Dans certaines formulations, ce paramètre est inclus dans la définition du régularisant.
\end{itemize}

Le terme de régularisation $\mathcal{R}(\mathbf{x})$ joue un rôle central dans la qualité de reconstruction obtenue. Il encode des hypothèses structurelles sur l'image recherchée et conditionne à la fois l'aspect visuel de la solution et la complexité numérique de l'algorithme utilisé.\newpage

Le choix du régularisateur dépend :
\begin{itemize}
    \item[-] du type d'images traitées,
    \item[-] de la nature du bruit,
    \item[-] de la structure de l'opérateur $\mathbf{A}$,
    \item[-] des contraintes de temps de calcul.
\end{itemize}
Des régularisations hybrides, combinant par exemple variation totale et parcimonie, sont fréquemment utilisées pour améliorer la qualité de reconstruction.

%===========================================================
\section{Régularisation classique}
%===========================================================
La régularisation classique désigne l'ensemble des méthodes visant à stabiliser la résolution des problèmes inverses en introduisant des contraintes supplémentaires sur la solution. Ces contraintes imposent des propriétés de l'image reconstruite telles que la régularité, la parcimonie des gradients ou la similarité avec des voisins. Les principales familles comprennent la régularisation quadratique de Tikhonov, la variation totale, les approches non locales et les méthodes multi-échelles.

% -------------------------------
\subsection{Régularisation de Tikhonov ($L^2$)}
% -------------------------------

Une méthode bien connue et efficace est la \textit{régularisation de Tikhonov}. Dans cette approche, une solution du problème \eqref{eq:inverse_problem} est approchée par une solution du problème de minimisation

\begin{equation}
    \min \|\mathcal{A}\mathbf{x} - \mathbf{y}\|_2^2 + \alpha \|\mathbf{x} - \mathbf{x}^*\|_2^2 \quad 
    \label{eq:tikhonov_problem}
\end{equation}

où \(\alpha > 0\) est un petit paramètre, \(\mathbf{y} \in \mathbb{R}^m\) est une approximation du membre de gauche exact \(\mathbf{y}\) du problème \eqref{eq:inverse_problem}, et \(x^*\) est une estimation a priori de la solution inconnue.\\
% Dans le cas d'un opérateur linéaire \(\mathcal{A}\), les aspects de stabilité, de convergence et de vitesses de convergence (quand \(\delta \to 0\)) ont été largement étudiés [2]. 
% Le rôle de la régularisation de Tikhonov pour stabiliser les problèmes d'estimation de paramètres a été examiné dans [3].
% Dans cette étude, nous montrons qu'une solution de l'équation
% \begin{equation}
%     (\mathcal{A}^*\mathcal{A} + \alpha \mathbf{I})x(\alpha) = \mathcal{A}^*\mathbf{y}
%     \label{eq:tikhonov_solution}
% \end{equation}
% existe et est unique, et que \(\mathcal{R}_{xy}\) défini par \(x(\alpha)\) est un régularisateur de l'équation \eqref{eq:inverse_problem} sur \(\mathbb{R}^n\), à condition que l'équation \(Ax = 0\) n'ait que la solution nulle.

D'après l'article \cite{7}, la démonstration de l'existence, de l'unicité et de la stabilité de la solution du problème de minimisation de Tikhonov est principalement détaillée dans les sections 2 et 3.

\subsubsection{Existence de la solution}
L'existence d'une solution au problème de minimisation est établie à la fois par l'analyse fonctionnelle générale et par une construction opératorielle spécifique :
\begin{itemize}
    \item \textbf{Lemme 1 :} Les auteurs affirment qu'une solution $R_\alpha y$ du problème de minimisation $\min (\|Av - y\|^2_Y + \alpha M(v))$ \textbf{existe} . La preuve repose sur le fait que $M$ est une fonctionnelle semi-continue inférieurement et que l'ensemble $\{v \in X_M : M(v) \leq r\}$ est compact .
    \item \textbf{Système de fonctions propres :} Dans la section 3, le texte déclare explicitement : « Tout d'abord, nous prouvons l'\textbf{existence} et l'unicité ». Cela est démontré en montrant que la solution $x(\alpha)$ peut être représentée à l'aide d'un système complet de fonctions propres $\{L_k\}$ de l'opérateur autoadjoint $\mathcal{A}^*\mathcal{A}$ .
\end{itemize}

\subsubsection{Unicité de la solution}
L'article fournit deux justifications mathématiques distinctes pour l'unicité de la solution :
\begin{itemize}
    \item \textbf{Détermination des composantes :} Dans la section 3, les composantes de la solution $x_k(\alpha)$ sont montrées être \textbf{déterminées de manière unique} par la formule :
    \[ x_k(\alpha) = \frac{y^A_k}{\lambda_k + \alpha} \]
    où $\lambda_k$ sont les valeurs propres de $\mathcal{A}^*\mathcal{A}$
    \item \textbf{Opérateur injectif :} Les références prouvent en outre l'unicité en montrant que l'opérateur $(\mathcal{A}^*\mathcal{A} + \alpha I)$ est positif. Plus précisément, ils démontrent que le \textbf{noyau} de $(\mathcal{A}^*\mathcal{A} + \alpha I)$ est $\{0\}$, ce qui garantit que toute solution existante est nécessairement unique.
\end{itemize}

\subsubsection{Stabilité de la solution}
La stabilité est abordée comme la dépendance continue de la solution par rapport aux données et au paramètre $\alpha$ :
\begin{itemize}
    \item \textbf{Preuve de continuité :} Dans la section 3, les auteurs déclarent explicitement : « Nous montrons maintenant que $R_\alpha$, c'est-à-dire $x(\alpha)$, est \textbf{continu} » [5]. Ils fournissent une preuve montrant que lorsque la différence entre les paramètres $|\alpha' - \alpha|$ tend vers zéro, la distance entre les solutions correspondantes $\|x(\alpha) - x(\alpha')\|$ s'annule également.
    \item \textbf{Propriété de régularisation :} Par définition, la famille d'opérateurs $R_\alpha$ est montrée être un \textbf{régulariseur}, ce qui signifie qu'elle fournit une approximation stable qui converge ponctuellement vers la solution exacte lorsque $\alpha \to 0$, sous réserve que les données soient cohérentes ($y = \mathcal{A}x$).
\end{itemize}

% % -------------------------------
\subsection{Variation Totale (TV)}
% -------------------------------

Afin de pallier les limitations de la régularisation quadratique, la régularisation par variation totale a été introduite pour préserver les discontinuités. Dans sa forme isotrope, elle est définie par :
\begin{equation}
    \mathcal{R}_{\text{TV}}(\mathbf{x}) 
    = \sum_{i,j} \sqrt{(\nabla_x x_{i,j})^2 + (\nabla_y x_{i,j})^2}
    = \|\nabla \mathbf{x}\|_1.
\end{equation}

L'emploi de la norme $L^1$ du gradient conduit à :
\begin{itemize}
    \item la préservation des bords nets,
    \item la suppression efficace du bruit,
    \item la production d'images par morceaux quasi-constants.
\end{itemize}

La minimisation associée est cependant non lisse et nécessite des méthodes itératives de type primal-dual, ADMM ou Split-Bregman \cite{8}. Un inconvénient notable est l'apparition du phénomène dit de \emph{staircasing}, caractérisé par des paliers artificiels.

% -------------------------------
\subsection{Méthodes non locales}
% -------------------------------

Les méthodes non locales exploitent la redondance statistique présente dans l'image en considérant des similarités entre pixels spatialement éloignés. Plutôt que de ne considérer que le voisinage local, elles s'appuient sur une mesure de similarité entre patchs.

Une formulation typique repose sur la pénalisation :
\begin{equation}
    \mathcal{R}_{\text{NL}}(\mathbf{x}) =
    \sum_{i,j} w_{ij} \, (x_i - x_j)^2,
\end{equation}
où $w_{ij}$ représente un poids mesurant la similarité entre les régions autour des pixels $i$ et $j$.

Ces approches permettent :
\begin{itemize}
    \item meilleure préservation des textures,
    \item réduction du bruit sans sur-lissage,
    \item exploitation de structures répétitives.
\end{itemize}

Elles sont à la base des méthodes telles que \textit{Non-Local Means} et des approches basées patchs pour la débruitage et la défloutage.

% -------------------------------
\subsection{Approches multi-échelles (ondelettes, curvelets)}
% -------------------------------

Les approches multi-échelles s'appuient sur le fait que les images naturelles présentent des structures à plusieurs résolutions. Les représentations dans des bases telles que les ondelettes, les curvelets ou les contourlets permettent de capturer efficacement :

\begin{itemize}
    \item les singularités orientées,
    \item les contours,
    \item les textures fines.
\end{itemize}

La régularisation consiste alors à imposer la parcimonie des coefficients transformés :
\begin{equation}
    \mathcal{R}_{\text{MS}}(\mathbf{x}) = \|\mathbf{\Psi x}\|_1,
\end{equation}
où $\mathbf{\Psi}$ désigne une transformée multi-résolution.

Les ondelettes sont particulièrement adaptées aux singularités ponctuelles, tandis que les curvelets et shearlets offrent une meilleure représentation des structures anisotropes et courbes. Ces méthodes constituent un lien direct avec le \textit{Compressed Sensing} et les modèles parcimonieux modernes.


%===========================================================
\section{Modèles parcimonieux}
%===========================================================

Les modèles parcimonieux reposent sur l'hypothèse qu'une image ou un signal peut être représenté par un nombre réduit de coefficients significatifs dans une base appropriée ou un dictionnaire sur-complet. Cette propriété est à la base de nombreuses techniques modernes de reconstruction d'images, de compression et de \textit{Compressed Sensing}. L'objectif est de promouvoir des représentations compactes permettant de régulariser les problèmes inverses mal posés.

% -------------------------------
\subsection{Bases orthogonales vs dictionnaires}
% -------------------------------

Une représentation parcimonieuse peut être obtenue soit dans une \textbf{base orthogonale}, soit dans un \textbf{dictionnaire sur-complet}.

\paragraph{Bases orthogonales}

Une base orthogonale $\{\mathbf{\phi}_k\}_{k=1}^{N}$ de $\mathbb{R}^n$ permet d'écrire :
\begin{equation}
    \mathbf{x} = \sum_{k=1}^{N} \alpha_k \mathbf{\phi}_k,
\end{equation}
où les coefficients $\alpha_k$ sont uniques. Exemples courants :
\begin{itemize}
    \item ondelettes orthogonales,
    \item base de Fourier,
    \item transformée discrète du cosinus (DCT).
\end{itemize}

Les bases orthogonales ont l'avantage de garantir l'unicité des coefficients et de permettre des calculs rapides via des transformées rapides (FFT, DWT, DCT).

\paragraph{Dictionnaires sur-complets}

Un dictionnaire $\mathbf{D} \in \mathbb{R}^{N \times K}$ avec $K > n$ est dit sur-complet. Dans ce cas, un signal peut avoir plusieurs décompositions possibles :
\begin{equation}
    \mathbf{x} = \mathbf{D}\mathbf{\alpha},
\end{equation}
où $\mathbf{\alpha} \in \mathbb{R}^{K}$ est un vecteur de coefficients.

Les dictionnaires sur-complets offrent :
\begin{itemize}
    \item une meilleure capacité d'adaptation aux structures complexes,
    \item des représentations plus parcimonieuses,
    \item la possibilité d'apprentissage à partir des données.
\end{itemize}
Cependant, la décomposition n'est plus unique et nécessite la résolution de problèmes d'optimisation.

% -------------------------------
\subsection{Modèles sparse : $L^0$ et $L^1$}
% -------------------------------

La parcimonie consiste à rechercher une représentation comportant le moins de coefficients non nuls possible. Le problème fondamental est :
\begin{equation}
    \min_{\mathbf{\alpha}} \|\mathbf{\alpha}\|_0 
    \quad \text{sous la contrainte} \quad 
    \mathbf{x} = \mathbf{D}\mathbf{\alpha},
\end{equation}
où $\|\mathbf{\alpha}\|_0$ désigne le nombre de coefficients non nuls. Ce problème est combinatoire et NP-difficile.

Pour rendre la résolution praticable, on remplace la norme $L^0$ par la norme convexe $L^1$ :
\begin{equation}
    \min_{\mathbf{\alpha}} \|\mathbf{\alpha}\|_1 
    \quad \text{sous} \quad 
    \mathbf{x} = \mathbf{D}\mathbf{\alpha},
\end{equation}
ou, en présence de bruit,
\begin{equation}
    \min_{\mathbf{\alpha}} 
    \left\{
    \frac{1}{2}\|\mathbf{x} - \mathbf{D}\mathbf{\alpha}\|_2^2 + 
    \alpha \|\mathbf{\alpha}\|_1
    \right\}.
\end{equation}

La relaxation $L^1$ constitue la base mathématique des approches de \textit{Basis Pursuit}, \textit{LASSO} et du \textit{Compressed Sensing}.

% -------------------------------
\subsection{Algorithmes d'approximation parcimonieuse}
% -------------------------------

Plusieurs classes d'algorithmes permettent de résoudre les problèmes parcimonieux.

\paragraph{Méthodes gloutonnes}

Ces méthodes sélectionnent itérativement les atomes du dictionnaire qui expliquent au mieux le signal résiduel. Parmi elles :
\begin{itemize}
    \item Matching Pursuit (MP),
    \item Orthogonal Matching Pursuit (OMP),
    \item Stagewise OMP (StOMP).
\end{itemize}

Elles présentent un faible coût de calcul et sont adaptées aux dictionnaires de grande taille.

\paragraph{Méthodes par seuillage}

Ces méthodes reposent sur le seuillage doux ou dur des coefficients :
\begin{itemize}
    \item ISTA (Iterative Shrinkage-Thresholding Algorithm),
    \item FISTA (version accélérée),
    \item algorithmes proximal-gradient.
\end{itemize}

Elles sont particulièrement adaptées à la minimisation de fonctionnelles $L^2$–$L^1$ convexes.

% -------------------------------
\subsection{K-SVD : apprentissage de dictionnaire}
% -------------------------------

Le K-SVD est un algorithme d'apprentissage de dictionnaire visant à construire un dictionnaire sur-complet directement à partir d'un ensemble d'images ou de patchs. L'objectif est de résoudre :
\begin{equation}
    \min_{\mathbf{D},\mathbf{\alpha}_i}
    \sum_{i}
    \left\|
    \mathbf{x}_i - \mathbf{D}\mathbf{\alpha}_i
    \right\|_2^2
    \quad
    \text{sous la contrainte}
    \quad
    \|\mathbf{\alpha}_i\|_0 \leq T_0,
\end{equation}
où $\{\mathbf{x}_i\}$ sont les signaux d'apprentissage et $T_0$ fixe le niveau de parcimonie.

L'algorithme alterne :
\begin{enumerate}
    \item une étape de \textbf{codage parcimonieux} des coefficients,
    \item une étape de \textbf{mise à jour du dictionnaire} atome par atome via la décomposition en valeurs singulières (SVD).
\end{enumerate}

Le K-SVD permet de construire des dictionnaires adaptés aux structures réelles des images, conduisant à d'excellentes performances en :
\begin{itemize}
    \item débruitage,
    \item défloutage,
    \item inpainting,
    \item super-résolution.
\end{itemize}


% ============================================================
\section{Théorie du Compressive Sensing}
% ============================================================

Le compressed sensing fournit un cadre rigoureux pour la reconstruction de signaux parcimonieux ou compressibles à partir de mesures linéaires fortement sous-échantillonnées. Il offre une solution fondée sur des principes solides aux problèmes inverses sous-déterminés, via des méthodes d'optimisation favorisant la parcimonie ou des algorithmes d'approximation gloutons. Cette approche a un impact majeur en imagerie médicale, imagerie computationnelle, résolution de problèmes inverses et systèmes de communication modernes.
\begin{definition}
    Le \emph{compressed sensing} (CS) est un cadre mathématique et algorithmique qui permet la reconstruction de signaux de grande dimension à partir d'un nombre de mesures significativement inférieur à celui requis par les méthodes traditionnelles.
\end{definition}
\begin{definition}
    Soit $x\in \mathbb{R}^{n}$ un signal inconnu. On dit que $x$  est \(k\)-parcimonieux dans une base (ou dictionnaire) \(\Psi\) (ex: Fourier, wavelet, DCT) si
    \[
        x=\Psi \alpha, \qquad \text{où } \alpha \text{ possède au plus } k \ll n \text{ coefficients non nuls}.   
    \]
\end{definition}
% Le principe fondamental repose sur la \textbf{parcimonie}. 
En pratique, de nombreux signaux ne sont pas parcimonieux dans leur domaine original (canonique), mais le deviennent après l'application d'une transformation linéaire. 
On observe des mesures linéaires de la forme
\[
y = A x,
\]
où \(A \in \mathbb{R}^{m \times n}\) avec \(m \ll n\). La théorie classique de l'échantillonnage exige \(m = n\) mesures indépendantes pour une reconstruction exacte, tandis que le compressed sensing montre que
\[
m \gtrsim k \log(n/k)
\]
est suffisant pour une reconstruction exacte ou stable, sous des conditions appropriées sur \(A\), telles que la propriété d'isométrie restreinte (\emph{Restricted Isometry Property}, RIP) ou l'incohérence.\\

Les résultats classiques de l'échantillonnage, tels que le théorème de Nyquist--Shannon, imposent qu'un signal soit échantillonné à une fréquence proportionnelle à sa bande passante. Le compressed sensing remet en cause ce paradigme en observant que de nombreux signaux réels (images, données médicales, spectres) sont parcimonieux ou compressibles dans une base de transformation (par exemple ondelettes, Fourier, DCT). Par conséquent, leur dimension effective est bien plus faible que le nombre d'échantillons disponibles. Le compressed sensing exploite cette redondance pour réduire drastiquement les coûts d'acquisition. Le compressed sensing s'attaque au problème général suivant :
\begin{center}
    \vspace*{\fill}
        Comment reconstruire un signal parcimonieux de grande dimension à partir d'un ensemble sous-déterminé de mesures linéaires ?
    \vspace*{\fill}
\end{center}
Ce cadre permet de résoudre plusieurs limitations pratiques :
\paragraph{Réduction du nombre de mesures.}\text{}\\ 
De nombreux systèmes d'acquisition sont limités par le coût, le temps ou l'énergie. Le compressed sensing permet :
\begin{itemize}
    \item[-] une acquisition plus rapide des données,
    \item[-] une réduction de la complexité matérielle,
    \item[-] une diminution de la dose de radiation (par exemple en tomodensitométrie),
    \item[-] une réduction des coûts de stockage et de transmission.
\end{itemize}

\paragraph{Problèmes inverses mal posés (ill-posed inverse problems).}\text{}\\
Lorsque le nombre de mesures est insuffisant pour garantir une solution unique, le CS introduit une régularisation fondée sur la parcimonie, permettant une reconstruction stable. Les principales applications incluent :
\begin{itemize}
    \item[-] la tomographie (CT, IRM, PET),
    \item[-] l'imagerie à super-résolution,
    \item[-] la déconvolution,
    \item[-] les inversions géophysiques et les essais non destructifs.
\end{itemize}

\paragraph{Robustesse au bruit et aux données incomplètes.} \text{}\\
Le CS garantit une reconstruction stable même en présence de bruit, de corruptions ou d'observations manquantes.

\subsection{Reconstruction de signaux par Compressed Sensing}

\subsubsection{Reconstruction par optimisation}
La formulation canonique de la reconstruction est
\[
\min_{\alpha} \|\alpha\|_{1} \quad \text{s.c.} \quad y = A \Psi \alpha,
\]
ou, en présence de bruit,
\[
\min_{\alpha} \|\alpha\|_{1} \quad \text{s.c.} \quad \|A \Psi \alpha - y\|_{2} \le \epsilon.
\]
Cela correspond aux formulations de type \emph{Basis Pursuit} ou \emph{LASSO}. La minimisation de la norme \(\ell_1\) favorise la parcimonie tout en conservant un problème d'optimisation convexe et calculable efficacement.

\subsubsection{Algorithmes gloutons}

Des alternatives plus rapides incluent :
\begin{itemize}
    \item l'\emph{Orthogonal Matching Pursuit} (OMP),
    \item le \emph{Compressive Sampling Matching Pursuit} (CoSaMP),
    \item l'\emph{Iterative Hard Thresholding} (IHT).
\end{itemize}
Ces méthodes échangent une partie de la précision contre un coût computationnel réduit.

\subsection{Applications du Compressed Sensing}

\paragraph{Imagerie médicale.}
\begin{itemize}
    \item acquisition IRM accélérée,
    \item CT à dose réduite,
    \item échographie à haute cadence d'images.
\end{itemize}

\paragraph{Imagerie computationnelle.}
\begin{itemize}
    \item caméras à pixel unique,
    \item imagerie à ouverture codée,
    \item reconstruction hyperspectrale.
\end{itemize}

\paragraph{Télédétection et géophysique.}
\begin{itemize}
    \item inversion sismique parcimonieuse,
    \item imagerie radar et radar à synthèse d'ouverture (SAR).
\end{itemize}

\paragraph{Communications sans fil.}
\begin{itemize}
    \item estimation parcimonieuse de canaux,
    \item réduction des pilotes dans les systèmes MIMO massifs.
\end{itemize}

\paragraph{Apprentissage automatique et traitement du signal.}
\begin{itemize}
    \item régression parcimonieuse (LASSO),
    \item apprentissage de dictionnaires,
    \item ACP robuste et modèles de rang faible apparentés.
\end{itemize}


% Le compressed sensing (CS) est un cadre mathématique et algorithmique qui permet de reconstruire des signaux de grande dimension à partir d'un nombre de mesures bien inférieur à celui requis par les approches traditionnelles. Il exploite la parcimonie (sparsity) comme principal a priori structurel.
% \subsection{Hypothèse de parcimonie}
% Si un signal est parcimonieux ou compressible dans une certaine base, alors il peut être reconstruit exactement (ou avec une erreur contrôlée) à partir d'un nombre de mesures linéaires bien inférieur à sa dimension ambiante.
% \subsection{Incohérence et propriété de RIP}
% \subsection{Basis Pursuit et LASSO}
% \subsection{OMP et algorithmes gloutons}


%===========================================================
\section{Méthodes d'optimisation}
%===========================================================

Les problèmes inverses régularisés et les modèles parcimonieux conduisent le plus souvent à la minimisation de fonctionnelles non différentiables, voire contraintes. Le choix d'une méthode d'optimisation adaptée conditionne la qualité de la reconstruction, la rapidité de convergence et la robustesse numérique. Les approches modernes reposent notamment sur les méthodes proximales, les schémas de décomposition de type ADMM, ainsi que les méthodes de gradient projeté.

% -------------------------------
\subsection{Méthodes proximales}
% -------------------------------

Les méthodes proximales constituent le cadre de référence pour l'optimisation de fonctionnelles comportant des termes non lisses. Soit une fonction convexe propre et semi-continue inférieurement $f : \mathbb{R}^n \rightarrow \mathbb{R}\cup\{+\infty\}$. L'opérateur proximal associé est défini par :
\begin{equation}
    \mathrm{prox}_{\alpha f}(\mathbf{x}) =
    \arg\min_{\mathbf{z}} 
    \left\{
        f(\mathbf{z}) + \frac{1}{2\alpha}\|\mathbf{z} - \mathbf{x}\|_2^2
    \right\}.
\end{equation}

Cet opérateur permet de traiter naturellement des pénalités non différentiables telles que :
\begin{itemize}
    \item la norme $\ell_1$ (seuilage doux),
    \item la variation totale,
    \item les contraintes indicatrices de convexes fermés.
\end{itemize}

Les algorithmes emblématiques incluent :
\begin{itemize}
    \item Forward–Backward Splitting,
    \item FISTA (accéléré de Nesterov),
    \item Primal–Dual de Chambolle–Pock.
\end{itemize}

Ils sont particulièrement adaptés aux problèmes de la forme :
\begin{equation}
    \min_{\mathbf{x}} \; f(\mathbf{x}) + g(\mathbf{x}),
\end{equation}
où $f$ est différentiable à gradient lipschitzien et $g$ est convexe éventuellement non lisse.

% -------------------------------
\subsection{Méthodes ADMM}
% -------------------------------

L'Alternating Direction Method of Multipliers (ADMM) est une méthode de décomposition permettant de résoudre des problèmes séparables en introduisant des variables auxiliaires. On considère typiquement le problème :
\begin{equation}
    \min_{\mathbf{x},\mathbf{z}} \;
    f(\mathbf{x}) + g(\mathbf{z})
    \quad \text{sous la contrainte} \quad
    \mathbf{Kx} = \mathbf{z}.
\end{equation}

Le schéma itératif repose sur :
\begin{enumerate}
    \item minimisation alternée sur $\mathbf{x}$ et $\mathbf{z}$,
    \item mise à jour des multiplicateurs de Lagrange.
\end{enumerate}

Les avantages majeurs d'ADMM sont :
\begin{itemize}
    \item traitement naturel des contraintes linéaires,
    \item parallélisation possible des sous-problèmes,
    \item robustesse pour les grands problèmes mal conditionnés.
\end{itemize}

ADMM est aujourd'hui une référence pour la résolution de problèmes de variation totale, de débruitage parcimonieux et d'apprentissage de dictionnaire.

% -------------------------------
\subsection{Méthodes de gradient projeté}
% -------------------------------

Les méthodes de gradient projeté visent la résolution de problèmes d'optimisation sous contraintes convexes :
\begin{equation}
    \min_{\mathbf{x} \in C} f(\mathbf{x}),
\end{equation}
où $C$ est un ensemble convexe fermé. L'itération générique s'écrit :
\begin{equation}
    \mathbf{x}^{k+1} =
    \Pi_{C}\left(
        \mathbf{x}^k - \alpha_k \nabla f(\mathbf{x}^k)
    \right),
\end{equation}
où $\Pi_{C}$ désigne l'opérateur de projection sur $C$.

Ces méthodes sont notamment utilisées pour :
\begin{itemize}
    \item l'imposition de contraintes de positivité,
    \item la borne supérieure ou inférieure sur des intensités,
    \item les contraintes de norme sur des coefficients parcimonieux.
\end{itemize}

Elles constituent des schémas simples, peu coûteux en mémoire, et très utilisés en traitement d'images et en reconstruction tomographique lorsque la fonction coût est différentiable.

% \subsection{Unrolling des algorithmes (vers deep learning)}

% \section{Synthèse critique}
% \subsection{Avantages et limites}
% \subsection{Cas où le compressive sensing excelle}
% \subsection{Motivation des approches apprises}
