\chapter{NOTRE MOD√àLE}
% \section{Les traitements pr√©alables √† la reconstruction}

% \subsection{ M√©thodes dans le domaine spatial}
% % =================== TODO ==================
% \subsection{ Filtrage lin√©aire}
% % =========================================


% \subsection{ M√©thodes dans le domaine transform√©}
\section{ Transform√©e de Fourier}
\begin{definition}[Transform√©e de Fourier]
    Soit \( f \) une fonction absolument int√©grable sur \( \mathbb{R} \).
    La transform√©e de Fourier de \( f \), not√©e \( \mathcal{F}f \), est d√©finie
    pour tout nombre r√©el \( \xi \) par
    \[
    (\mathcal{F}f)(\xi)
    = \int_{-\infty}^{\infty} f(x)\, e^{-2\pi i \xi x}\, dx.
    \]
\end{definition}
La transform√©e de Fourier est fr√©quemment utilis√©e en analyse du signal et permet de transformer une fonction du temps en une fonction de la fr√©quence ; la variable $x$ repr√©sente le temps en secondes et la variable \( \xi \) repr√©sente la fr√©quence de la fonction en hertz.\\

Il existe une d√©finition alternative faisant intervenir la fr√©quence angulaire $w=2\pi \xi$, ce qui conduit √† l'expression suivante.
\[(\mathcal{F}f)(w) = \int_{-\infty}^{\infty} f(x)\, e^{-i w x}\, dx\]
Comme pour la transform√©e de Radon, nous allons √©num√©rer plusieurs propri√©t√©s de la transform√©e de Fourier.
\begin{proposition}
    Pour des constantes r√©elles $\alpha$ et $\beta$, et des fonctions absolument int√©grables $f$ et $g$, on a:
    \begin{itemize}
        \item[(i)] Lin√©arit√© : $\mathcal{F}(\alpha f + \beta g)(w) = \alpha \mathcal{F}f(w) + \beta \mathcal{F}g(w)$
        \item[(ii)] $\mathcal{F}f(w) < +\infty$
    \end{itemize}
\end{proposition}

\begin{definition}[Transform√©e de Fourier inverse]
Soit \( f \) une fonction absolument int√©grable.
La transform√©e de Fourier inverse de \( f \), not√©e \( \mathcal{F}^{-1}f \),
√©valu√©e en \( x \), est d√©finie par
\begin{equation}
    (\mathcal{F}^{-1}f)(x)
    = \cfrac{1}{2\pi}\int_{-\infty}^{\infty} f(w)\, e^{iw x}\, dw.
    \label{formula:fourier_inverse}
\end{equation}
\end{definition}
Ceci nous conduit imm√©diatement au th√©or√®me suivant.
\begin{proposition}[Th√©or√®me d'inversion de Fourier]
Soit $f$ une fonction absolument integrale sur $\mathbb{R}$.
Le th√©or√®me d'inversion de Fourier affirme que, pour tout \( x \),
\[
(\mathcal{F}^{-1} \circ \mathcal{F})f(x)=f(x)
\]
\end{proposition}
Jusqu'√† pr√©sent, nous n'avons abord√© la transform√©e de Fourier que dans une dimension. Il existe des d√©finitions correspondantes en dimensions sup√©rieures, mais, pour nos besoins, nous n'utiliserons que les analogues en deux dimensions.

\begin{definition}[Transform√©e de Fourier bidimensionnelle]
Soit \( g \) une fonction absolument int√©grable d√©finie sur \( \mathbb{R}^2 \).
La transform√©e de Fourier bidimensionnelle de \( g \), not√©e \( \mathcal{F}_2 g \),
est d√©finie pour tout \((X,Y) \in \mathbb{R}^2\) par
\begin{equation}
    (\mathcal{F}_2 g)(X,Y) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} 
    g(x,y)\, e^{-i (xX + yY)} \, dx\, dy.
    \label{eq:fourier_2d}
\end{equation}

\end{definition}

De mani√®re similaire, nous d√©finissons la transform√©e de Fourier inverse sur $\mathbb{R}^2$.
\begin{definition}[Transform√©e de Fourier bidimensionnelle inverse]
Soit \( g \) une fonction absolument int√©grable d√©finie sur \( \mathbb{R}^2 \).
La transform√©e de Fourier bidimensionnelle inverse de \( g \), √©valu√©e en \((x,y)\)
et not√©e \( \mathcal{F}_2^{-1} g(x,y) \), est donn√©e par
\[
(\mathcal{F}_2^{-1} g)(x,y) = \cfrac{1}{4\pi^2}\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} 
g(X,Y)\, e^{i (xX + yY)} \, dX\, dY.
\]
\end{definition}

% ================== TODO ==================
% \subsection{ Filtre de Wiener}
% 1. Filtre de Wiener
% ‚úÖ Oui, tout √† fait applicable
% Le filtre de Wiener est un filtre lin√©aire adaptatif.

% Il peut √™tre appliqu√© :
%    - sur chaque projection (variable s)
%    - ou localement sur le sinogramme

% Int√©r√™t :
%    - r√©duction du bruit additif (souvent gaussien)
%    - compromis bruit / flou optimal au sens MSE

% ‚ö†Ô∏è Limites :
%    - n√©cessite une estimation du bruit et du spectre du signal
%    - un mauvais mod√®le d√©grade la reconstruction

% üìå Tr√®s utilis√© comme pr√©traitement des sinogrammes en CT √† faible dose.

% \subsection{ Curvelets}
% 4. Curvelets
% ‚úÖ Oui, tr√®s pertinent
% Les curvelets sont th√©oriquement bien adapt√©es :
%    - excellente repr√©sentation des singularit√©s le long de courbes

% Les lignes du sinogramme correspondent √† :
%    - des courbes li√©es aux bords de l'objet

% üìå Tr√®s utilis√© dans :
%    - CT basse dose
%    - Les m√©thodes variationnelles et it√©ratives
% =================================================

\section{Convolution}
\textbf{D√©finition 8.1.}
Pour deux fonctions int√©grables $f$ et $g$ d√©finies sur $\mathbb{R}$,
nous d√©finissons la convolution de $f$ et $g$, not√©e $f \star g$, par
\[
(f \star g)(x) = \int_{-\infty}^{\infty} f(t)\,g(x - t)\,dt,
\]
o√π $x \in \mathbb{R}$.

Nous pouvons facilement √©tendre cette d√©finition √† l'espace
bidimensionnel. Pour les fonctions polaires, nous prenons uniquement
l'int√©grale par rapport √† la variable radiale, tandis que pour les
fonctions cart√©siennes nous int√©grons par rapport aux deux variables.
Les d√©finitions explicites sont donn√©es ci-dessous.

\begin{definition}
    Pour des fonctions polaires int√©grables $f(t,\theta)$ et $g(t,\theta)$,
    nous d√©finissons la convolution de $f$ et $g$ par
    \[
        (f \star g)(t,\theta)
        =
        \int_{-\infty}^{\infty}
        f(s,\theta)\,g(t - s,\theta)\,ds.
    \]
\end{definition}

Pour des fonctions int√©grables $F$ et $G$ sur $\mathbb{R}^2$,
nous d√©finissons la convolution de $F$ et $G$ par
\[
    (F \star G)(x,y)
    =
    \int_{-\infty}^{\infty}
    \int_{-\infty}^{\infty}
    F(s,t)\,G(x - s, y - t)\,ds\,dt.
\]

La convolution est une m√©thode math√©matique permettant de moyenner
une fonction $f$ √† l'aide du d√©placement d'une autre fonction $g$.
Dans la convolution $f \star g$, la fonction $g$ est translat√©e √† travers
la fonction $f$, et la fonction r√©sultante d√©pend de la zone de recouvrement
au cours de cette translation.
En un certain sens, on peut voir $g$ comme un filtre utilis√© pour effectuer
une moyenne de $f$ sur un intervalle donn√©.
La fonction de filtrage agit ainsi comme un lisseur pour les donn√©es bruit√©es
fournies par la fonction originale.

\begin{proposition}
    Pour des fonctions int√©grables $f$, $g$, $h$ d√©finies sur $\mathbb{R}$
    et des constantes $\alpha, \beta \in \mathbb{R}$ :
    
    \begin{itemize}
      \item[(i)] $f \star g = g \star f$ \quad (commutativit√©),
      \item[(ii)] $f \star (\alpha g + \beta h)
      = \alpha (f \star g) + \beta (f \star h)$ \quad (lin√©arit√©).
      \item[(iii)] $\mathcal{F}(f). \mathcal{F}(g)  = \mathcal{F}(f \star g)$
    \end{itemize}
\end{proposition}


% =================================================
\section{La transform√©e de Radon}
% =================================================
L'hypoth√®se fondamentale est que le d√©tecteur mesure l'att√©nuation int√©gr√©e le long d'un rayon. 
\begin{definition}
    Pour un faisceau de rayons $\mathbf{X}$ d'√©nergie $\mathbf{E}$ donn√©e et un taux de propagation des photons $\mathbf{N}(x)$, l'intensit√© du faisceau $\mathbf{I}(x)$ √† une distance $x$ de l'origine est d√©finie comme \[\mathbf{I}(x) = \mathbf{N}(x) \mathbf{E}\]
\end{definition}

\begin{definition}
    La proportion de photons absorb√©s par millim√®tre de substance √† une distance $x$ de l'origine est appel√©e le coefficient d'att√©nuation $\mathbf{A}(x)$ du milieu.
\end{definition}


Nous connaissons les intensit√©s initiale et finale, $I_0$ et $I_1$ d'un faisceau unique. L'objectif est d'utiliser ces intensit√©s pour d√©terminer le coefficient d'att√©nuation le long du trajet du faisceau. Heureusement, la loi de Beer-Lambert √©tablit une relation entre ces deux grandeurs.

\begin{definition}[Loi de Beer-Lambert]
Pour un faisceau de rayons X monochromatique, non r√©fractif et de largeur nulle,
traversant un milieu homog√®ne sur une distance \(x\) √† partir de l'origine,
l'intensit√© \(I(x)\) est donn√©e par
\begin{equation}
    I(x) = I_0 e^{-\mathbf{A}(x)x}
    \label{eq:loi_beer_lambert}
\end{equation}
\end{definition}
En l'√©tat, cette √©quation ne nous est pas particuli√®rement utile. Elle exprime le coefficient d'att√©nuation en un point donn√© en fonction de l'intensit√© en ce point, alors que nous ne connaissons la valeur de l'intensit√© qu'en des points situ√©s √† l'ext√©rieur de l'objet. Ce que nous cherchons r√©ellement est une relation entre le coefficient d'att√©nuation √† l'int√©rieur de l'objet et la variation de l'intensit√© du faisceau. Pour cela, nous allons manipuler l√©g√®rement l'√©quation \eqref{eq:loi_beer_lambert}.\\
En passant √†  la d√©riv√©e de la loi de Beer-Lambert, nous obtenons la relation suivante :
\[
    \frac{dI}{dx} = -\mathbf{A}(x)I(x)
\]
Soit $I(x_0)=I_0$ la valeur initiale de l'intensit√© du faisceau et $I(x_1)=I_1$ la valeur finale de l'intensit√© du faisceau. En utilisant cette relation, nous obtenons la relation suivante :

\[
    -\int_{x_0}^{x_1} \mathbf{A}(x)dx = \int_{x_0}^{x_1}\cfrac{dI}{I(x)}=ln(\frac{I_1}{I_0})
\]
ou encore \vspace{10pt}
\begin{equation}
    \int_{x_0}^{x_1} \mathbf{A}(x)dx = ln(\frac{I_0}{I_1})
    \label{eq:radon_transformation}
\end{equation}

$ln(\frac{I_0}{I_1})$ d√©signe les donn√©es de projection, commun√©ment appel√©es le sinogramme, qui r√©sultent de l'acquisition des projections tomographiques.
\medskip
\noindent
Nous sommes maintenant pr√™ts √† introduire des outils math√©matiques ‚Äî en particulier la transform√©e de Radon ‚Äî qui joueront un r√¥le central dans la d√©termination du coefficient d'att√©nuation dans l'√©quation \eqref{eq:loi_beer_lambert}.

L'√©criture sous forme normale d'une √©quation de droite joue un r√¥le cl√© dans la transform√©e de Radon, car elle permet une param√©trisation naturelle et compl√®te de toutes les droites du plan, ce qui est essentiel pour la d√©finition math√©matique et le calcul pratique de cette transformation.\\ 
Cette √©quation sous forme normale fournit :
\begin{itemize}
    \item[(i)] Une param√©trisation unique et continue de toutes les droites du plan. La forme normale (ou forme normale de Hesse) de l'√©quation de la droite s'√©crit : $$x\,\cos(\theta)+y\,\sin(\theta)=\rho$$ o√π $\rho$ est la distance par rapport √† l'origine et $\theta$ est l'angle par rapport √† l'axe des abscisses.
    \item[(ii)] Une interpr√©tation g√©om√©trique claire de $\rho$ et $\theta$. Chaque droite du plan correspond  √† un unique couple ($\rho,\theta$). Cette param√©trisation √©vite les redondances et garantit qu'on parcourt toutes les droites une et une seule fois (√† une convention pr√®s).
    \item[(iii)] Une mesure naturelle sur l'espace des droites, utilis√©e dans les formules d'inversion.
    \item[(iv)] Un formalisme adapt√© au th√©or√®me de coupe, reliant transform√©e de Radon et transform√©e de Fourier 2D. 
    \item[(v)] Une mesure naturelle sur l'espace des droites, utilis√©e dans les formules d'inversion.
\end{itemize}\vspace{10pt}
\subsection{\small Construction de l'orientation et de la distance}
Nous connaissons tous l'id√©e qu'une droite \( l \) dans \( \mathbb{R}^2 \) peut √™tre repr√©sent√©e par l'√©quation 
\[
ax + by = c
\]
o√π \( a, b, c \in \mathbb{R} \) et \( a^2 + b^2 \neq 0 \).\\ On peut alors √©crire cette √©quation d'une droite sous la forme \[w_1x + w_2y = t\]
o√π $\mathbf{w}:=(w_1, w_2) = (\cfrac{a}{\sqrt{a^2 + b^2}}, \cfrac{b}{\sqrt{a^2 + b^2}})$ et $t=\cfrac{c}{\sqrt{a^2 + b^2}}$, que nous pouvons voir comme un point situ√© sur le
cercle unitaire, pour \[\left(\cfrac{a}{\sqrt{a^2 + b^2}}\right)^{2} + \left(\cfrac{b}{\sqrt{a^2 + b^2}}\right)^{2} = 1\]
Cela implique que $\mathbf{w} := (\cos(\theta), \sin(\theta)) \text{ est un vecteur normal unitaire }$, $\theta \in [0, 2\pi)$ repr√©sente l'orientation, et $t$ est exactement la distance √† l'origine. On a \[x\cos(\theta) + y\sin(\theta) = t\]
Notez que dans les √©quations ci-dessus, $t$ et $\theta$ sont fixes et d√©terminent une droite sp√©cifique \( l \) dans le plan. On peut donc dire que $t$ et $\theta$ param√®trent une droite \( l_{t,\theta} \) et que $\mathbf{z}$ d√©termine des points sp√©cifiques sur cette droite \( l \). Ou encore
\[l_{t,\theta} = \{ \mathbf{z} \in \mathbb{R}^2 : \langle z, (\cos \theta, \sin \theta) \rangle = t \}.\]
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{./images/l_t_theta.png}
    \caption{param√®trisation d'une droite \( l_{t,\theta} \) par \( t \) et \( \theta \)}
    \label{fig:l_t_theta}
\end{figure}
On voit  que $(t\, \cos(\theta), t\, \sin(\theta))$ est un point situ√© sur la droite \( l_{t,\theta} \) (\Cref{fig:l_t_theta}) et $(-\sin(\theta), \cos(\theta))$ est un vecteur perpendiculaire au vecteur unitaire $\mathbf{w}$.\\ En g√©om√©trie affine √©l√©mentaire, une ligne est un point plus une direction. Par cons√©quent, nous pouvons d√©crire un point particulier $(x, y)$ sur $l_{t, \theta}$ en termes de nombre r√©el s comme suit :
\begin{equation}
    l_{t, \theta} = \{(t\, \cos(\theta) - s\,\sin(\theta), t\,\sin(\theta) + s\,\cos(\theta)); s\in \mathbb{R}\}
    \label{set:l_t_theta}
\end{equation}
\begin{definition}[Transform√©e de Radon]
Soit \( f(t,\theta) \) une fonction d√©finie sur \( \mathbb{R}^2 \) √† support compact.
La transform√©e de Radon de \( f \), not√©e \( \mathcal{R}f \), est d√©finie pour
\( t \in \mathbb{R} \) et \( \theta \in (0, 2\pi] \) par
\[
\mathcal{R}f(t,\theta) = \int_{-\infty}^{\infty} f(x(s),y(x))\mathrm{d}s
\]
\end{definition}

La transform√©e de Radon permet de d√©terminer la densit√© totale d'une fonction $f$ le long d'une droite donn√©e $l$. Cette droite $l$ est d√©finie par un angle $\theta$  par rapport √† l'axe 
$x$ et une distance $t$ par rapport √† l'origine. Comme illustr√© √† la \Cref{fig:radon}, si l'on calcule la transform√©e de Radon le long de plusieurs droites √† des angles diff√©rents (ici $\theta_1$ et $\theta_2$), on peut d√©terminer plusieurs fonctions de densit√© pour notre objet. Intuitivement, on peut interpr√©ter la transform√©e de Radon comme une version ¬´ √©tal√©e ¬ª de notre objet initial. Supposons que la r√©gion en forme de tache repr√©sent√©e √† la \Cref{fig:radon} soit une tache d'encre; si l'on √©tale cette tache le long de lignes de direction $\theta_1$, on s'attend √† ce que les r√©gions les plus larges de la tache correspondent √† des zones plus √©tendues que les r√©gions plus petites, ce qui correspond exactement √† ce que l'on observe.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{./images/radon.png}
    \caption{Transform√©e de Radon pour $\theta_1$ et $\theta_2$.}
    \label{fig:radon}
\end{figure}
L'int√©grale $\mathcal{R}f(t,\theta)$ repr√©sente le membre gauche de l'√©quation \eqref{eq:radon_transformation}. Rappelons que, dans cette √©quation, $\mathbf{A}(x)$ est inconnue et que $\ln(\frac{I_1}{I_0})$ correspond √† une information mesur√©e.
Autrement dit, $\ln(\frac{I_1}{I_0})$ est la transform√©e de Radon, et la transform√©e de Radon repr√©sente donc des donn√©es connues issues de la mesure.

L'objectif est maintenant de trouver une formule d'inversion de la transform√©e de Radon qui nous permettra de reconstruire la fonction initiale $f$ (ou, dans le contexte de l'imagerie m√©dicale, 
$\mathbf{A}(x)$). Pour ce faire, il sera utile de rappeler plusieurs propri√©t√©s de la transform√©e de Radon.
\begin{proposition}
    Soit $\alpha$ et $\beta$ deux r√©els et $f$ et $g$ deux fonctions continues sur $\mathbb{R}^2$ √† support compact. On a
    \begin{itemize}
        \item[(i)] Lin√©arit√© : $\mathcal{R}(\alpha f + \beta g) = \alpha \mathcal{R}f + \beta \mathcal{R}g$
        \item[(ii)] Parit√©: $\mathcal{R}f(-t,-\theta) = \mathcal{R}f(t,\theta)$
        \item[(iii)] $\mathcal{R}f(t, \theta) = \int_{-\infty}^{\infty} f(x(s), y(s))\mathrm{d}s = \int_{-\infty}^{\infty} f(t\,cos(\theta)-s\,sin(\theta), t\,sin(\theta)+s\,cos(\theta))\mathrm{d}s$
        % \item[(iv)] Invariance par rotation : \(\mathcal{R}(f \circ R_{\psi}) = \mathcal{R}f(t,\theta - \psi)\)
        % \item[(v)] Relation avec la convolution : \(\mathcal{R}(f * g) = \mathcal{R}f * \mathcal{R}g\)
    \end{itemize}
\end{proposition}
Nous d√©finissons en outre le domaine naturel de la transform√©e de Radon comme l'ensemble des fonctions $f$ sur $\mathbb{R}^2$ telles que \[\int_{-\infty}^{\infty} |f(x(s), y(s))|\mathrm{d}s < \infty\]

\subsection{Le Th√©or√®me de la Coupe Centrale}
Le th√©or√®me de la coupe centrale, √©galement appel√© th√©or√®me de projection-transforme de Fourier ou th√©or√®me de Fourier-Slice, est un r√©sultat fondamental en traitement d'image et en tomographie. Il √©tablit un lien profond entre la transform√©e de Radon (utilis√©e pour d√©crire les projections d'un objet) et la transform√©e de Fourier (utilis√©e pour analyser les fr√©quences spatiales). Ce th√©or√®me constitue la pierre angulaire math√©matique de la plupart des m√©thodes de reconstruction tomographique moderne.

\begin{proposition}
    Soit \( g \) une fonction absolument integrale sur \( \mathbb{R}^2 \).
    Le th√©or√®me de la coupe centrale affirme que, pour tout $S \in \mathbb{R}$ et $\theta \in [0,2\pi]$, on a : \[\mathcal{F}_2 g(S\cos(\theta), S\sin(\theta)) = \mathcal{F}(\mathcal{R}g)(S, \theta)\]
\end{proposition}
\textbf{Preuve}: En utilisant la d√©finition de la transform√©e de Fourier bidimensionnelle \eqref{eq:fourier_2d} on obtient 
\[
    \mathcal{F}_{2}g(S\,\cos(\theta), S\,\sin(\theta)) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x, y)\, e^{-iS (x\,\cos(\theta) + y\,\sin(\theta))}\, dx\, dy
\]
Nous effectuons maintenant un changement de variables conform√©ment au syst√®me
de coordonn√©es que nous avons d√©fini √† la \textit{Construction de l'orientation et de la distance}.
Rappelons que, lors de la param√©trisation de la droite $\ell_{t,\theta}$,
nous avons montr√© que, pour $s\in\mathbb{R}$, on peut √©crire :
\[
x(s)=t\cos\theta - s\sin\theta, 
\qquad
y(s)=t\sin\theta + s\cos\theta,
\qquad
t = x\cos\theta + y\sin\theta.
\]

En examinant le d√©terminant du Jacobien associ√© √† $x(s)$ et $y(s)$, on obtient :
\[
\det
\begin{pmatrix}
\dfrac{\partial x}{\partial t} & \dfrac{\partial x}{\partial s} \\[6pt]
\dfrac{\partial y}{\partial t} & \dfrac{\partial y}{\partial s}
\end{pmatrix}
= 1.
\]

Nous en d√©duisons que
\[
ds\,dt = dx\,dy.
\]
et donc
\[
\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x, y)\, e^{-iS (x\,\cos(\theta) + y\,\sin(\theta))}\, dx\, dy = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}
g(t\cos\theta - s\sin\theta,\; t\sin\theta + s\cos\theta)\,
e^{-iSt}\,ds\,dt.
\]

Comme $e^{-iSt}$ ne d√©pend pas de la variable $s$, nous pouvons r√©arranger
l'int√©grale pr√©c√©dente de la mani√®re suivante :
\[
\int_{-\infty}^{\infty}
\left(
\int_{-\infty}^{\infty}
g(t\cos\theta - s\sin\theta,\; t\sin\theta + s\cos\theta)\,ds
\right)
e^{-iSt}\,dt.
\]

L'int√©grale int√©rieure est exactement la transform√©e de Radon de $f$,
√©valu√©e en $(t,\theta)$, ce qui implique que l'expression pr√©c√©dente devient :
\[
\int_{-\infty}^{\infty}
(Rg(t,\theta))\,e^{-iSt}\,dt.
\]

Cette derni√®re int√©grale n'est autre que la transform√©e de Fourier de
$Rg(S,\theta)$, ce qui conclut la d√©monstration.
\hfill$\square$

% \section{Inversion analytique de la transform√©e de Radon}
\subsection{R√©troprojection filtr√©e (FBP)}
Nous sommes maintenant enfin pr√™ts √† effectuer une premi√®re tentative pour retrouver la fonction de coefficient d'att√©nuation.
Rappelons que, d'un point de vue physique, la transform√©e de Radon
$\mathcal{R}f(t,\theta)$ nous donne la densit√© totale de l'objet $f$ le long d'une droite
$\ell_{t,\theta}$.
Nous avons d√©termin√© cette densit√© en mesurant les intensit√©s initiale et finale
d'un faisceau de rayons $\mathbf{X}$ traversant l'objet le long de cette droite.
En proc√©dant ainsi pour plusieurs droites diff√©rentes, nous sommes capables de
reconstruire une coupe unique de l'objet initial, et en faisant varier l'angle
$\theta$ de ces rayons $\mathbf{X}$, nous pouvons d√©finir de nombreuses coupes.

Si nous sommes capables, d'une certaine mani√®re, de ¬´ r√©troprojeter ¬ª ces
densit√©s sur le plan, nous pourrons peut-√™tre reconstituer l'objet initial.
Intuitivement, on peut interpr√©ter ce processus comme le fait de prendre les
donn√©es du sinogramme et de les ¬´ d√©flouter ¬ª pour les ramener dans le plan.
\begin{definition}
Soit $h = h(t,\theta)$. On d√©finit la \emph{r√©troprojection},
not√©e $\mathcal{B}h$, en un point $(x,y)$ par :
\[
\mathcal{B}h(x,y) = \frac{1}{\pi}\int_{0}^{\pi} h(x\cos\theta + y\sin\theta,\theta)\,d\theta.
\]

En appliquant cette formule de r√©troprojection √† la transform√©e de Radon, on
obtient :
\begin{equation}
    \mathcal{B}\mathcal{R}f(x,y) = \frac{1}{\pi}\int_{0}^{\pi}
    \mathcal{R}f(x\cos\theta + y\sin\theta,\theta)\,d\theta.
    \label{eq:FBP}
\end{equation}
\end{definition}
Nous sommes capables d'effectuer la r√©troprojection sur les coupes que nous
avons mesur√©es. Comme illustr√© √† la \Cref{fig:FBP}, effectuer une r√©troprojection
selon seulement quelques directions $\theta$ constitue une m√©thode extr√™mement
impr√©cise pour reconstituer ne serait-ce qu'un objet simple. Toutefois, m√™me si
nous augmentons de mani√®re significative le nombre de r√©troprojections
(par exemple jusqu'√† $1000$ directions), il subsiste encore une quantit√©
importante de bruit qui brouille l'image reconstruite.
En r√©alit√©, quel que soit le nombre de directions selon lesquelles nous tentons
d'effectuer la r√©troprojection, nous ne serons jamais capables de reconstruire
parfaitement l'image √† l'aide de la formule de r√©troprojection donn√©e par
l'√©quation \eqref{eq:FBP}.
Pour que ce proc√©d√© soit r√©ellement utile, il est n√©cessaire de d√©river une
m√©thode permettant de filtrer une partie du bruit que la formule de
r√©troprojection semble introduire dans l'image, afin d'obtenir une
repr√©sentation plus lisse de l'objet.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{./images/fbp.png}
    \caption{Retroprojection d'un carr√© dans 5, 25, 100 et 1000 directions}
    \label{fig:FBP}
\end{figure}

Dans ce but, nous d√©finissons une formule de \emph{r√©troprojection filtr√©e}.
\begin{proposition}
    Soit $f$ une fonction absolument int√©grable d√©finie sur $\mathbb{R}^2$. Alors,
    \begin{equation}
        f(x,y)
        =
        \frac{1}{2}\,
        \mathcal{B}\!\left\{
        \mathcal{F}^{-1}
        \!\left[
        |S|\,
        \mathcal{F}\!\left(\mathcal{R}f\right)(S,\theta)
        \right]
        \right\}(x,y).
        \label{eq:FBP_filter}
    \end{equation}
\end{proposition}
\textit{D√©monstration.}
Nous commen√ßons par rappeler que, pour la transform√©e de Fourier bidimensionnelle
et son inverse, on a :
\begin{equation}
f(x,y) = \mathcal{F}_2^{-1}\,\mathcal{F}_2 f(x,y)
= \frac{1}{4\pi^2}
\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}
\mathcal{F}_2 f(X,Y)\,e^{i(Xx+Yy)}\,dX\,dY.
\label{eq:fourier_2d_inverse}
\end{equation}

Nous allons maintenant effectuer un changement de variables des coordonn√©es
cart√©siennes $(X,Y)$ vers les coordonn√©es polaires $(S,\theta)$, d√©finies par
\[
X = S\cos\theta,
\qquad
Y = S\sin\theta,
\]
o√π $S \in \mathbb{R}$ et $\theta \in [0,\pi]$.
Ce changement de variables conduit au d√©terminant jacobien suivant :
\[\det
\begin{pmatrix}
    \dfrac{\partial X}{\partial s} & \dfrac{\partial X}{\partial \theta} \\[6pt]
    \dfrac{\partial Y}{\partial s} & \dfrac{\partial Y}{\partial \theta}
\end{pmatrix}
=|S|
\]
Ce qui nous dit que $dX\,dY = |S|\,dS\,d\theta$. En incorporant ce nouveau changement de variables, l'√©quation \eqref{eq:fourier_2d_inverse} devient :
\[
f(x,y) = \frac{1}{4\pi^{2}} \int_{0}^{\pi} \int_{-\infty}^{\infty}
\mathcal{F}_{2}f(S\cos\theta, S\sin\theta)\,
e^{iS(x\cos\theta + y\sin\theta)}\,|S|\,dS\,d\theta.
\]
Et en utilisant le th√©or√®me de la tranche centrale, nous voyons que l'√©quation ci-dessus est en fait √©gale √†
\begin{equation}
    f(x,y) = \frac{1}{4\pi^{2}} \int_{0}^{\pi} \int_{-\infty}^{\infty}
    \mathcal{F}\bigl(\mathcal{R}f(S,\theta)\bigr)\,
    e^{iS(x\cos\theta + y\sin\theta)}\,|S|\,dS\,d\theta.
    \label{eq:fourier_radon}
\end{equation}
Prenons maintenant un regard plus attentif sur l'int√©grale int√©rieure de l'√©quation \eqref{eq:fourier_radon} et en utilisant la d√©finition de la Transform√©e de Fourier inverse, on a :
\[
    \begin{array}{rcl}
        \int_{-\infty}^{\infty}
        \mathcal{F}\bigl(\mathcal{R}f(S,\theta)\bigr)\,
        e^{iS(x\cos\theta + y\sin\theta)}\,|S|\,dS
        &=&
        2\pi \left(
        \frac{1}{2\pi} \int_{-\infty}^{\infty}
        \mathcal{F}\bigl(\mathcal{R}f(S,\theta)\bigr)\,
        e^{iS(x\cos\theta + y\sin\theta)}\,|S|\,dS
        \right)\\
        &=&
        2\pi\,\mathcal{F}^{-1}
        \Bigl(
        |S|\,\mathcal{F}\bigl(\mathcal{R}f\bigr)(S,\theta)
        \Bigr)
        \bigl(x\cos\theta + y\sin\theta,\theta\bigr)\\
    \end{array}
\]


Autrement dit, l'int√©grale int√©rieure de l'√©quation (7.4) est √©gale √† $2\pi$ fois l'inverse de la transform√©e de Fourier de
$|S|\,\mathcal{F}\bigl(\mathcal{R}f\bigr)(S,\theta)$
au point $(x\cos\theta + y\sin\theta,\theta)$.
Nous pouvons alors voir que l'√©quation (7.4) est en fait √©gale √†
\[
\frac{1}{2\pi} \int_{0}^{\pi}
\mathcal{F}^{-1}
\Bigl(
|S|\,\mathcal{F}\bigl(\mathcal{R}f\bigr)(S,\theta)
\Bigr)
\bigl(x\cos\theta + y\sin\theta,\theta\bigr)
\,d\theta.
\]

Finalement, nous constatons que l'int√©grale ci-dessus est √©gale √† $\tfrac{1}{2}$ de la r√©troprojection donn√©e dans la d√©finition \eqref{eq:FBP} pour
$\mathcal{F}^{-1}\bigl[|S|\,\mathcal{F}(\mathcal{R}f)(S,\theta)\bigr]$.
Nous simplifions donc l'√©quation pr√©c√©dente pour obtenir
\[
\frac{1}{2}\,
\mathcal{B}
\Bigl\{
\mathcal{F}^{-1}
\bigl[|S|\,\mathcal{F}\bigl(\mathcal{R}f(S,\theta)\bigr)\bigr]
\Bigr\}(x,y).
\]

Ce qui nous conduit √† la conclusion souhait√©e :
\[
f(x,y)
=
\frac{1}{2}\,
\mathcal{B}
\Bigl\{
\mathcal{F}^{-1}
\bigl[|S|\,\mathcal{F}\bigl(\mathcal{R}f(S,\theta)\bigr)\bigr]
\Bigr\}(x,y).
\]
\hfill $\square$\\
Le facteur important dans cette formule est le multiplicateur $|S|$ qui appara√Æt entre la transform√©e de Fourier et son inverse. Sans ce facteur, ces deux termes s'annuleraient mutuellement et nous nous retrouverions avec la formule standard de r√©troprojection pour la transform√©e de Radon que nous avons rencontr√©e pr√©c√©demment et qui, comme nous l'avons vu, ne nous donne pas directement $f(x, y)$. Nous appelons ce $|S|$ suppl√©mentaire un \textbf{filtre} de la transform√©e de Radon, ce qui nous donne le nom de la formule de \textbf{r√©troprojection filtr√©e}.
\begin{proposition}
    Soit $f$ et $g$ deux fonctions int√©grables d√©finies sur $\mathbb{R}$, alors
    \[(\mathcal{B}g\star f)(X, Y) = \mathcal{B}(g\star \mathcal{R}f)(X, Y)\]
\end{proposition}
Consid√©rons maintenant la relation \eqref{eq:FBP_filter} et 
supposons qu'il existe une fonction, not√©e $\varphi(t)$, dont la transform√©e de Fourier
soit √©gale √† notre facteur de filtrage $|S|$. Autrement dit, supposons qu'il existe une
fonction $\varphi(t)$ telle que
\[
\mathcal{F}\varphi(S) = |S|.
\]
Plus simplement, supposons que nous connaissions une fonction dont la transform√©e de
Fourier est √©gale √† la fonction valeur absolue. Nous pourrions alors r√©√©crire la
r√©troprojection sous la forme suivante :
\begin{equation}
    f(x,y) = \frac{1}{2}\,\mathcal{B}
    \left\{
    \mathcal{F}^{-1}
    \bigl[
    \mathcal{F}\varphi \cdot \mathcal{F}(\mathcal{R}f)(S,\theta)
    \bigr]
    \right\}(x,y).
    \label{eq:FBP_varphi}
\end{equation}

Cependant, le membre de droite de l'√©quation \eqref{eq:FBP_varphi} contient un produit de transform√©esde Fourier, que nous savons √™tre √©gal √† la convolution des fonctions transform√©es
\[
    f(x,y)
    =
    \frac{1}{2}\,\mathcal{B}
    \left\{
    \mathcal{F}^{-1}
    \bigl[
    \mathcal{F}(\varphi \star \mathcal{R}f)(S,\theta)
    \bigr]
    \right\}(x,y).
\]

Mais ceci n'est rien d'autre que la transform√©e de Fourier inverse de la transform√©e
de Fourier, ce qui nous ram√®ne √† la fonction de d√©part. Cela nous conduit √† la formule
de r√©troprojection filtr√©e beaucoup plus simple :
\begin{equation}
    f(x,y) = \frac{1}{2}\,\mathcal{B}(\varphi \star \mathcal{R}f)(x,y).
    % 
    \label{eq:FBP_varphi_simple}
\end{equation}

L'√©quation \eqref{eq:FBP_varphi_simple} est bien plus √©l√©gante que notre formule initiale de r√©troprojection filtr√©e et ne semble pas difficile √† appliquer. Physiquement parlant, $\mathcal{R}f$ repr√©sente nos donn√©es mesur√©es et l'√©quation \eqref{eq:FBP_varphi_simple} requiert simplement de les filtrer √† l'aide de notre nouvelle fonction $\varphi$, puis d'appliquer la formule de r√©troprojection, qui est une int√©grale relativement simple.

Malheureusement, il n'existe pas de fonction $\varphi$ dont la transform√©e de Fourier
soit exactement √©gale √† la valeur absolue. Consid√©rons la fonction $\mathcal{F}\varphi$ :
\[
\mathcal{F}\varphi(\omega)
=
\int_{-\infty}^{\infty}
\varphi(x)\,e^{-i\omega x}\,dx.
\]

Nous pouvons constater que, lorsque $\omega \to \infty$,
$\mathcal{F}\varphi(\omega) \to 0$ (remarquons l'exponentielle n√©gative).
Cependant, pour la fonction valeur absolue $|\omega|$, lorsque $\omega \to \infty$,
$|\omega| \to \infty$.
Par cons√©quent, il est impossible de trouver une fonction $\varphi$ telle que,
pour tout $\omega$, $\mathcal{F}\varphi(\omega) = |\omega|$.

Toutefois, tout notre travail pr√©c√©dent n'est pas vain. Examinons plut√¥t le type de
fonctions sur lesquelles nous avons restreint notre √©tude. Nous ne consid√©rons notre
fonction que sur un intervalle fini et supposons en fait qu'elle soit nulle en dehors
de cet intervalle. En √©tendant cette id√©e √† la transform√©e de Fourier, nous constatons
que nous devons porter notre attention sur les \emph{fonctions √† bande limit√©e}.

\begin{definition}
    Une fonction $\varphi$ est dite \emph{√† bande limit√©e} s'il existe un r√©el $L > 0$ tel que
    \begin{equation}
        \mathcal{F}\varphi(\omega)
        =
        \int_{-\infty}^{\infty}
        \varphi(x)\,e^{-i\omega x}\,dx
        =
        0
        \quad \text{pour tout } \omega \notin [-L, L].
        % 
        \label{eq:FBP_varphi_banded}
    \end{equation}
\end{definition}

Le facteur de filtrage $|S|$ sert √† amplifier le terme $\mathcal{F}(\mathcal{R}f)$ dans la formule de r√©troprojection filtr√©e originale \eqref{eq:FBP_filter}. En pratique, $\mathcal{F}(\mathcal{R}f)$ est tr√®s sensible aux hautes fr√©quences.

En concentrant notre attention sur les basses fr√©quences √† l'aide d'une fonction √† bande limit√©e $\varphi$, nous sommes en mesure d'√©viter ce probl√®me. Notre objectif est de remplacer $S$ par ce que l'on appelle un \emph{filtre passe-bas} (not√© $S'$), qui prend en compte les effets des basses fr√©quences tout en att√©nuant les hautes fr√©quences. Cette fonction $S'$ doit avoir un support compact et √™tre de la forme
\[
S' = \mathcal{F}\varphi
\]
(sur un intervalle compact).

Le co√ªt de l'utilisation de $S'(\omega)$ est que nous ne disposons plus de l'√©galit√© pr√©sent√©e dans l'√©quation \eqref{eq:FBP_varphi_simple}. En revanche, nous obtenons :
\begin{equation}
    f(x,y) \approx \frac{1}{2}\,\mathcal{B}\!\left(\mathcal{F}^{-1} S' \star \mathcal{R}f \right)(x,y).
    \label{eq:FBP_varphi_approx}
\end{equation}

De mani√®re g√©n√©rale, la plupart des filtres passe-bas sont de la forme
\[
S'(\omega) = |\omega| \cdot F(\omega) \cdot \Pi_L(\omega),
\]
o√π $L > 0$ d√©finit la r√©gion sur laquelle le filtrage est effectu√©. Diff√©rentes fonctions $F$ d√©terminent les caract√©ristiques pr√©cises du filtre, et $\Pi_L(\omega)$ est d√©finie comme suit :
\[
    \Pi_L(\omega) =
    \begin{cases}
        1 & \text{si } |\omega| \leq L, \\
        0 & \text{si } |\omega| > L.
    \end{cases}
\]

Nous introduisons maintenant deux filtres couramment utilis√©s en imagerie num√©rique et en traitement du signal : le filtre \emph{Ram-Lak} et le filtre \emph{Hann}.

\subsection*{Filtre Ram-Lak}

Le filtre Ram-Lak est d√©fini par :
\[
S'(\omega) = |\omega| \cdot \Pi_L(\omega) =
\begin{cases}
|\omega| & \text{si } |\omega| \leq L, \\
0 & \text{si } |\omega| > L.
\end{cases}
\]

Le filtre Ram-Lak constitue la base de nombreux autres filtres utilis√©s en analyse du signal, car il remplace simplement la fonction $F(\omega)$ par la fonction constante √©gale √† 1. D'autres filtres, tels que le filtre Hann, consistent g√©n√©ralement en des produits de fonctions sinus ou cosinus destin√©es √† √©liminer le bruit ind√©sirable.

\subsection*{Filtre Hann}

Le filtre Hann est donn√© par :
\[
S'(\omega) = |\omega| \cdot \frac{1}{2}
\left( 1 + \cos\!\left( \frac{2\pi \omega}{L} \right) \right)
\cdot \Pi_L(\omega).
\]

Le filtre Hann utilise la fonction de Hann
\[
\frac{1}{2}\left( 1 + \cos\!\left( \frac{2\pi \omega}{L} \right) \right)
\]
comme fonction $F(\omega)$,
% et son efficacit√© est illustr√©e dans le sinogramme et la r√©troprojection de la transform√©e de Radon de Johann, pr√©sent√©s √† la Figure~5.
% ====== TODO ======
% Python implementation
% ==================

\section{Discr√©tisation des m√©thodes analytiques}
Ainsi nous avons trait√© presque exclusivement des int√©grales continues pour la transform√©e de Radon, la transform√©e de Fourier et les formules de r√©troprojection. En pratique, cependant, nous n'avons qu'un ensemble fini de donn√©es avec lesquelles travailler. Par cons√©quent, nous devrons former des versions discr√®tes de toutes les formules que nous avons utilis√©es dans notre r√©troprojection filtr√©e.

Une fonction discr√®te est une fonction d√©finie uniquement sur un ensemble d√©nombrable. Pour nos besoins, nous consid√©rerons des fonctions discr√®tes d√©finies sur des ensembles finis (l'ensemble √©tant compos√© des lignes sur lesquelles nous avons pris nos mesures d'intensit√©). Soit \(g_n\) la fonction discr√®te \(g\) √† la valeur \(n\). Comme nous connaissons cette fonction discr√®te sur un ensemble fini, soit \(N\), nous pouvons dire que \(g = g_n : 0 \le n \le N - 1\). Si nous voulons √©tendre cette d√©finition √† tous les entiers, nous pouvons simplement ¬´ r√©p√©ter ¬ª notre fonction encore et encore ; c‚Äôest-√†-dire, nous pouvons la rendre p√©riodique avec une p√©riode \(N\). Cette extension sera utile pour certaines des formules discr√®tes que nous rencontrerons.

Supposons que nous prenions des mesures √† \(P\) angles diff√©rents \(\theta\) et que pour chaque angle nous ayons \(2M + 1\) faisceaux espac√©s d'une distance \(d\). Alors nous pouvons d√©finir des valeurs particuli√®res \(\theta_k\) et \(t_j\) comme

\[
\theta_k = \left\{ \frac{k \pi}{P} : 0 \le k \le P - 1 \right\},
\]

\[
t_j = \{ jd : -M \le j \le M \}.
\]

Ce qui nous permet de d√©finir une ligne particuli√®re comme \(l_{t_j, \theta_k}\). Nous d√©finissons donc la transform√©e de Radon discr√®te comme suit :

\begin{definition}
Pour une fonction absolument int√©grable \(f\) et \(0 \le k \le P\) et \(-M \le j \le M\), \((P, M > 0)\), nous d√©finissons la transform√©e de Radon discr√®te de \(f\), not√©e \(\mathcal{R}_D f\), comme

\[
\mathcal{R}_D f_{j,k} = \mathcal{R} f(t_j, \theta_k).
\]

\end{definition}
Pour mettre en ≈ìuvre la formule de r√©troprojection filtr√©e \eqref{eq:FBP_varphi_approx}, nous devons √©galement d√©finir la convolution de deux fonctions discr√®tes.

\begin{definition}
Pour deux fonctions discr√®tes \(N\)-p√©riodiques \(f\) et \(g\), nous d√©finissons la \textbf{convolution discr√®te} de \(f\) et \(g\), not√©e \(f \star g\), comme

\[
(f \star g)_m = \sum_{j=0}^{N-1} f_j \cdot g_{(m-j)}, \quad \text{pour } m \in \mathbb{Z}.
\]
√âvidemment, nous aurons √©galement besoin de la transform√©e de Fourier discr√®te.
\end{definition}

\begin{definition}[Transform√©e de Fourier discr√®te]
√âtant donn√©e une fonction discr√®te $N$-p√©riodique $f$, nous d√©finissons la \textbf{transform√©e de Fourier discr√®te} de $f$, not√©e $\mathcal{F}_D f$, par
\begin{equation}
(\mathcal{F}_D f)_j = \sum_{k=0}^{N-1} f_k e^{i 2 \pi k j / N}, \quad \text{pour } j = 0, 1, \dots, (N-1).
\end{equation}
Il convient de noter que la $N$-p√©riodicit√© de $f$ nous permet de remplacer les bornes de la sommation par tout ensemble d'entiers de longueur $(N-1)$. Avec cette d√©finition, il n'est pas surprenant que nous d√©finissions la transform√©e de Fourier discr√®te inverse de la mani√®re suivante.
\end{definition}

\begin{definition}[Transform√©e de Fourier discr√®te inverse]
√âtant donn√©e une fonction discr√®te $N$-p√©riodique $g$, la \textbf{transform√©e de Fourier discr√®te inverse} de $g$, not√©e $\mathcal{F}_D^{-1} g$, est d√©finie par
\begin{equation}
(\mathcal{F}_D^{-1} g)_n = \frac{1}{N} \sum_{k=0}^{N-1} g_k e^{i 2 \pi k n / N}, \quad \text{pour } n = 0, 1, \dots, (N-1).
\end{equation}
\end{definition}

Nous remarquons que plusieurs des m√™mes propri√©t√©s de la transform√©e de Fourier que nous avons d√©finies dans le cadre continu s'appliquent √©galement au cas discret avec de l√©g√®res modifications :

\begin{proposition}[Propri√©t√©s des fonctions discr√®tes $N$-p√©riodiques]
Pour des fonctions discr√®tes $N$-p√©riodiques $f$ et $g$ :
\begin{enumerate}
    \item $\mathcal{F}_D(f \star g) = (\mathcal{F}_D f) \cdot (\mathcal{F}_D g)$
    \item $\mathcal{F}_D(f \cdot g) = \frac{1}{N} (\mathcal{F}_D f) \star (\mathcal{F}_D g)$
    \item $\mathcal{F}_D^{-1}(\mathcal{F}_D f)_n = f_n \quad \text{pour tout } n \in \mathbb{Z}$
\end{enumerate}
\end{proposition}

Nous sommes maintenant pr√™ts √† aborder la discr√©tisation de la formule de r√©troprojection elle-m√™me. Rappelons que la formule de r√©troprojection √©tait d√©finie comme une int√©grale de $0$ √† $\pi$ par rapport √† $d\theta$. Dans le cas discret, nous avons remplac√© ce $d\theta$ continu par $k\pi / P$ pour $0 \le k \le (P-1)$. Cela conduit √† la d√©finition suivante de la \textbf{r√©troprojection discr√®te} :

\begin{definition}[R√©troprojection discr√®te]
√âtant donn√©e une fonction discr√®te $h$, nous d√©finissons la \textbf{r√©troprojection discr√®te} de $h$, not√©e $\mathcal{B}_D h$, par
\begin{equation}
\mathcal{B}_D h(x,y) = \frac{1}{N} \sum_{k=0}^{N-1} h \big(x \cos \frac{k \pi}{N} + y \sin \frac{k \pi}{N}, k \pi / N \big).
\label{eq:discrete_backprojection}
\end{equation}
\end{definition}

Rappelons notre forme finale pour la formule filtr√©e de r√©troprojection en √©quation \eqref{eq:FBP_varphi_approx} :
$$
f(x,y) \approx \frac{1}{2} \mathcal{B} (\mathcal{F}^{-1} S' \star \mathcal{R} f)(x,y).
$$

Pour former la version discr√®te de cette √©quation, nous voyons que nous devons appliquer la formule suivante:
\begin{equation}
f(x,y) \approx \frac{1}{2} \mathcal{B}_D \left( \mathcal{F}_D^{-1} \mathcal{S}' \ast \mathcal{R}_D f \right)(x,y).
\end{equation}

Nous rencontrons maintenant un l√©ger probl√®me. $\mathcal{R}_D f$ repr√©sente les donn√©es mesur√©es bas√©es sur les intensit√©s finales d'un seul faisceau de rayons X. Nous avons d√©fini les emplacements des diff√©rents faisceaux (et donc des diff√©rentes coupes) en utilisant un syst√®me de coordonn√©es perpendiculaire aux coordonn√©es polaires bas√© sur des angles discrets $\theta$ et des distances $t$.  

En examinant l'√©quation \eqref{eq:discrete_backprojection}, nous voyons que nous devons sommer sur $h$ en diff√©rents points $(x,y)$ dans le syst√®me de coordonn√©es cart√©sien pour cr√©er une grille de niveaux de gris rectangulaire qui repr√©sente notre objet original. Les syst√®mes de coordonn√©es polaires et cart√©siens ne correspondent pas n√©cessairement parfaitement, et nous devons donc \emph{interpoler} les points de donn√©es manquants. L'interpolation consiste √† cr√©er une fonction continue (ou au minimum par morceaux continues) √† partir d'un ensemble discret de valeurs. Il existe de nombreuses m√©thodes pour interpoler une fonction (spline cubique, Lagrange, etc.), chacune ayant ses avantages et inconv√©nients.  

Pour nos besoins, nous allons d√©finir un type g√©n√©ral d'interpolation bas√© sur une fonction de pond√©ration $W$ qui d√©termine comment nous allons choisir nos points interpol√©s. Nous ne d√©finissons pas de fonction de pond√©ration particuli√®re $W$, car les d√©tails de l'interpolation ne sont pas aussi importants que le fait que nous pouvons remplir les "trous" dans nos donn√©es.

\begin{definition}
Pour une fonction de pond√©ration donn√©e $W$ et une fonction discr√®te $N$-p√©riodique $g$, l'\emph{interpolation $W$} de $g$ est d√©finie par :
\begin{equation}
\mathcal{I}_W(g)(x) = \sum_n g(n) \cdot W\left(\frac{x}{d}-n\right), \quad \text{pour } -\infty < x < \infty.
\end{equation}
\end{definition}

Maintenant que nous avons couvert toutes les parties de l'√©quation \eqref{eq:FBP_varphi_simple} dans un cadre discret et trait√© le probl√®me de l'interpolation, nous pouvons proposer un algorithme de reconstruction discret pour r√©soudre le coefficient d'att√©nuation √† partir d'un ensemble de donn√©es discret.  

Nous interpolons ici la fonction $\left(\mathcal{F}_D^{-1}\mathcal{S}'\right) \ast \mathcal{R}_D f(jd,k\pi/N)$ (c'est-√†-dire que nous remplissons les trous apr√®s le filtrage de la transform√©e de Radon). D√©finissons cette fonction interpol√©e comme $\mathcal{I}$. Cela conduit √† la formule de reconstruction suivante :

\begin{equation}
\begin{aligned}
f(x_m,y_n) &\approx \frac{1}{2} \mathcal{B}_D \left( \left( \mathcal{F}_D^{-1}\mathcal{S}' \right) \ast \mathcal{R}_D f \right) (jd, k\pi/N) \\
&\approx \frac{1}{2} \mathcal{B}_D \mathcal{I}(x_m,y_n) \\
&= \frac{1}{2N} \sum_{k=0}^{N-1} \mathcal{I} \left( x_m \cos \frac{k\pi}{N} + y_n \sin \frac{k\pi}{N}, \frac{k\pi}{N} \right).
\end{aligned}
\end{equation}

L'√©quation pr√©c√©dente tient compte de la nature discr√®te de nos donn√©es r√©elles et traite les probl√®mes (comme le manque de donn√©es) qui surviennent lorsque l'on dispose d'un nombre fini de mesures.


\section{Formulation lin√©aire -- Synth√®se}
% Alternative: Formulation lin√©aire de la reconstruction tomographique
D'accord, nous allons expliquer pas √† pas comment passer de la formulation int√©grale continue de la r√©troprojection filtr√©e √† la \textbf{formulation lin√©aire discr√®te \(g = Af\)} utilis√©e en pratique en tomographie.

\subsection{R√©troprojection filtr√©e continue}
On a la formule continue pour la reconstruction filtr√©e :
\[
f(x,y) = \frac{1}{2} \int_0^\pi \Big( (\mathcal{F}^{-1} S') \star \mathcal{R} f \Big)(x\cos\theta + y\sin\theta, \theta) \, d\theta
\]

Ici :
\begin{itemize}
    \item $f(x,y)$ : coefficient d'att√©nuation √† reconstruire,
    \item $\mathcal{R} f(t,\theta)$ : transform√©e de Radon (projection √† l'angle $\theta$),
    \item $\mathcal{F}^{-1} S'$ : filtre appliqu√© sur chaque projection,
    \item $\star$ : convolution dans $t$.
\end{itemize}

C'est une \textbf{formule int√©grale continue}, d√©pendante de coordonn√©es polaires.

\subsection{Discr√©tisation des coordonn√©es et des angles}

Pour passer au discret :
\begin{enumerate}
    \item On ne mesure que $P$ angles : $\theta_k = k\pi/P$, $k = 0,\dots,P-1$,
    \item On ne mesure que $2M+1$ faisceaux par angle, espac√©s de $d$ : $t_j = j d, j=-M,\dots,M$,
    \item On obtient donc la \textbf{transform√©e de Radon discr√®te} :
    \[
    \mathcal{R}_D f_{j,k} = \mathcal{R} f(t_j, \theta_k).
    \]
\end{enumerate}

\subsection{Convolution et filtrage discrets}

On applique ensuite le filtre sur chaque projection :
\[
h_{j,k} = (\mathcal{F}_D^{-1} \mathcal{S}' \ast \mathcal{R}_D f)_{j,k}
\]

Ici, $\ast$ est la \textbf{convolution discr√®te} dans $t$ :
\[
(f \ast g)_m = \sum_{n=0}^{N-1} f_n \, g_{(m-n)}.
\]

\subsection{Discr√©tisation de la r√©troprojection}

La r√©troprojection discr√®te est :
\[
f(x_m,y_n) \approx \frac{1}{2N} \sum_{k=0}^{N-1} h\Big( x_m \cos \frac{k\pi}{N} + y_n \sin \frac{k\pi}{N}, \frac{k\pi}{N} \Big).
\]

Comme les coordonn√©es cart√©siennes $(x_m, y_n)$ ne tombent pas exactement sur les positions $t_j$, on \textbf{interpole} :
\[
h\big(x_m\cos\theta_k + y_n \sin\theta_k, \theta_k\big) \approx \sum_j h_{j,k} \, W\left(\frac{x_m \cos\theta_k + y_n \sin\theta_k - t_j}{d}\right),
\]
o√π $W$ est la fonction de pond√©ration de l'interpolation (lin√©aire, spline, etc.).  
Cela transforme chaque $f(x_m, y_n)$ en \textbf{combinaison lin√©aire des mesures $h_{j,k}$}.

\subsection{Passage √† la forme matricielle lin√©aire}

Si on note :
\begin{itemize}
    \item $f$ le vecteur de tous les $f(x_m, y_n)$ sur la grille,
    \item $g$ le vecteur de toutes les mesures projet√©es filtr√©es $h_{j,k}$,
    \item $A$ la matrice repr√©sentant la \textbf{r√©troprojection + interpolation},
\end{itemize}

alors :
\[
f_i = \sum_j A_{ij} \, g_j
\]

Chaque coefficient $A_{ij}$ repr√©sente le poids avec lequel la projection $g_j$ contribue au pixel $f_i$.  

On obtient donc :
\[
\boxed{g = Af} \quad \text{ou souvent } f = A g \text{ selon la notation.}
\]

En pratique, $A$ est \textbf{tr√®s grande et creuse}, mais la reconstruction se r√©duit √† un simple \textbf{produit matriciel}.

\subsection{Synth√®se}

Le passage de l'int√©grale continue √† $g = Af$ se fait en quatre √©tapes principales :
\begin{enumerate}
    \item \textbf{√âchantillonnage discret} des angles et des faisceaux ‚Üí $\mathcal{R}_D f$,
    \item \textbf{Filtrage discret} via convolution et transform√©e de Fourier discr√®te,
    \item \textbf{R√©troprojection discr√®te} et interpolation sur la grille cart√©sienne,
    \item \textbf{√âcriture lin√©aire} : chaque pixel reconstruit est une combinaison lin√©aire des mesures ‚Üí matrice $A$.
\end{enumerate}

Ainsi, \textbf{toute la formule int√©grale est transform√©e en somme discr√®te}, et la lin√©arit√© de la convolution et de la r√©troprojection permet de la repr√©senter par $A$.

% ---- TODO -----
% \section{Limites de la m√©thode analytique}
% ============================================================
\section{Th√©orie du Compressed Sensing}
% ============================================================

En tomodensitom√©trie (CT), la r√©duction du nombre de projections et de la dose de rayonnement
constitue un enjeu majeur de s√©curit√© clinique et de performance op√©rationnelle. La diminution
de l'exposition aux rayons $\mathbf{X}$ vise √† limiter les risques biologiques associ√©s aux
rayonnements ionisants, en particulier dans les contextes d'examens r√©p√©t√©s ou pour les
populations sensibles. Toutefois, cette r√©duction conduit in√©vitablement √† une acquisition
de donn√©es incompl√®tes et bruit√©es, rendant la reconstruction d'image plus difficile.\vspace{5pt}\\
D'un point de vue math√©matique, cette situation se traduit par un probl√®me inverse
sous-d√©termin√©, pour lequel les m√©thodes analytiques classiques, telles que la
r√©troprojection filtr√©e, deviennent instables ou g√©n√®rent des artefacts importants.
Le \emph{Compressed Sensing} (CS) fournit un cadre th√©orique et algorithmique permettant
d'aborder cette probl√©matique en exploitant des propri√©t√©s structurelles des images CT.

\begin{definition}
    Le \emph{compressed sensing} (CS) est un cadre math√©matique et algorithmique permettant la
    reconstruction de signaux de grande dimension √† partir d'un nombre de mesures
    significativement inf√©rieur √† celui requis par les m√©thodes d'√©chantillonnage classiques,
    sous r√©serve que le signal pr√©sente une structure de parcimonie adapt√©e.
\end{definition}

\begin{definition}
    Soit $x \in \mathbb{R}^{n}$ un signal inconnu. On dit que $x$ est
    \(k\)-parcimonieux dans une base (ou un dictionnaire) \(\Psi\)
    (par exemple ondelettes, DCT) si
    \[
        x = \Psi \alpha, \qquad \text{o√π } \alpha \text{ poss√®de au plus } k \ll n
        \text{ coefficients non nuls}.
    \]
\end{definition}

Dans le cas des images CT, bien que la distribution d'att√©nuation ne soit pas parcimonieuse
dans le domaine spatial, elle est souvent compressible dans des bases multi-√©chelles ou via
le gradient de l'image. Cette propri√©t√© constitue le fondement de l'application du
compressed sensing √† la reconstruction tomographique.

Les mesures acquises lors d'un examen CT peuvent √™tre mod√©lis√©es par un ensemble de relations
lin√©aires :
\[
    \mathbf{y} = \mathbf{A}\mathbf{x},
\]
o√π $\mathbf{A} \in \mathbb{R}^{m \times n}$ repr√©sente l'op√©rateur de projection discr√©tis√©
(assimilable √† la transform√©e de Radon discr√®te) et $m \ll n$ lorsque le nombre de projections
est r√©duit.

Contrairement au cadre classique de l'√©chantillonnage, qui impose un nombre de mesures au
moins √©gal √† la dimension du signal, le compressed sensing montre que
\[
    m \gtrsim k \log(n/k)
\]
peut √™tre suffisant pour une reconstruction stable, sous des conditions appropri√©es sur
l'op√©rateur $\mathbf{A}$, telles que l'incoh√©rence ou la propri√©t√© d'isom√©trie restreinte
(\emph{Restricted Isometry Property}, RIP).

\subsection{Le probl√®me inverse en tomodensitom√©trie}

La reconstruction CT s'inscrit dans le cadre g√©n√©ral des probl√®mes inverses, o√π l'objectif
est d'estimer une image √† partir de mesures indirectes, bruit√©es et incompl√®tes. Ce probl√®me
peut √™tre formul√© sous la forme :
\begin{equation}
    \mathbf{y} = \mathcal{A}\mathbf{x} + \mathbf{n},
    \label{eq:inverse_problem}
\end{equation}
o√π :
\begin{itemize}
    \item[-] $\mathbf{x} \in \mathbb{R}^n$ repr√©sente la distribution d'att√©nuation √† reconstruire,
    \item[-] $\mathbf{y} \in \mathbb{R}^m$ correspond aux donn√©es de projection (sinogramme),
    \item[-] $\mathcal{A}$ mod√©lise le processus de projection CT,
    \item[-] $\mathbf{n}$ repr√©sente le bruit de mesure, principalement de nature quantique.
\end{itemize}

Lorsque le nombre de projections est r√©duit, l'op√©rateur $\mathcal{A}$ devient non inversible
et le probl√®me est sous-d√©termin√©. Cette situation est inh√©rente aux strat√©gies de r√©duction
de dose et ne peut √™tre √©vit√©e sans compromettre la s√©curit√© du patient.

\subsection{Mal-positude et cons√©quences pratiques}

\begin{definition}
    Un probl√®me est dit \textbf{bien pos√©} au sens de Hadamard s'il v√©rifie l'existence,
    l'unicit√© et la stabilit√© de la solution. Si l'une de ces conditions n'est pas satisfaite,
    le probl√®me est dit \emph{mal pos√©}.
\end{definition}

Dans le contexte de la reconstruction CT √† faible dose, la condition d'unicit√© est viol√©e
du fait de la sous-d√©termination, et la condition de stabilit√© est fortement compromise par
la pr√©sence de bruit. De faibles fluctuations du sinogramme peuvent ainsi engendrer des
artefacts marqu√©s dans l'image reconstruite.

\subsection{R√©gularisation par parcimonie et Compressed Sensing}

Pour rendre le probl√®me inverse traitable, il est n√©cessaire d'introduire des informations
a priori sur la solution recherch√©e. Le compressed sensing propose d'utiliser la parcimonie
ou la compressibilit√© de l'image CT dans une repr√©sentation appropri√©e comme m√©canisme de
r√©gularisation.\vspace{5pt}\\
Cette hypoth√®se restreint l'ensemble des solutions admissibles et permet de transformer un
probl√®me inverse mal pos√© en un probl√®me d'optimisation bien conditionn√©, pour lequel une
solution stable et physiquement plausible peut √™tre obtenue malgr√© la r√©duction du nombre
de projections.\vspace{5pt}\\
Jusqu'√† pr√©sent, le compressed sensing a √©t√© pr√©sent√© comme un cadre
th√©orique exploitant la parcimonie pour r√©soudre des probl√®mes inverses
sous-d√©termin√©s. En pratique, cette hypoth√®se de parcimonie est int√©gr√©e
au processus de reconstruction via des formulations variationnelles.
Ces formulations constituent un cadre g√©n√©ral permettant d'unifier les
approches classiques de r√©gularisation et les m√©thodes issues du
compressed sensing.

\subsection{Formulation variationnelle des probl√®mes inverses}
% ============================================================================================

Dans de nombreux probl√®mes d'imagerie, et en particulier en tomodensitom√©trie
√† faible dose, l'objectif est de reconstruire une image inconnue
$\mathbf{x} \in \mathbb{R}^n$ √† partir d'un ensemble de mesures
$\mathbf{y} \in \mathbb{R}^m$ obtenues par un syst√®me d'acquisition indirect.
Ce processus est g√©n√©ralement mod√©lis√© par une relation lin√©aire de la forme
\[
\mathbf{y} = \mathcal{A}\mathbf{x} + \boldsymbol{\varepsilon},
\]
o√π $\mathcal{A}$ repr√©sente l'op√©rateur direct du syst√®me CT et
$\boldsymbol{\varepsilon}$ un terme de bruit.

Lorsque les donn√©es sont bruit√©es et/ou acquises de mani√®re incompl√®te
($m \ll n$), l'op√©rateur $\mathcal{A}$ devient non inversible ou mal conditionn√©.
Dans ce cas, une inversion directe est soit impossible, soit extr√™mement
instable, et de petites perturbations des donn√©es peuvent engendrer de fortes
d√©gradations de la solution reconstruite. Ce ph√©nom√®ne est caract√©ristique des
probl√®mes inverses mal pos√©s.

\begin{definition}
Un \emph{probl√®me inverse} consiste √† estimer une quantit√© inconnue
$\mathbf{x}$ √† partir d'observations indirectes $\mathbf{y}$, reli√©es par un
op√©rateur $\mathcal{A}$, lorsque l'inversion directe de cet op√©rateur est
impossible ou instable.
\end{definition}

\paragraph{Principe de la r√©gularisation.}
Afin de rendre le probl√®me inverse traitable, il est n√©cessaire d'introduire
des informations a priori sur la solution recherch√©e. Cette d√©marche est
connue sous le nom de \emph{r√©gularisation}.

\begin{definition}
Une r√©gularisation est une application
$\mathfrak{R}_{\alpha} : \mathbb{R}^m \rightarrow \mathbb{R}^n$ qui associe √†
des donn√©es observ√©es $\mathbf{y}$ une solution stable $\hat{\mathbf{x}}$,
en incorporant des hypoth√®ses suppl√©mentaires sur la structure de la solution.
\end{definition}

Intuitivement, une m√©thode de r√©gularisation vise √† √©tendre la notion d'inverse
au cadre bruit√© et mal pos√©, de sorte que
\[
\mathfrak{R}_{\alpha}(\mathcal{A}\mathbf{x} + \boldsymbol{\varepsilon})
\approx \mathbf{x},
\]
m√™me lorsque $\boldsymbol{\varepsilon} \neq \mathbf{0}$ ou que
$\mathcal{A}$ n'est pas inversible.

\paragraph{Formulation variationnelle.}
Une approche largement utilis√©e pour impl√©menter la r√©gularisation consiste √†
formuler le probl√®me inverse comme un probl√®me d'optimisation variationnelle,
dans lequel on recherche une solution √©quilibrant fid√©lit√© aux donn√©es et
conformit√© aux a priori. Cette formulation s'√©crit g√©n√©ralement sous la forme

\begin{equation}
    \hat{\mathbf{x}} =
    \underset{\mathbf{x} \in \mathbb{R}^n}{\arg\min}
    \left\{
    \underbrace{\left\| \mathcal{A}\mathbf{x} - \mathbf{y} \right\|_{2}^{2}}_{\text{fid√©lit√© aux donn√©es}}
    + \alpha
    \underbrace{\mathcal{R}(\mathbf{x})}_{\text{terme de r√©gularisation}}
    \right\}.
\end{equation}

Les diff√©rents termes de cette formulation jouent des r√¥les compl√©mentaires :

\begin{itemize}
    \item \textbf{Fid√©lit√© aux donn√©es :}
    Ce terme impose la coh√©rence entre l'image reconstruite $\mathbf{x}$ et les
    mesures observ√©es $\mathbf{y}$. Dans un contexte bruit√©, il n'est pas souhaitable
    de l'annuler strictement, car cela conduirait √† une reconstruction amplifiant
    le bruit.

    \item \textbf{Terme de r√©gularisation :}
    Le r√©gularisant $\mathcal{R}(\mathbf{x})$ encode les informations a priori
    disponibles sur la solution recherch√©e, telles que la r√©gularit√©, la
    parcimonie ou des contraintes physiques. Il permet de restreindre l'ensemble
    des solutions admissibles et d'am√©liorer la stabilit√© du probl√®me.

    \item \textbf{Param√®tre de r√©gularisation $\alpha$ :}
    Le param√®tre $\alpha > 0$ contr√¥le le compromis entre fid√©lit√© aux donn√©es
    et influence de l'a priori. Un choix inad√©quat peut conduire soit √† une
    reconstruction bruit√©e (faible $\alpha$), soit √† une image excessivement
    liss√©e (grand $\alpha$).
\end{itemize}

\paragraph{Cas particulier : r√©gularisation de Tikhonov.}
Une r√©gularisation classique consiste √† choisir un r√©gularisant quadratique,
conduisant √† la r√©gularisation dite de Tikhonov. Par exemple, en supposant que
la solution recherch√©e soit proche d'un mod√®le de r√©f√©rence $\boldsymbol{\mu}$,
on peut d√©finir
\[
\mathcal{R}(\mathbf{x})
= \| \mathbf{x} - \boldsymbol{\mu} \|_{L^{2},\mathcal{Q}}^{2}
:= \langle \mathbf{x} - \boldsymbol{\mu},
\mathcal{Q}(\mathbf{x} - \boldsymbol{\mu}) \rangle,
\]
o√π $\mathcal{Q}$ est un op√©rateur positif d√©finissant une pond√©ration
directionnelle.

Bien que cette approche soit math√©matiquement simple et num√©riquement stable,
elle favorise des solutions lisses et ne permet pas de promouvoir des structures
parcimonieuses. Dans le contexte de la reconstruction CT √† faible dose, elle est
souvent insuffisante pour pr√©server les contours et les d√©tails fins.

\medskip
\noindent
Le compressed sensing s'inscrit naturellement dans ce cadre variationnel en
choisissant des r√©gularisants non quadratiques con√ßus pour promouvoir la
parcimonie ou la compressibilit√© de l'image, tels que les normes $\ell_1$ ou la
variation totale. Ces choix conduisent √† des probl√®mes d'optimisation
non diff√©rentiables, n√©cessitant des algorithmes it√©ratifs sp√©cifiques, qui
seront abord√©s dans les sections suivantes.

% ============================================================================================
\subsection{Formulation du probl√®me}

Dans le cadre de la tomographie par rayons X (CT), la reconstruction d‚Äôimage √† partir d‚Äôun nombre limit√© de projections conduit √† un probl√®me inverse sous-d√©termin√©. Le cadre du \emph{Compressed Sensing} (CS) permet de r√©soudre ce probl√®me en exploitant la parcimonie intrins√®que des images CT dans un domaine appropri√©, typiquement le domaine du gradient.

\begin{definition}[Image et repr√©sentation parcimonieuse]
Consid√©rons une image $f$, vue comme un vecteur colonne de dimension $n \times 1$ dans $\mathbb{R}^n$, dont les √©l√©ments individuels $f_j$, pour $j = 1, 2, \ldots, n$, repr√©sentent les $n$ valeurs de pixels de l'image. On d√©veloppe le vecteur $f$ dans une base orthonorm√©e $\Psi$ comme suit :
\[
f = \Psi \mathbf{x},
\]
o√π $\Psi$ est la matrice $n \times n$ $[\boldsymbol{\psi}_1, \ldots, \boldsymbol{\psi}_n]$, dont les vecteurs $\{\boldsymbol{\psi}_i\}_{i=1}^{n}$ constituent les colonnes, et o√π $\mathbf{x}$ est un vecteur colonne de dimension $n \times 1$.  

Si la majorit√© des composantes du vecteur $\mathbf{x}$ sont nulles ou quasi nulles, on dira que $f$ est \textbf{parcimonieuse} dans le domaine $\Psi$, et que $\mathbf{x}$ constitue sa \textbf{repr√©sentation parcimonieuse}.
\end{definition}

Dans le cas des images CT, la parcimonie ne s‚Äôexprime g√©n√©ralement pas directement dans le domaine spatial, mais plut√¥t dans le domaine du gradient. Les images CT sont en effet caract√©ris√©es par des r√©gions quasi homog√®nes s√©par√©es par des discontinuit√©s nettes, ce qui rend leur gradient parcimonieux.

Consid√©rons l'exemple du fant√¥me de Shepp--Logan repr√©sent√© √† la \Cref{fig:shepp-logan} et de son √©quivalent en gradient √† la \Cref{fig:shepp-logan-gradient}. On note l'intensit√© d'un pixel d'une image bidimensionnelle par $f_{h,w}$, o√π $h = 1,2,\ldots,H$ et $w = 1,2,\ldots,W$ ; $H$ et $W$ d√©signent respectivement la hauteur et la largeur de l'image 2D, et $W \times H = n$.

\begin{definition}[Module du gradient]
Si les valeurs des pixels sont not√©es $f_{h,w}$, le module du gradient discret est d√©fini comme suit :
\begin{equation}
\left| \nabla f_{h,w} \right|
=
\sqrt{
\left( f_{h+1,w} - f_{h,w} \right)^2
+
\left( f_{h,w+1} - f_{h,w} \right)^2
}.
\label{eq:gradient-modulus}
\end{equation}
\end{definition}

La \emph{variation totale} (Total Variation, TV) de l‚Äôimage est alors d√©finie comme la somme du module du gradient sur l‚Äôensemble des pixels :
\[
\mathrm{TV}(f) = \sum_{h,w} \left| \nabla f_{h,w} \right|.
\]
La minimisation de la variation totale correspond √† la minimisation de la norme $\ell_1$ du gradient et constitue une p√©nalisation standard dans le cadre du Compressed Sensing appliqu√© au CT.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{./images/shepp-logan phantom.png}
    \caption{Fant√¥me de Shepp--Logan}
    \label{fig:shepp-logan}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{./images/shepp-logan phantom gradient.png}
    \caption{Gradient du fant√¥me de Shepp--Logan}
    \label{fig:shepp-logan-gradient}
\end{figure}

\begin{proposition}[Mod√®le d'acquisition en tomographie CT]
En imagerie CT r√©aliste, les donn√©es de projection √† faisceau parall√®le, √©galement appel√©es \emph{sinogramme}, sont mod√©lis√©es par un syst√®me lin√©aire discret :
\begin{equation}
\mathbf{g} = \Phi \mathbf{f},
\end{equation}
o√π $\mathbf{g} \in \mathbb{R}^m$ est le vecteur des mesures de projection, et $\Phi \in \mathbb{R}^{m \times n}$ est la matrice syst√®me d√©crivant la g√©om√©trie d‚Äôacquisition CT.
\end{proposition}

En introduisant la repr√©sentation parcimonieuse de l‚Äôimage, le mod√®le devient :
\begin{equation}
\mathbf{g} = \Phi \mathbf{f} = \Phi \Psi \mathbf{x} = \Phi' \mathbf{x},
\label{eq:4}
\end{equation}
o√π $\Phi' = \Phi \Psi$.

Lorsque le nombre de projections est limit√©, on a $m \ll n$, ce qui rend le syst√®me sous-d√©termin√©.

\begin{proposition}[Reconstruction CT par Compressed Sensing]
La reconstruction de l‚Äôimage consiste alors √† r√©soudre le probl√®me d‚Äôoptimisation suivant :
\begin{equation}
\mathbf{x}
=
\arg\min_{\tilde{\mathbf{x}}}
\left\| \tilde{\mathbf{x}} \right\|_{1}
\quad
\text{sous la contrainte}
\quad
\left\| \Phi^{'}\tilde{\mathbf{x}} - \mathbf{g} \right\|_{2} \leq \varepsilon,
\end{equation}
o√π $\varepsilon$ mod√©lise le bruit pr√©sent dans les mesures.
\end{proposition}

Dans le cas particulier du CT, cette formulation est √©quivalente √† une minimisation de la variation totale de l‚Äôimage sous contrainte de fid√©lit√© aux donn√©es.

\subsection{Algorithmes de reconstruction it√©rative en Compressed Sensing}
\subsubsection{Descente de gradient}

\begin{definition}[Descente de gradient pour la minimisation de la variation totale]
Afin de minimiser la norme $\ell_1$ du gradient (variation totale), une m√©thode de descente de gradient est employ√©e. La mise √† jour de l'image $f$ s'effectue selon :
\begin{equation}
f^{\text{suivant}} = f^{\text{courant}} - \alpha \,\vec{\Delta}^{\,\text{courant}},
\end{equation}
o√π $\alpha$ est un pas de descente. Le terme $\vec{\Delta}$ correspond au gradient r√©gularis√© de la variation totale.
\end{definition}


% =======================================================================================================
% \subsection{Formulation du probl√®me}
% \begin{definition}[Image et repr√©sentation parcimonieuse]
% Consid√©rons une image $f$, vue comme un vecteur colonne de dimension $n \times 1$ dans $\mathbb{R}^n$, dont les √©l√©ments individuels $f_j$, pour $j = 1, 2, \ldots, n$, repr√©sentent les $n$ valeurs de pixels de l'image. On d√©veloppe le vecteur $f$ dans une base orthonorm√©e $\Psi$ comme suit :
% \[
% f = \Psi \mathbf{x},
% \]
% o√π $\Psi$ est la matrice $n \times n$ $[\boldsymbol{\psi}_1, \ldots, \boldsymbol{\psi}_n]$, dont les vecteurs $\{\boldsymbol{\psi}_i\}_{i=1}^{n}$ de dimension $n \times 1$ constituent les colonnes, et o√π $\mathbf{x}$ est √©galement un vecteur colonne de dimension $n \times 1$. Si toutes les composantes du vecteur $\mathbf{x}$, √† l'exception de quelques-unes, sont nulles ou quasi nulles, on dira que $f$ est \textbf{parcimonieuse} dans le domaine $\Psi$ et que $\mathbf{x}$ est sa \textbf{repr√©sentation parcimonieuse}.
% \end{definition}

% Consid√©rons l'exemple du fant√¥me de Shepp-Logan repr√©sent√© √† la \Cref{fig:shepp-logan} et de son √©quivalent en gradient √† la \Cref{fig:shepp-logan-gradient}. On note l'intensit√© d'un pixel d'une image bidimensionnelle par $f_{h,w}$, o√π $h = 1,2,\ldots,H$ et $w = 1,2,\ldots,W$ ; $H$ et $W$ d√©signent respectivement la hauteur et la largeur de l'image 2D, et $W \times H = n$.

% \begin{definition}[Module du gradient]
% Si les valeurs des pixels sont not√©es $f_{h,w}$, le module du gradient est d√©fini comme suit :
% \begin{equation}
% \left| \nabla f_{h,w} \right|
% =
% \sqrt{
% \left( f_{h+1,w} - f_{h,w} \right)^2
% +
% \left( f_{h,w+1} - f_{h,w} \right)^2
% }
% \label{eq:gradient-modulus}
% \end{equation}
% \end{definition}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\textwidth]{./images/shepp-logan phantom.png}
%     \caption{Shepp-Logan phantom}
%     \label{fig:shepp-logan}
% \end{figure}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\textwidth]{./images/shepp-logan phantom gradient.png}
%     \caption{Shepp-Logan phantom gradient}
%     \label{fig:shepp-logan-gradient}
% \end{figure}

% \begin{proposition}[Mod√®le d'acquisition en tomographie]
% En imagerie CT r√©aliste, on suppose que les donn√©es de projection √† faisceau parall√®le √©chantillonn√©es de l'image $f$ sont mod√©lis√©es par un syst√®me lin√©aire discret
% \begin{equation}
% \mathbf{g} = \Phi \mathbf{f},
% \end{equation}
% o√π le vecteur $\mathbf{g}$ est de longueur $m$, ses mesures individuelles √©tant not√©es $g_i$, pour $i = 1,2,\ldots,m$, et o√π $\Phi$ est la matrice syst√®me $m \times n$ produisant l'ensemble discret des mesures de projection pour un balayage √† faisceau parall√®le de l'objet.
% En substituant $\Psi \mathbf{x}$ √† $\mathbf{f}$, ce mod√®le s'√©crit :
% \begin{equation}
% \mathbf{g} = \Phi \mathbf{f} = \Phi \Psi \mathbf{x} = \Phi' \mathbf{x},
% \label{eq:4}
% \end{equation}
% o√π $\Phi' = \Phi \Psi$ est une matrice de dimension $m \times n$.
% \end{proposition}

% \begin{proposition}[Probl√®me de reconstruction par minimisation $\ell_1$]
% Pour une image parcimonieuse, puisque $m << n$ dans \eqref{eq:4}, il existe une infinit√© de vecteurs $\tilde{\mathbf{x}}$ satisfaisant $\mathbf{g} = \Phi' \tilde{\mathbf{x}}$. Par cons√©quent, la reconstruction d'image vise √† d√©terminer le vecteur $\mathbf{x}$ dans le domaine transform√© en r√©solvant le programme d'optimisation suivant :
% \begin{equation}
% \mathbf{x}
% =
% \arg\min_{\tilde{\mathbf{x}}}
% \left\| \tilde{\mathbf{x}} \right\|_{1}
% \quad
% \text{sous la contrainte}
% \quad
% \left| \Phi^{'}\tilde{\mathbf{x}} - \mathbf{g} \right| < \varepsilon,
% \end{equation}
% o√π $\varepsilon$ est un petit facteur d'erreur tenant compte du bruit dans les mesures, et o√π la norme $\ell_1$ est d√©finie par $\left\| \mathbf{x} \right\|_{1} = \sum_{i=1}^{N} |x_i|$.
% \end{proposition}

\begin{definition}[Mise √† jour par descente de gradient pour la norme $\ell_1$ du gradient]
    Pour minimiser la norme $\ell_{1}$ de l'image de gradient, une m√©thode de descente de gradient est employ√©e. La mise √† jour de l'image $f$ s'effectue it√©rativement selon :
    \begin{equation}
    f^{\text{suivant}} = f^{\text{courant}} - \alpha \,\vec{\Delta}^{\,\text{courant}},
    \end{equation}
    o√π $\alpha$ est une constante contr√¥lant la vitesse de descente. Le terme $\vec{\Delta}$ est une image dont la valeur de chaque pixel $(h,w)$ est donn√©e par la d√©riv√©e partielle de la norme $\ell_1$ du gradient :
    \begin{equation}
        \begin{array}{l l l}
            \nu_{h,w} & = &
            \dfrac{\partial \lVert \nabla f_{h,w} \rVert_{1}}{\partial f_{h,w}} \\[1.2ex]
            & = &
            \dfrac{2f_{h,w} - f_{h+1,w} - f_{h,w+1}}
            {\sqrt{\varepsilon + (f_{h+1,w} - f_{h,w})^{2} + (f_{h,w+1} - f_{h,w})^{2}}} \\[2ex]
            & + &
            \dfrac{f_{h,w} - f_{h-1,w}}
            {\sqrt{\varepsilon + (f_{h,w} - f_{h-1,w})^{2} + (f_{h-1,w+1} - f_{h-1,w})^{2}}} \\[2ex]
            & + &
            \dfrac{f_{h,w} - f_{h,w-1}}
            {\sqrt{\varepsilon + (f_{h+1,w-1} - f_{h,w-1})^{2} + (f_{h,w} - f_{h,w-1})^{2}}}
        \end{array}
    \end{equation}
\end{definition}

\begin{definition}[Matrice syst√®me $\Phi$ et poids $\varphi_{i,j}$]
Dans le cadre discret, le vecteur de donn√©es de projection √† faisceau parall√®le $\vec{g}$ est mod√©lis√© par une somme pond√©r√©e sur les pixels travers√©s par le rayon X :
\begin{equation}
g_i = \sum_{j=1}^{N} \varphi_{i,j} \cdot f_j, \quad \text{o√π } i = 1, 2, \cdots, M.
\end{equation}
Le coefficient de pond√©ration $\varphi_{i,j}$ de la matrice syst√®me $\Phi$ est √©gal √† la longueur d'intersection du $i$-√®me rayon √† travers le $j$-√®me pixel.
\end{definition}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{./images/projection √† faisceau parall√®le.png}
    \caption{Calcul du coefficient de poids $\varphi_{i,j}$ de la matrice syst√®me $\Phi$ √† partir de la longueur d'intersection du $i$-√®me rayon √† travers le $j$-√®me pixel.}
    \label{fig:phi}
\end{figure}

Le calcul direct de chaque $\varphi_{i,j}$ est co√ªteux. Pour acc√©l√©rer la reconstruction, on peut pr√©-calculer et stocker ces poids, et exploiter les propri√©t√©s de sym√©trie des projections √† faisceau parall√®le pour r√©duire le nombre de calculs n√©cessaires.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{./images/projection √† faisceau parall√®le-2.png}
    \caption{Mesures des rayons-\textbf{X} $a$, $b$, $c$ et $d$ pour des angles de rotation $\alpha$, $90-\alpha$, $90+\alpha$ et $180-\alpha$. Les propri√©t√©s de sym√©trie permettent de d√©duire les poids d'un rayon √† partir d'un autre.}
    \label{fig:mesure-rayons-X}
\end{figure}


\subsubsection{Pseudo-code}
\begin{algorithm}[H]
\caption{M√©thode de reconstruction hybride (SART + Descente de gradient)}
\label{alg:hybrid-reconstruction}
\begin{algorithmic}[1]
\Require $\varphi$ - matrice de projection, $g$ - donn√©es d'acquisition,
$M$ - nombre d'it√©rations SART, $\lambda$ - param√®tre de relaxation,
$\alpha$ - pas d'apprentissage
\Ensure $\hat{f}$ - image reconstruite

\Statex \textbf{(1) Initialisation de l'image}
\State $f^{(0)} \gets 0$

\Statex
\Statex \textbf{(2) Processus it√©ratif de type SART}
\For{$k = 1$ \textbf{√†} $M$} \Comment{Une p√©riode compl√®te d'it√©ration}
    \For{$j = 1$ \textbf{√†} $N$}
        \State $f_j^{(k)} \gets f_j^{(k-1)}
        + \lambda
        \cdot
        \frac{
            g_i - \sum_{n=1}^{N} \varphi_{i,n} f_n^{(k-1)}
        }{
            \sum_{n=1}^{N} \varphi_{i,n}^2
        }
        \cdot \varphi_{i,j}$
    \EndFor
\EndFor

\Statex
\Statex \textbf{(3) Initialisation pour la descente de gradient}
\State $\hat{f}^{(0)} \gets f^{(M)}$

\Statex
\Statex \textbf{(4) Descente de gradient (contrainte de parcimonie)}
\For{$l = 1$ \textbf{√†} $5$}
    \State $\vec{\Delta}_l \gets
    \left| \hat{f}^{(0)} - f^{(0)} \right|
    \cdot
    \frac{\nu_{x,y}}{\left| \nu_{x,y} \right|}$
    \State $\hat{f}^{(l)} \gets \hat{f}^{(l-1)} - \alpha \cdot \vec{\Delta}_l$
\EndFor

\Statex
\Statex \textbf{(5) Initialisation de l'√©tape it√©rative suivante}
\State $f^{(0)} \gets \hat{f}^{(5)}$
\State \Return $\hat{f}^{(5)}$

\end{algorithmic}
\end{algorithm}

% \textbf{(1) Initialisation de l'image \(f\) :}
% \[
% f^{(0)} = 0 ;
% \]

% \medskip
% \noindent
% \textbf{(2) Processus it√©ratif (type SART) :}
% Pour \(k\) variant de \(1\) √† \(M\) (une p√©riode compl√®te d'it√©ration) :
% \[
% f_j^{(k)} = f_j^{(k-1)} +
% \lambda \,
% \frac{
% g_i - \sum_{n=1}^{N} \varphi_{i,n}\, f_n^{(k-1)}
% }{
% \sum_{i=1}^{N} \varphi_{i,n}^{2}
% }
% \, \varphi_{i,j} ;
% \]
% o√π le param√®tre de relaxation \(\lambda\) est un nombre r√©el positif.

% \medskip
% \noindent
% \textbf{(3) Initialisation de l'image pour la descente de gradient :}
% \[
% \hat{f}^{(0)} = f^{(M)} ;
% \]

% \medskip
% \noindent
% \textbf{(4) It√©ration de descente de gradient (contrainte de parcimonie) :}
% Pour \(l = 1\) jusqu'√† \(5\) :
% \[
%     \hat{f}^{(l)} = \hat{f}^{(l-1)} - \alpha \cdot \vec{\Delta}_l ,
% \]
% avec
% \[
%     \vec{\Delta}_l =
%     \left| \hat{f}^{(0)} - f^{(0)} \right|
%     \cdot
%     \frac{\nu_{x,y}}{\left| \nu_{x,y} \right|}.
% \]

% \medskip
% \noindent
% \textbf{(5) Initialisation de l'√©tape it√©rative suivante :}
% \[
% f^{(0)} = \hat{f}^{(\text{end})} ;
% \]

Les √©tapes (2) √† (5) sont r√©p√©t√©es jusqu'√† ce que la diff√©rence entre deux images successives $f^{(M)}$ soit inf√©rieure √† un seuil (e.g., $0.001$) ou que le nombre d'it√©rations d√©passe une limite (e.g., $1000$). Les param√®tres typiques sont $\lambda = 1.0$, $\varepsilon = 0.0001$, $\alpha = 0.5$.
% =======================================================================================================



\subsection{M√©triques de performance}
\begin{definition}[M√©triques de similarit√© d'image]
Soient $f_r$ et $f_o$ les vecteurs repr√©sentant respectivement l'image reconstruite et l'image originale, compos√©es de $N$ pixels. On d√©finit les m√©triques suivantes :
\begin{itemize}
    \item \textbf{Erreur quadratique moyenne (RMSE)} :
    $\displaystyle \mathrm{RMSE} = \sqrt{\frac{\sum_{i=1}^{N} \left( f_{r_i} - f_{o_i} \right)^2}{N}}$
    \item \textbf{Indice universel de qualit√© (UQI)} :
    $\displaystyle \mathrm{UQI} =
    \frac{2\,\mathrm{Cov}\{f_r,f_o\}}{D(f_r)+D(f_o)}
    \cdot
    \frac{2\,\bar{f}_r\,\bar{f}_o}{\bar{f}_r^{\,2}+\bar{f}_o^{\,2}}$
    \item \textbf{Coefficient de corr√©lation (CC)} :
    $\displaystyle \mathrm{CC} =
    \frac{2\,\mathrm{Cov}\{f_r,f_o\}}
    {\sqrt{D(f_r)\cdot D(f_o)}}$
\end{itemize}
avec $\bar{f}_o = \frac{1}{N}\sum_{i=1}^{N} f_{o_i}$, $\bar{f}_r = \frac{1}{N}\sum_{i=1}^{N} f_{r_i}$, $D(f) = \frac{1}{N-1}\sum_{i=1}^{N} \left(f_{i}-\bar{f}\right)^2$,\\ et \\$\mathrm{Cov}\{f_r,f_o\} = \frac{1}{N-1}\sum_{i=1}^{N} \left(f_{r_i}-\bar{f}_r\right) \left(f_{o_i}-\bar{f}_o\right)$.
\end{definition}

% ============================================== DRAFT ==============================================
% Ce cadre permet de r√©soudre plusieurs limitations pratiques :
% \paragraph{R√©duction du nombre de mesures.}\text{}\\ 
% De nombreux syst√®mes d'acquisition sont limit√©s par le co√ªt, le temps ou l'√©nergie. Le compressed sensing permet :
% \begin{itemize}
%     \item[-] une acquisition plus rapide des donn√©es,
%     \item[-] une r√©duction de la complexit√© mat√©rielle,
%     \item[-] une diminution de la dose de radiation (par exemple en tomodensitom√©trie),
%     \item[-] une r√©duction des co√ªts de stockage et de transmission.
% \end{itemize}

% \paragraph{Probl√®mes inverses mal pos√©s (ill-posed inverse problems).}\text{}\\
% Lorsque le nombre de mesures est insuffisant pour garantir une solution unique, le CS introduit une r√©gularisation fond√©e sur la parcimonie, permettant une reconstruction stable. Les principales applications incluent :
% \begin{itemize}
%     \item[-] la tomographie (CT, IRM, PET),
%     \item[-] l'imagerie √† super-r√©solution,
%     \item[-] la d√©convolution,
%     \item[-] les inversions g√©ophysiques et les essais non destructifs.
% \end{itemize}

% \paragraph{Robustesse au bruit et aux donn√©es incompl√®tes.} \text{}\\
% Le CS garantit une reconstruction stable m√™me en pr√©sence de bruit, de corruptions ou d'observations manquantes.

% \subsection{Reconstruction de signaux par Compressed Sensing}

% \subsection{Reconstruction par optimisation}
% La formulation canonique de la reconstruction est
% \[
% \min_{\alpha} \|\alpha\|_{1} \quad \text{s.c.} \quad y = A \Psi \alpha,
% \]
% ou, en pr√©sence de bruit,
% \[
% \min_{\alpha} \|\alpha\|_{1} \quad \text{s.c.} \quad \|A \Psi \alpha - y\|_{2} \le \epsilon.
% \]
% Cela correspond aux formulations de type \emph{Basis Pursuit} ou \emph{LASSO}. La minimisation de la norme \(\ell_1\) favorise la parcimonie tout en conservant un probl√®me d'optimisation convexe et calculable efficacement.

% \subsection{Algorithmes gloutons}

% Des alternatives plus rapides incluent :
% \begin{itemize}
%     \item l'\emph{Orthogonal Matching Pursuit} (OMP),
%     \item le \emph{Compressive Sampling Matching Pursuit} (CoSaMP),
%     \item l'\emph{Iterative Hard Thresholding} (IHT).
% \end{itemize}
% Ces m√©thodes √©changent une partie de la pr√©cision contre un co√ªt computationnel r√©duit.

% \subsection{Applications du Compressed Sensing}

% \paragraph{Imagerie m√©dicale.}
% \begin{itemize}
%     \item acquisition IRM acc√©l√©r√©e,
%     \item CT √† dose r√©duite,
%     \item √©chographie √† haute cadence d'images.
% \end{itemize}

% \paragraph{Imagerie computationnelle.}
% \begin{itemize}
%     \item cam√©ras √† pixel unique,
%     \item imagerie √† ouverture cod√©e,
%     \item reconstruction hyperspectrale.
% \end{itemize}

% \paragraph{T√©l√©d√©tection et g√©ophysique.}
% \begin{itemize}
%     \item inversion sismique parcimonieuse,
%     \item imagerie radar et radar √† synth√®se d'ouverture (SAR).
% \end{itemize}

% \paragraph{Communications sans fil.}
% \begin{itemize}
%     \item estimation parcimonieuse de canaux,
%     \item r√©duction des pilotes dans les syst√®mes MIMO massifs.
% \end{itemize}

% \paragraph{Apprentissage automatique et traitement du signal.}
% \begin{itemize}
%     \item r√©gression parcimonieuse (LASSO),
%     \item apprentissage de dictionnaires,
%     \item ACP robuste et mod√®les de rang faible apparent√©s.
% \end{itemize}


% % Le compressed sensing (CS) est un cadre math√©matique et algorithmique qui permet de reconstruire des signaux de grande dimension √† partir d'un nombre de mesures bien inf√©rieur √† celui requis par les approches traditionnelles. Il exploite la parcimonie (sparsity) comme principal a priori structurel.
% % \subsection{Hypoth√®se de parcimonie}
% % Si un signal est parcimonieux ou compressible dans une certaine base, alors il peut √™tre reconstruit exactement (ou avec une erreur contr√¥l√©e) √† partir d'un nombre de mesures lin√©aires bien inf√©rieur √† sa dimension ambiante.
% % \subsection{Incoh√©rence et propri√©t√© de RIP}
% % \subsection{Basis Pursuit et LASSO}
% % \subsection{OMP et algorithmes gloutons}

